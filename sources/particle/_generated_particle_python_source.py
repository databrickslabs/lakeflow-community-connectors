# ==============================================================================
# Merged Lakeflow Source: particle
# ==============================================================================
# This file is auto-generated by tools/scripts/merge_python_source.py
# Do not edit manually. Make changes to the source files instead.
# ==============================================================================

from datetime import datetime
from decimal import Decimal
from typing import (
    Any,
    Dict,
    Iterator,
    List,
)
import json
import os

from pyspark.sql import Row
from pyspark.sql.datasource import DataSource, DataSourceReader, SimpleDataSourceStreamReader
from pyspark.sql.types import *
import base64
import requests


def register_lakeflow_source(spark):
    """Register the Lakeflow Python source with Spark."""

    ########################################################
    # libs/utils.py
    ########################################################

    def _parse_struct(value: Any, field_type: StructType) -> Row:
        """Parse a dictionary into a PySpark Row based on StructType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for StructType, got {type(value)}")
        # Spark Python -> Arrow conversion require missing StructType fields to be assigned None.
        if value == {}:
            raise ValueError(
                "field in StructType cannot be an empty dict. "
                "Please assign None as the default value instead."
            )
        field_dict = {}
        for field in field_type.fields:
            if field.name in value:
                field_dict[field.name] = parse_value(value.get(field.name), field.dataType)
            elif field.nullable:
                field_dict[field.name] = None
            else:
                raise ValueError(f"Field {field.name} is not nullable but not found in the input")
        return Row(**field_dict)


    def _parse_array(value: Any, field_type: ArrayType) -> list:
        """Parse a list into a PySpark array based on ArrayType schema."""
        if not isinstance(value, list):
            if field_type.containsNull:
                return [parse_value(value, field_type.elementType)]
            raise ValueError(f"Expected a list for ArrayType, got {type(value)}")
        return [parse_value(v, field_type.elementType) for v in value]


    def _parse_map(value: Any, field_type: MapType) -> dict:
        """Parse a dictionary into a PySpark map based on MapType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for MapType, got {type(value)}")
        return {
            parse_value(k, field_type.keyType): parse_value(v, field_type.valueType)
            for k, v in value.items()
        }


    def _parse_string(value: Any) -> str:
        """Convert value to string."""
        return str(value)


    def _parse_integer(value: Any) -> int:
        """Convert value to integer."""
        if isinstance(value, str) and value.strip():
            return int(float(value)) if "." in value else int(value)
        if isinstance(value, (int, float)):
            return int(value)
        raise ValueError(f"Cannot convert {value} to integer")


    def _parse_float(value: Any) -> float:
        """Convert value to float."""
        return float(value)


    def _parse_decimal(value: Any) -> Decimal:
        """Convert value to Decimal."""
        return Decimal(value) if isinstance(value, str) and value.strip() else Decimal(str(value))


    def _parse_boolean(value: Any) -> bool:
        """Convert value to boolean."""
        if isinstance(value, str):
            lowered = value.lower()
            if lowered in ("true", "t", "yes", "y", "1"):
                return True
            if lowered in ("false", "f", "no", "n", "0"):
                return False
        return bool(value)


    def _parse_date(value: Any) -> datetime.date:
        """Convert value to date."""
        if isinstance(value, str):
            for fmt in ("%Y-%m-%d", "%m/%d/%Y", "%d-%m-%Y", "%Y/%m/%d"):
                try:
                    return datetime.strptime(value, fmt).date()
                except ValueError:
                    continue
            return datetime.fromisoformat(value).date()
        if isinstance(value, datetime):
            return value.date()
        raise ValueError(f"Cannot convert {value} to date")


    def _parse_timestamp(value: Any) -> datetime:
        """Convert value to timestamp."""
        if isinstance(value, str):
            ts_value = value.replace("Z", "+00:00") if value.endswith("Z") else value
            try:
                return datetime.fromisoformat(ts_value)
            except ValueError:
                for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S"):
                    try:
                        return datetime.strptime(ts_value, fmt)
                    except ValueError:
                        continue
        elif isinstance(value, (int, float)):
            return datetime.fromtimestamp(value)
        elif isinstance(value, datetime):
            return value
        raise ValueError(f"Cannot convert {value} to timestamp")


    def _decode_string_to_bytes(value: str) -> bytes:
        """Try to decode a string as base64, then hex, then UTF-8."""
        try:
            return base64.b64decode(value)
        except Exception:
            pass
        try:
            return bytes.fromhex(value)
        except Exception:
            pass
        return value.encode("utf-8")


    def _parse_binary(value: Any) -> bytes:
        """Convert value to bytes. Tries base64, then hex, then UTF-8 for strings."""
        if isinstance(value, bytes):
            return value
        if isinstance(value, bytearray):
            return bytes(value)
        if isinstance(value, str):
            return _decode_string_to_bytes(value)
        if isinstance(value, list):
            return bytes(value)
        return str(value).encode("utf-8")


    # Mapping of primitive types to their parser functions
    _PRIMITIVE_PARSERS = {
        StringType: _parse_string,
        IntegerType: _parse_integer,
        LongType: _parse_integer,
        FloatType: _parse_float,
        DoubleType: _parse_float,
        DecimalType: _parse_decimal,
        BooleanType: _parse_boolean,
        DateType: _parse_date,
        TimestampType: _parse_timestamp,
        BinaryType: _parse_binary,
    }


    def parse_value(value: Any, field_type: DataType) -> Any:
        """
        Converts a JSON value into a PySpark-compatible data type based on the provided field type.
        """
        if value is None:
            return None

        # Handle complex types
        if isinstance(field_type, StructType):
            return _parse_struct(value, field_type)
        if isinstance(field_type, ArrayType):
            return _parse_array(value, field_type)
        if isinstance(field_type, MapType):
            return _parse_map(value, field_type)

        # Handle primitive types via type-based lookup
        try:
            field_type_class = type(field_type)
            if field_type_class in _PRIMITIVE_PARSERS:
                return _PRIMITIVE_PARSERS[field_type_class](value)

            # Check for custom UDT handling
            if hasattr(field_type, "fromJson"):
                return field_type.fromJson(value)

            raise TypeError(f"Unsupported field type: {field_type}")
        except (ValueError, TypeError) as e:
            raise ValueError(f"Error converting '{value}' ({type(value)}) to {field_type}: {str(e)}")


    ########################################################
    # sources/particle/particle.py
    ########################################################

    class LakeflowConnect:
        """Particle.io REST API connector implementing the LakeflowConnect interface."""

        def __init__(self, options: dict[str, str]) -> None:
            """
            Initialize the Particle connector with connection-level options.

            Authentication options (choose one):
                1. access_token: Pre-generated Particle access token for API authentication
                2. username + password: Credentials to automatically generate access token

            Optional:
                - base_url: Override for Particle API base URL (default: https://api.particle.io)
                - token_cache_file: File to cache generated tokens (default: None)
            """
            self.base_url = options.get("base_url", "https://api.particle.io").rstrip("/")
            self.token_cache_file = options.get("token_cache_file")

            # Initialize token and session
            self._access_token = None
            self._session = requests.Session()

            # Try to get token from various sources
            access_token = options.get("access_token")
            username = options.get("username")
            password = options.get("password")

            if access_token:
                # Direct token provided
                self._access_token = access_token
            elif username and password:
                # Generate token using OAuth
                self._access_token = self._generate_oauth_token(username, password)
            else:
                raise ValueError(
                    "Particle connector requires authentication. Provide either:\n"
                    "  - access_token: Pre-generated token from Particle console\n"
                    "  - username + password: To automatically generate token via OAuth"
                )

            # Configure session with the token
            self._session.headers.update(
                {
                    "Authorization": f"Bearer {self._access_token}",
                    "Content-Type": "application/json",
                }
            )

        def _generate_oauth_token(self, username: str, password: str) -> str:
            """
            Generate an access token using Particle OAuth password grant flow.

            Args:
                username: Particle account email
                password: Particle account password

            Returns:
                Access token string

            Raises:
                ValueError: If token generation fails
            """
            # Check cache first
            if self.token_cache_file and self._load_cached_token():
                return self._access_token

            url = f"{self.base_url}/oauth/token"
            auth = ("particle", "particle")  # Standard Particle OAuth client

            data = {
                "grant_type": "password",
                "username": username,
                "password": password
            }

            try:
                # For OAuth token generation, disable SSL verification if there are issues
                # This helps in environments where SSL certificates might not be properly configured
                response = self._session.post(url, auth=auth, data=data, timeout=30, verify=False)

                if response.status_code == 200:
                    token_data = response.json()
                    access_token = token_data.get("access_token")

                    if not access_token:
                        raise ValueError("No access_token in OAuth response")

                    # Cache the token
                    if self.token_cache_file:
                        self._cache_token(access_token)

                    return access_token
                else:
                    error_msg = response.json().get("error_description", response.text)
                    raise ValueError(f"OAuth token generation failed: {error_msg}")

            except requests.RequestException as e:
                raise ValueError(f"Failed to generate OAuth token: {e}")

        def _load_cached_token(self) -> bool:
            """
            Load cached token from file if it exists and is valid.

            Returns:
                True if cached token was loaded successfully
            """
            if not self.token_cache_file:
                return False

            try:
                with open(self.token_cache_file, 'r') as f:
                    cached_data = f.read().strip()
                    if cached_data:
                        # Quick validation - try to use the token
                        self._access_token = cached_data
                        self._session.headers.update({
                            "Authorization": f"Bearer {self._access_token}"
                        })

                        # Test the token with a simple request
                        test_response = self._session.get(f"{self.base_url}/v1/user", timeout=10)
                        if test_response.status_code == 200:
                            return True
                        else:
                            # Token is invalid, remove cache file
                            os.remove(self.token_cache_file)
                            return False
            except (FileNotFoundError, requests.RequestException, OSError):
                return False

            return False

        def _cache_token(self, token: str) -> None:
            """Cache the token to file."""
            if self.token_cache_file:
                try:
                    os.makedirs(os.path.dirname(self.token_cache_file), exist_ok=True)
                    with open(self.token_cache_file, 'w') as f:
                        f.write(token)
                except OSError:
                    # Ignore cache write failures
                    pass

        def list_tables(self) -> List[str]:
            """
            List names of all tables supported by this connector.
            """
            return [
                "devices",
                "products",
                "sims",
                "user",
                "oauth_clients",
                "product_devices",
                "product_sims",
                "diagnostics",
                "sim_data_usage",
                "fleet_data_usage",
            ]

        # -------------------------------------------------------------------------
        # Schema Definitions
        # -------------------------------------------------------------------------

        def _get_devices_schema(self) -> StructType:
            """Return the devices table schema."""
            return StructType(
                [
                    StructField("id", StringType(), False),
                    StructField("name", StringType(), True),
                    StructField("owner", StringType(), True),
                    StructField("last_ip_address", StringType(), True),
                    StructField("last_heard", StringType(), True),
                    StructField("online", BooleanType(), True),
                    StructField("platform_id", LongType(), True),
                    StructField("cellular", BooleanType(), True),
                    StructField("notes", StringType(), True),
                    StructField("functions", ArrayType(StringType(), True), True),
                    StructField("variables", MapType(StringType(), StringType(), True), True),
                    StructField("status", StringType(), True),
                    StructField("serial_number", StringType(), True),
                    StructField("iccid", StringType(), True),
                    StructField("imei", StringType(), True),
                    StructField("mac_wifi", StringType(), True),
                    StructField("mobile_secret", StringType(), True),
                    StructField("system_firmware_version", StringType(), True),
                    StructField("firmware_updates_enabled", BooleanType(), True),
                    StructField("firmware_updates_forced", BooleanType(), True),
                    StructField("product_id", LongType(), True),
                ]
            )

        def _get_products_schema(self) -> StructType:
            """Return the products table schema."""
            return StructType(
                [
                    StructField("id", LongType(), False),
                    StructField("name", StringType(), True),
                    StructField("slug", StringType(), True),
                    StructField("description", StringType(), True),
                    StructField("platform_id", LongType(), True),
                    StructField("organization", StringType(), True),
                    StructField("type", StringType(), True),
                    StructField("hardware_version", StringType(), True),
                    StructField("config_id", StringType(), True),
                    StructField("settings", MapType(StringType(), StringType(), True), True),
                ]
            )

        def _get_sims_schema(self) -> StructType:
            """Return the SIM cards table schema."""
            return StructType(
                [
                    StructField("_id", StringType(), False),
                    StructField("status", StringType(), True),
                    StructField("base_country_code", StringType(), True),
                    StructField("carrier", StringType(), True),
                    StructField("first_activated_on", StringType(), True),
                    StructField("last_device_id", StringType(), True),
                    StructField("last_device_name", StringType(), True),
                    StructField("user_id", StringType(), True),
                    StructField("product_id", LongType(), True),
                    StructField("data_limit", LongType(), True),
                    StructField("overage_enabled", BooleanType(), True),
                ]
            )

        def _get_user_schema(self) -> StructType:
            """Return the user table schema."""
            account_info_struct = StructType(
                [
                    StructField("first_name", StringType(), True),
                    StructField("last_name", StringType(), True),
                    StructField("business_account", BooleanType(), True),
                    StructField("company_name", StringType(), True),
                ]
            )
            mfa_struct = StructType(
                [
                    StructField("enabled", BooleanType(), True),
                ]
            )
            return StructType(
                [
                    StructField("username", StringType(), False),
                    StructField("subscription_ids", ArrayType(StringType(), True), True),
                    StructField("account_info", account_info_struct, True),
                    StructField("mfa", mfa_struct, True),
                    StructField("wifi_device_count", LongType(), True),
                    StructField("cellular_device_count", LongType(), True),
                ]
            )

        def _get_oauth_clients_schema(self) -> StructType:
            """Return the OAuth clients table schema."""
            return StructType(
                [
                    StructField("id", StringType(), False),
                    StructField("name", StringType(), True),
                    StructField("type", StringType(), True),
                    StructField("redirect_uri", StringType(), True),
                    StructField("created_at", StringType(), True),
                ]
            )

        def _get_product_devices_schema(self) -> StructType:
            """Return the product devices table schema."""
            return StructType(
                [
                    StructField("id", StringType(), False),
                    StructField("product_id_or_slug", StringType(), False),
                    StructField("name", StringType(), True),
                    StructField("owner", StringType(), True),
                    StructField("last_ip_address", StringType(), True),
                    StructField("last_heard", StringType(), True),
                    StructField("online", BooleanType(), True),
                    StructField("platform_id", LongType(), True),
                    StructField("cellular", BooleanType(), True),
                    StructField("notes", StringType(), True),
                    StructField("functions", ArrayType(StringType(), True), True),
                    StructField("variables", MapType(StringType(), StringType(), True), True),
                    StructField("status", StringType(), True),
                    StructField("serial_number", StringType(), True),
                    StructField("iccid", StringType(), True),
                    StructField("imei", StringType(), True),
                    StructField("system_firmware_version", StringType(), True),
                    StructField("firmware_version", StringType(), True),
                    StructField("desired_firmware_version", StringType(), True),
                    StructField("targeted_firmware_release_version", StringType(), True),
                    StructField("groups", ArrayType(StringType(), True), True),
                    StructField("development", BooleanType(), True),
                    StructField("quarantined", BooleanType(), True),
                    StructField("denied", BooleanType(), True),
                ]
            )

        def _get_product_sims_schema(self) -> StructType:
            """Return the product SIM cards table schema."""
            return StructType(
                [
                    StructField("_id", StringType(), False),
                    StructField("product_id_or_slug", StringType(), False),
                    StructField("status", StringType(), True),
                    StructField("base_country_code", StringType(), True),
                    StructField("carrier", StringType(), True),
                    StructField("first_activated_on", StringType(), True),
                    StructField("last_device_id", StringType(), True),
                    StructField("last_device_name", StringType(), True),
                    StructField("data_limit", LongType(), True),
                    StructField("overage_enabled", BooleanType(), True),
                ]
            )

        def _get_diagnostics_schema(self) -> StructType:
            """Return the device diagnostics table schema."""
            device_network_struct = StructType(
                [
                    StructField("signal", MapType(StringType(), StringType(), True), True),
                ]
            )
            device_cloud_struct = StructType(
                [
                    StructField("connection", MapType(StringType(), StringType(), True), True),
                    StructField("coap", MapType(StringType(), StringType(), True), True),
                    StructField("publish", MapType(StringType(), StringType(), True), True),
                ]
            )
            device_system_struct = StructType(
                [
                    StructField("uptime", LongType(), True),
                    StructField("memory", MapType(StringType(), StringType(), True), True),
                ]
            )
            device_struct = StructType(
                [
                    StructField("network", device_network_struct, True),
                    StructField("cloud", device_cloud_struct, True),
                    StructField("system", device_system_struct, True),
                    StructField("power", MapType(StringType(), StringType(), True), True),
                ]
            )
            payload_struct = StructType(
                [
                    StructField("device", device_struct, True),
                    StructField("service", MapType(StringType(), StringType(), True), True),
                ]
            )
            return StructType(
                [
                    StructField("device_id", StringType(), False),
                    StructField("updated_at", StringType(), True),
                    StructField("payload", payload_struct, True),
                ]
            )

        def _get_sim_data_usage_schema(self) -> StructType:
            """Return the SIM data usage table schema."""
            usage_day_struct = StructType(
                [
                    StructField("date", StringType(), True),
                    StructField("mbs_used", DoubleType(), True),
                    StructField("mbs_used_cumulative", DoubleType(), True),
                ]
            )
            return StructType(
                [
                    StructField("iccid", StringType(), False),
                    StructField("usage_by_day", ArrayType(usage_day_struct, True), True),
                ]
            )

        def _get_fleet_data_usage_schema(self) -> StructType:
            """Return the fleet data usage table schema."""
            usage_day_struct = StructType(
                [
                    StructField("date", StringType(), True),
                    StructField("mbs_used", DoubleType(), True),
                    StructField("mbs_used_cumulative", DoubleType(), True),
                ]
            )
            return StructType(
                [
                    StructField("product_id_or_slug", StringType(), False),
                    StructField("total_mbs_used", DoubleType(), True),
                    StructField("total_active_sim_cards", LongType(), True),
                    StructField("usage_by_day", ArrayType(usage_day_struct, True), True),
                ]
            )

        def get_table_schema(
            self, table_name: str, table_options: dict[str, str]
        ) -> StructType:
            """
            Fetch the schema of a table.
            """
            schema_map = {
                "devices": self._get_devices_schema,
                "products": self._get_products_schema,
                "sims": self._get_sims_schema,
                "user": self._get_user_schema,
                "oauth_clients": self._get_oauth_clients_schema,
                "product_devices": self._get_product_devices_schema,
                "product_sims": self._get_product_sims_schema,
                "diagnostics": self._get_diagnostics_schema,
                "sim_data_usage": self._get_sim_data_usage_schema,
                "fleet_data_usage": self._get_fleet_data_usage_schema,
            }

            if table_name not in schema_map:
                raise ValueError(f"Unsupported table: {table_name!r}")
            return schema_map[table_name]()

        # -------------------------------------------------------------------------
        # Metadata Definitions
        # -------------------------------------------------------------------------

        def read_table_metadata(
            self, table_name: str, table_options: dict[str, str]
        ) -> dict:
            """
            Fetch metadata for the given table.
            """
            metadata_map = {
                "devices": {
                    "primary_keys": ["id"],
                    "ingestion_type": "snapshot",
                },
                "products": {
                    "primary_keys": ["id"],
                    "ingestion_type": "snapshot",
                },
                "sims": {
                    "primary_keys": ["_id"],
                    "ingestion_type": "snapshot",
                },
                "user": {
                    "primary_keys": ["username"],
                    "ingestion_type": "snapshot",
                },
                "oauth_clients": {
                    "primary_keys": ["id"],
                    "ingestion_type": "snapshot",
                },
                "product_devices": {
                    "primary_keys": ["id", "product_id_or_slug"],
                    "ingestion_type": "snapshot",
                },
                "product_sims": {
                    "primary_keys": ["_id", "product_id_or_slug"],
                    "ingestion_type": "snapshot",
                },
                "diagnostics": {
                    "primary_keys": ["device_id", "updated_at"],
                    "cursor_field": "updated_at",
                    "ingestion_type": "append",
                },
                "sim_data_usage": {
                    "primary_keys": ["iccid"],
                    "ingestion_type": "snapshot",
                },
                "fleet_data_usage": {
                    "primary_keys": ["product_id_or_slug"],
                    "ingestion_type": "snapshot",
                },
            }

            if table_name not in metadata_map:
                raise ValueError(f"Unsupported table: {table_name!r}")
            return metadata_map[table_name]

        # -------------------------------------------------------------------------
        # Table Reader Methods
        # -------------------------------------------------------------------------

        def read_table(
            self, table_name: str, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read records from a table and return raw JSON-like dictionaries.
            """
            reader_map = {
                "devices": self._read_devices,
                "products": self._read_products,
                "sims": self._read_sims,
                "user": self._read_user,
                "oauth_clients": self._read_oauth_clients,
                "product_devices": self._read_product_devices,
                "product_sims": self._read_product_sims,
                "diagnostics": self._read_diagnostics,
                "sim_data_usage": self._read_sim_data_usage,
                "fleet_data_usage": self._read_fleet_data_usage,
            }

            if table_name not in reader_map:
                raise ValueError(f"Unsupported table: {table_name!r}")
            return reader_map[table_name](start_offset, table_options)

        def _read_devices(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the devices snapshot table using GET /v1/devices.
            """
            url = f"{self.base_url}/v1/devices"

            response = self._session.get(url, timeout=30)
            if response.status_code != 200:
                raise RuntimeError(
                    f"Particle API error for devices: {response.status_code} {response.text}"
                )

            devices = response.json() or []
            if not isinstance(devices, list):
                raise ValueError(
                    f"Unexpected response format for devices: {type(devices).__name__}"
                )

            records: List[Dict[str, Any]] = []
            for device in devices:
                record: Dict[str, Any] = dict(device)
                records.append(record)

            return iter(records), {}

        def _read_products(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the products snapshot table using GET /v1/products.
            """
            url = f"{self.base_url}/v1/products"

            response = self._session.get(url, timeout=30)
            if response.status_code != 200:
                raise RuntimeError(
                    f"Particle API error for products: {response.status_code} {response.text}"
                )

            data = response.json() or {}
            products = data.get("products", [])
            if not isinstance(products, list):
                raise ValueError(
                    f"Unexpected response format for products: {type(products).__name__}"
                )

            records: List[Dict[str, Any]] = []
            for product in products:
                record: Dict[str, Any] = dict(product)
                records.append(record)

            return iter(records), {}

        def _read_sims(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the SIM cards snapshot table using GET /v1/sims.
            """
            url = f"{self.base_url}/v1/sims"

            response = self._session.get(url, timeout=30)
            if response.status_code != 200:
                raise RuntimeError(
                    f"Particle API error for sims: {response.status_code} {response.text}"
                )

            data = response.json() or {}
            sims = data.get("sims", [])
            if not isinstance(sims, list):
                # Handle case where response is a list directly
                if isinstance(data, list):
                    sims = data
                else:
                    raise ValueError(
                        f"Unexpected response format for sims: {type(data).__name__}"
                    )

            records: List[Dict[str, Any]] = []
            for sim in sims:
                record: Dict[str, Any] = dict(sim)
                records.append(record)

            return iter(records), {}

        def _read_user(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the user snapshot table using GET /v1/user.
            """
            url = f"{self.base_url}/v1/user"

            response = self._session.get(url, timeout=30)
            if response.status_code != 200:
                raise RuntimeError(
                    f"Particle API error for user: {response.status_code} {response.text}"
                )

            user_obj = response.json() or {}
            if not isinstance(user_obj, dict):
                raise ValueError(
                    f"Unexpected response format for user: {type(user_obj).__name__}"
                )

            record: Dict[str, Any] = dict(user_obj)
            return iter([record]), {}

        def _read_oauth_clients(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the OAuth clients snapshot table using GET /v1/clients.
            """
            url = f"{self.base_url}/v1/clients"

            response = self._session.get(url, timeout=30)
            if response.status_code != 200:
                raise RuntimeError(
                    f"Particle API error for oauth_clients: {response.status_code} {response.text}"
                )

            data = response.json() or {}
            clients = data.get("clients", [])
            if not isinstance(clients, list):
                raise ValueError(
                    f"Unexpected response format for oauth_clients: {type(clients).__name__}"
                )

            records: List[Dict[str, Any]] = []
            for client in clients:
                record: Dict[str, Any] = dict(client)
                records.append(record)

            return iter(records), {}

        def _read_product_devices(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the product devices snapshot table using
            GET /v1/products/:productIdOrSlug/devices.

            Required table_options:
                - product_id_or_slug: The product ID or slug to list devices for.

            Optional table_options:
                - per_page: Records per page (default 25).
                - groups: Filter by group names (comma-separated).
                - device_name: Filter by device name (partial match).
                - device_id: Filter by device ID (partial match).
                - serial_number: Filter by serial number (partial match).
                - sort_attr: Sort by attribute (deviceName, deviceId, firmwareVersion, lastConnection).
                - sort_dir: Sort direction (asc, desc).
            """
            product_id_or_slug = table_options.get("product_id_or_slug")
            if not product_id_or_slug:
                raise ValueError(
                    "table_options for 'product_devices' must include 'product_id_or_slug'"
                )

            try:
                per_page = int(table_options.get("per_page", 100))
            except (TypeError, ValueError):
                per_page = 100
            per_page = max(1, min(per_page, 100))

            try:
                max_pages = int(table_options.get("max_pages", 100))
            except (TypeError, ValueError):
                max_pages = 100

            url = f"{self.base_url}/v1/products/{product_id_or_slug}/devices"
            params: Dict[str, Any] = {"perPage": per_page}

            # Optional filters
            if table_options.get("groups"):
                params["groups"] = table_options["groups"]
            if table_options.get("device_name"):
                params["deviceName"] = table_options["device_name"]
            if table_options.get("device_id"):
                params["deviceId"] = table_options["device_id"]
            if table_options.get("serial_number"):
                params["serialNumber"] = table_options["serial_number"]
            if table_options.get("sort_attr"):
                params["sortAttr"] = table_options["sort_attr"]
            if table_options.get("sort_dir"):
                params["sortDir"] = table_options["sort_dir"]

            records: List[Dict[str, Any]] = []
            page = 1

            while page <= max_pages:
                params["page"] = page
                response = self._session.get(url, params=params, timeout=30)
                if response.status_code != 200:
                    raise RuntimeError(
                        f"Particle API error for product_devices: "
                        f"{response.status_code} {response.text}"
                    )

                data = response.json() or {}
                devices = data.get("devices", [])
                if not isinstance(devices, list):
                    raise ValueError(
                        f"Unexpected response format for product_devices: "
                        f"{type(devices).__name__}"
                    )

                if not devices:
                    break

                for device in devices:
                    record: Dict[str, Any] = dict(device)
                    record["product_id_or_slug"] = product_id_or_slug
                    records.append(record)

                # Check if we've reached the end
                if len(devices) < per_page:
                    break

                page += 1

            return iter(records), {}

        def _read_product_sims(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the product SIM cards snapshot table using
            GET /v1/products/:productIdOrSlug/sims.

            Required table_options:
                - product_id_or_slug: The product ID or slug to list SIMs for.

            Optional table_options:
                - per_page: Records per page (default 25).
                - iccid: Filter by ICCID (partial match).
                - device_id: Filter by associated device ID.
                - device_name: Filter by associated device name.
            """
            product_id_or_slug = table_options.get("product_id_or_slug")
            if not product_id_or_slug:
                raise ValueError(
                    "table_options for 'product_sims' must include 'product_id_or_slug'"
                )

            try:
                per_page = int(table_options.get("per_page", 100))
            except (TypeError, ValueError):
                per_page = 100
            per_page = max(1, min(per_page, 100))

            try:
                max_pages = int(table_options.get("max_pages", 100))
            except (TypeError, ValueError):
                max_pages = 100

            url = f"{self.base_url}/v1/products/{product_id_or_slug}/sims"
            params: Dict[str, Any] = {"perPage": per_page}

            # Optional filters
            if table_options.get("iccid"):
                params["iccid"] = table_options["iccid"]
            if table_options.get("device_id"):
                params["deviceId"] = table_options["device_id"]
            if table_options.get("device_name"):
                params["deviceName"] = table_options["device_name"]

            records: List[Dict[str, Any]] = []
            page = 1

            while page <= max_pages:
                params["page"] = page
                response = self._session.get(url, params=params, timeout=30)
                if response.status_code != 200:
                    raise RuntimeError(
                        f"Particle API error for product_sims: "
                        f"{response.status_code} {response.text}"
                    )

                data = response.json() or {}
                sims = data.get("sims", [])
                if not isinstance(sims, list):
                    raise ValueError(
                        f"Unexpected response format for product_sims: "
                        f"{type(sims).__name__}"
                    )

                if not sims:
                    break

                for sim in sims:
                    record: Dict[str, Any] = dict(sim)
                    record["product_id_or_slug"] = product_id_or_slug
                    records.append(record)

                # Check if we've reached the end
                if len(sims) < per_page:
                    break

                page += 1

            return iter(records), {}

        def _read_diagnostics(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the device diagnostics append table using
            GET /v1/diagnostics/:deviceId.

            Required table_options:
                - device_id: The device ID to get diagnostics for.

            Optional table_options:
                - start_date: Oldest diagnostic to return (ISO8601 format).
                - end_date: Newest diagnostic to return (ISO8601 format).

            Incremental support:
                - Uses start_offset["cursor"] as the start_date for incremental reads.
                - Returns the latest updated_at as the next cursor.
            """
            device_id = table_options.get("device_id")
            if not device_id:
                raise ValueError(
                    "table_options for 'diagnostics' must include 'device_id'"
                )

            url = f"{self.base_url}/v1/diagnostics/{device_id}"
            params: Dict[str, Any] = {}

            # Handle incremental cursor
            cursor = None
            if start_offset and isinstance(start_offset, dict):
                cursor = start_offset.get("cursor")
            if cursor:
                params["start_date"] = cursor
            elif table_options.get("start_date"):
                params["start_date"] = table_options["start_date"]

            if table_options.get("end_date"):
                params["end_date"] = table_options["end_date"]

            response = self._session.get(url, params=params, timeout=30)
            if response.status_code != 200:
                raise RuntimeError(
                    f"Particle API error for diagnostics: "
                    f"{response.status_code} {response.text}"
                )

            data = response.json() or {}
            diagnostics_list = data.get("diagnostics", [])

            # Handle single diagnostic response (from /last endpoint compatibility)
            if isinstance(diagnostics_list, dict):
                diagnostics_list = [diagnostics_list]

            if not isinstance(diagnostics_list, list):
                raise ValueError(
                    f"Unexpected response format for diagnostics: "
                    f"{type(diagnostics_list).__name__}"
                )

            records: List[Dict[str, Any]] = []
            max_updated_at: str | None = None

            for diagnostic in diagnostics_list:
                record: Dict[str, Any] = {
                    "device_id": device_id,
                    "updated_at": diagnostic.get("updated_at"),
                    "payload": diagnostic.get("payload"),
                }
                records.append(record)

                updated_at = diagnostic.get("updated_at")
                if isinstance(updated_at, str):
                    if max_updated_at is None or updated_at > max_updated_at:
                        max_updated_at = updated_at

            # Return next cursor
            next_cursor = cursor
            if max_updated_at:
                next_cursor = max_updated_at

            if not records and start_offset:
                next_offset = start_offset
            else:
                next_offset = {"cursor": next_cursor} if next_cursor else {}

            return iter(records), next_offset

        def _read_sim_data_usage(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the SIM data usage snapshot table using
            GET /v1/sims/:iccid/data_usage.

            Required table_options:
                - iccid: The SIM ICCID to get data usage for.
            """
            iccid = table_options.get("iccid")
            if not iccid:
                raise ValueError(
                    "table_options for 'sim_data_usage' must include 'iccid'"
                )

            url = f"{self.base_url}/v1/sims/{iccid}/data_usage"

            response = self._session.get(url, timeout=30)
            if response.status_code != 200:
                raise RuntimeError(
                    f"Particle API error for sim_data_usage: "
                    f"{response.status_code} {response.text}"
                )

            data = response.json() or {}
            if not isinstance(data, dict):
                raise ValueError(
                    f"Unexpected response format for sim_data_usage: "
                    f"{type(data).__name__}"
                )

            record: Dict[str, Any] = {
                "iccid": data.get("iccid", iccid),
                "usage_by_day": data.get("usage_by_day", []),
            }

            return iter([record]), {}

        def _read_fleet_data_usage(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the fleet data usage snapshot table using
            GET /v1/products/:productIdOrSlug/sims/data_usage.

            Required table_options:
                - product_id_or_slug: The product ID or slug to get fleet data usage for.
            """
            product_id_or_slug = table_options.get("product_id_or_slug")
            if not product_id_or_slug:
                raise ValueError(
                    "table_options for 'fleet_data_usage' must include 'product_id_or_slug'"
                )

            url = f"{self.base_url}/v1/products/{product_id_or_slug}/sims/data_usage"

            response = self._session.get(url, timeout=30)
            if response.status_code != 200:
                raise RuntimeError(
                    f"Particle API error for fleet_data_usage: "
                    f"{response.status_code} {response.text}"
                )

            data = response.json() or {}
            if not isinstance(data, dict):
                raise ValueError(
                    f"Unexpected response format for fleet_data_usage: "
                    f"{type(data).__name__}"
                )

            record: Dict[str, Any] = {
                "product_id_or_slug": product_id_or_slug,
                "total_mbs_used": data.get("total_mbs_used"),
                "total_active_sim_cards": data.get("total_active_sim_cards"),
                "usage_by_day": data.get("usage_by_day", []),
            }

            return iter([record]), {}


    ########################################################
    # pipeline/lakeflow_python_source.py
    ########################################################

    METADATA_TABLE = "_lakeflow_metadata"
    TABLE_NAME = "tableName"
    TABLE_NAME_LIST = "tableNameList"
    TABLE_CONFIGS = "tableConfigs"
    IS_DELETE_FLOW = "isDeleteFlow"


    class LakeflowStreamReader(SimpleDataSourceStreamReader):
        """
        Implements a data source stream reader for Lakeflow Connect.
        Currently, only the simpleStreamReader is implemented, which uses a
        more generic protocol suitable for most data sources that support
        incremental loading.
        """

        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.lakeflow_connect = lakeflow_connect
            self.schema = schema

        def initialOffset(self):
            return {}

        def read(self, start: dict) -> (Iterator[tuple], dict):
            is_delete_flow = self.options.get(IS_DELETE_FLOW) == "true"
            # Strip delete flow options before passing to connector
            table_options = {
                k: v for k, v in self.options.items() if k != IS_DELETE_FLOW
            }

            if is_delete_flow:
                records, offset = self.lakeflow_connect.read_table_deletes(
                    self.options[TABLE_NAME], start, table_options
                )
            else:
                records, offset = self.lakeflow_connect.read_table(
                    self.options[TABLE_NAME], start, table_options
                )
            rows = map(lambda x: parse_value(x, self.schema), records)
            return rows, offset

        def readBetweenOffsets(self, start: dict, end: dict) -> Iterator[tuple]:
            # TODO: This does not ensure the records returned are identical across repeated calls.
            # For append-only tables, the data source must guarantee that reading from the same
            # start offset will always yield the same set of records.
            # For tables ingested as incremental CDC, it is only necessary that no new changes
            # are missed in the returned records.
            return self.read(start)[0]


    class LakeflowBatchReader(DataSourceReader):
        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.schema = schema
            self.lakeflow_connect = lakeflow_connect
            self.table_name = options[TABLE_NAME]

        def read(self, partition):
            all_records = []
            if self.table_name == METADATA_TABLE:
                all_records = self._read_table_metadata()
            else:
                all_records, _ = self.lakeflow_connect.read_table(
                    self.table_name, None, self.options
                )

            rows = map(lambda x: parse_value(x, self.schema), all_records)
            return iter(rows)

        def _read_table_metadata(self):
            table_name_list = self.options.get(TABLE_NAME_LIST, "")
            table_names = [o.strip() for o in table_name_list.split(",") if o.strip()]
            all_records = []
            table_configs = json.loads(self.options.get(TABLE_CONFIGS, "{}"))
            for table in table_names:
                metadata = self.lakeflow_connect.read_table_metadata(
                    table, table_configs.get(table, {})
                )
                all_records.append({TABLE_NAME: table, **metadata})
            return all_records


    class LakeflowSource(DataSource):
        def __init__(self, options):
            self.options = options
            self.lakeflow_connect = LakeflowConnect(options)

        @classmethod
        def name(cls):
            return "lakeflow_connect"

        def schema(self):
            table = self.options[TABLE_NAME]
            if table == METADATA_TABLE:
                return StructType(
                    [
                        StructField(TABLE_NAME, StringType(), False),
                        StructField("primary_keys", ArrayType(StringType()), True),
                        StructField("cursor_field", StringType(), True),
                        StructField("ingestion_type", StringType(), True),
                    ]
                )
            else:
                # Assuming the LakeflowConnect interface uses get_table_schema, not get_table_details
                return self.lakeflow_connect.get_table_schema(table, self.options)

        def reader(self, schema: StructType):
            return LakeflowBatchReader(self.options, schema, self.lakeflow_connect)

        def simpleStreamReader(self, schema: StructType):
            return LakeflowStreamReader(self.options, schema, self.lakeflow_connect)


    spark.dataSource.register(LakeflowSource)  # pylint: disable=undefined-variable
