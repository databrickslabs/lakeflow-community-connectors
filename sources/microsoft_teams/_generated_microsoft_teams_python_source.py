# ==============================================================================
# Merged Lakeflow Source: microsoft_teams
# ==============================================================================
# This file is auto-generated by tools/scripts/merge_python_source.py
# Do not edit manually. Make changes to the source files instead.
# ==============================================================================

from datetime import datetime, timedelta
from decimal import Decimal
from typing import (
    Any,
    Dict,
    Iterator,
    List,
    Tuple,
)
import json
import time

from concurrent.futures import ThreadPoolExecutor, as_completed
from pyspark.sql import Row
from pyspark.sql.datasource import DataSource, DataSourceReader, SimpleDataSourceStreamReader
from pyspark.sql.types import *
import base64
import requests


def register_lakeflow_source(spark):
    """Register the Lakeflow Python source with Spark."""

    ########################################################
    # libs/utils.py
    ########################################################

    def _parse_struct(value: Any, field_type: StructType) -> Row:
        """Parse a dictionary into a PySpark Row based on StructType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for StructType, got {type(value)}")
        # Spark Python -> Arrow conversion require missing StructType fields to be assigned None.
        if value == {}:
            raise ValueError(
                "field in StructType cannot be an empty dict. "
                "Please assign None as the default value instead."
            )
        field_dict = {}
        for field in field_type.fields:
            if field.name in value:
                field_dict[field.name] = parse_value(value.get(field.name), field.dataType)
            elif field.nullable:
                field_dict[field.name] = None
            else:
                raise ValueError(f"Field {field.name} is not nullable but not found in the input")
        return Row(**field_dict)


    def _parse_array(value: Any, field_type: ArrayType) -> list:
        """Parse a list into a PySpark array based on ArrayType schema."""
        if not isinstance(value, list):
            if field_type.containsNull:
                return [parse_value(value, field_type.elementType)]
            raise ValueError(f"Expected a list for ArrayType, got {type(value)}")
        return [parse_value(v, field_type.elementType) for v in value]


    def _parse_map(value: Any, field_type: MapType) -> dict:
        """Parse a dictionary into a PySpark map based on MapType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for MapType, got {type(value)}")
        return {
            parse_value(k, field_type.keyType): parse_value(v, field_type.valueType)
            for k, v in value.items()
        }


    def _parse_string(value: Any) -> str:
        """Convert value to string."""
        return str(value)


    def _parse_integer(value: Any) -> int:
        """Convert value to integer."""
        if isinstance(value, str) and value.strip():
            return int(float(value)) if "." in value else int(value)
        if isinstance(value, (int, float)):
            return int(value)
        raise ValueError(f"Cannot convert {value} to integer")


    def _parse_float(value: Any) -> float:
        """Convert value to float."""
        return float(value)


    def _parse_decimal(value: Any) -> Decimal:
        """Convert value to Decimal."""
        return Decimal(value) if isinstance(value, str) and value.strip() else Decimal(str(value))


    def _parse_boolean(value: Any) -> bool:
        """Convert value to boolean."""
        if isinstance(value, str):
            lowered = value.lower()
            if lowered in ("true", "t", "yes", "y", "1"):
                return True
            if lowered in ("false", "f", "no", "n", "0"):
                return False
        return bool(value)


    def _parse_date(value: Any) -> datetime.date:
        """Convert value to date."""
        if isinstance(value, str):
            for fmt in ("%Y-%m-%d", "%m/%d/%Y", "%d-%m-%Y", "%Y/%m/%d"):
                try:
                    return datetime.strptime(value, fmt).date()
                except ValueError:
                    continue
            return datetime.fromisoformat(value).date()
        if isinstance(value, datetime):
            return value.date()
        raise ValueError(f"Cannot convert {value} to date")


    def _parse_timestamp(value: Any) -> datetime:
        """Convert value to timestamp."""
        if isinstance(value, str):
            ts_value = value.replace("Z", "+00:00") if value.endswith("Z") else value
            try:
                return datetime.fromisoformat(ts_value)
            except ValueError:
                for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S"):
                    try:
                        return datetime.strptime(ts_value, fmt)
                    except ValueError:
                        continue
        elif isinstance(value, (int, float)):
            return datetime.fromtimestamp(value)
        elif isinstance(value, datetime):
            return value
        raise ValueError(f"Cannot convert {value} to timestamp")


    def _decode_string_to_bytes(value: str) -> bytes:
        """Try to decode a string as base64, then hex, then UTF-8."""
        try:
            return base64.b64decode(value)
        except Exception:
            pass
        try:
            return bytes.fromhex(value)
        except Exception:
            pass
        return value.encode("utf-8")


    def _parse_binary(value: Any) -> bytes:
        """Convert value to bytes. Tries base64, then hex, then UTF-8 for strings."""
        if isinstance(value, bytes):
            return value
        if isinstance(value, bytearray):
            return bytes(value)
        if isinstance(value, str):
            return _decode_string_to_bytes(value)
        if isinstance(value, list):
            return bytes(value)
        return str(value).encode("utf-8")


    # Mapping of primitive types to their parser functions
    _PRIMITIVE_PARSERS = {
        StringType: _parse_string,
        IntegerType: _parse_integer,
        LongType: _parse_integer,
        FloatType: _parse_float,
        DoubleType: _parse_float,
        DecimalType: _parse_decimal,
        BooleanType: _parse_boolean,
        DateType: _parse_date,
        TimestampType: _parse_timestamp,
        BinaryType: _parse_binary,
    }


    def parse_value(value: Any, field_type: DataType) -> Any:
        """
        Converts a JSON value into a PySpark-compatible data type based on the provided field type.
        """
        if value is None:
            return None

        # Handle complex types
        if isinstance(field_type, StructType):
            return _parse_struct(value, field_type)
        if isinstance(field_type, ArrayType):
            return _parse_array(value, field_type)
        if isinstance(field_type, MapType):
            return _parse_map(value, field_type)

        # Handle primitive types via type-based lookup
        try:
            field_type_class = type(field_type)
            if field_type_class in _PRIMITIVE_PARSERS:
                return _PRIMITIVE_PARSERS[field_type_class](value)

            # Check for custom UDT handling
            if hasattr(field_type, "fromJson"):
                return field_type.fromJson(value)

            raise TypeError(f"Unsupported field type: {field_type}")
        except (ValueError, TypeError) as e:
            raise ValueError(f"Error converting '{value}' ({type(value)}) to {field_type}: {str(e)}")


    ########################################################
    # sources/microsoft_teams/microsoft_teams.py
    ########################################################

    class LakeflowConnect:
        def __init__(self, options: dict[str, str]) -> None:
            """
            Initialize Microsoft Teams connector with OAuth 2.0 credentials.

            Required options (stored in UC Connection properties):
              - tenant_id: Azure AD tenant ID
              - client_id: Application (client) ID
              - client_secret: Client secret value

            Credentials should be stored in the Unity Catalog Connection using the
            {{secrets/scope/key}} format to reference Databricks Secrets.

            Authentication uses the Client Credentials Flow with Application Permissions.
            Requires admin consent for all permissions.
            """
            self.tenant_id = options.get("tenant_id")
            self.client_id = options.get("client_id")
            self.client_secret = options.get("client_secret")

            # NOTE: We do NOT validate credentials here.
            # This allows the connector to initialize for metadata discovery (_lakeflow_metadata)
            # where credentials are passed from the UC Connection properties.
            # Strict validation happens in _get_access_token() when we actually need to connect to Teams API.

            self.base_url = "https://graph.microsoft.com/v1.0"
            self._access_token = None
            self._token_expiry = None

            # Centralized object metadata configuration (following Stripe pattern)
            self._object_config = {
                "teams": {
                    "primary_keys": ["id"],
                    "ingestion_type": "snapshot",
                    "endpoint": "groups?$filter=resourceProvisioningOptions/Any(x:x eq 'Team')",
                    # Required permission: Team.ReadBasic.All (Application)
                    # Note: Using /groups with filter for Application Permissions (not /me/joinedTeams)
                },
                "channels": {
                    "primary_keys": ["id"],
                    "ingestion_type": "snapshot",
                    "endpoint": "teams/{team_id}/channels",
                    "requires_parent": [],  # team_id optional with fetch_all_teams
                    # Required permission: Channel.ReadBasic.All (Application)
                    # Supports fetch_all_teams=true to auto-discover all teams
                },
                "messages": {
                    "primary_keys": ["id"],
                    "cursor_field": "lastModifiedDateTime",
                    "ingestion_type": "cdc",
                    "endpoint": "teams/{team_id}/channels/{channel_id}/messages",
                    "requires_parent": ["team_id"],  # channel_id optional with fetch_all_channels
                    # Required permission: ChannelMessage.Read.All (Application)
                    # Supports fetch_all_channels=true to auto-discover all channels in team
                },
                "members": {
                    "primary_keys": ["id"],
                    "ingestion_type": "snapshot",
                    "endpoint": "teams/{team_id}/members",
                    "requires_parent": [],  # team_id optional with fetch_all_teams
                    # Required permission: TeamMember.Read.All (Application)
                    # Supports fetch_all_teams=true to auto-discover all teams
                },
                "message_replies": {
                    "primary_keys": ["id"],
                    "cursor_field": "lastModifiedDateTime",
                    "ingestion_type": "cdc",
                    "endpoint": "teams/{team_id}/channels/{channel_id}/messages/{message_id}/replies",
                    "requires_parent": ["team_id", "channel_id"],  # message_id optional with fetch_all_messages
                    # Required permission: ChannelMessage.Read.All (Application) - same as messages
                    # Supports fetch_all_messages=true to auto-discover all messages in channel
                },
            }

            # Reusable nested schemas (following Stripe pattern)
            self._identity_set_schema = StructType(
                [
                    StructField("application", StringType(), True),
                    StructField("device", StringType(), True),
                    StructField(
                        "user",
                        StructType(
                            [
                                StructField("id", StringType(), True),
                                StructField("displayName", StringType(), True),
                                StructField("userIdentityType", StringType(), True),
                            ]
                        ),
                        True,
                    ),
                ]
            )

            self._body_schema = StructType(
                [
                    StructField("contentType", StringType(), True),
                    StructField("content", StringType(), True),
                ]
            )

            self._attachment_schema = StructType(
                [
                    StructField("id", StringType(), True),
                    StructField("contentType", StringType(), True),
                    StructField("contentUrl", StringType(), True),
                    StructField("content", StringType(), True),
                    StructField("name", StringType(), True),
                    StructField("thumbnailUrl", StringType(), True),
                ]
            )

            self._mention_schema = StructType(
                [
                    StructField("id", LongType(), True),
                    StructField("mentionText", StringType(), True),
                    StructField("mentioned", self._identity_set_schema, True),
                ]
            )

            self._reaction_schema = StructType(
                [
                    StructField("reactionType", StringType(), True),
                    StructField("createdDateTime", StringType(), True),
                    StructField("user", self._identity_set_schema, True),
                ]
            )

            self._channel_identity_schema = StructType(
                [
                    StructField("teamId", StringType(), True),
                    StructField("channelId", StringType(), True),
                ]
            )

        def _get_access_token(self) -> str:
            """
            Acquire OAuth 2.0 access token using client credentials flow.

            The connector automatically refreshes tokens before expiry.
            Tokens are cached and reused until 5 minutes before expiration.

            Returns:
                str: Access token for Microsoft Graph API

            Raises:
                ValueError: If required credentials are missing
                RuntimeError: If token acquisition fails
            """
            # Validate credentials just-in-time before we need them
            if not self.tenant_id or not self.client_id or not self.client_secret:
                raise ValueError(
                    "Missing required options: tenant_id, client_id, and client_secret are required. "
                    "Pass them in the connection properties or in table_configuration for each table."
                )

            # Return cached token if still valid
            if (
                self._access_token
                and self._token_expiry
                and datetime.utcnow() < self._token_expiry
            ):
                return self._access_token

            # Request new token using client credentials flow
            token_url = (
                f"https://login.microsoftonline.com/{self.tenant_id}/oauth2/v2.0/token"
            )

            data = {
                "client_id": self.client_id,
                "client_secret": self.client_secret,
                "scope": "https://graph.microsoft.com/.default",
                "grant_type": "client_credentials",
            }

            try:
                response = requests.post(token_url, data=data, timeout=30)
                if response.status_code != 200:
                    # Redact sensitive info but show enough to debug
                    tenant_preview = f"{self.tenant_id[:8]}..." if self.tenant_id and len(self.tenant_id) > 8 else "INVALID"
                    raise RuntimeError(
                        f"Token acquisition failed: {response.status_code}\n"
                        f"URL: {token_url}\n"
                        f"Tenant ID (first 8 chars): {tenant_preview}\n"
                        f"Response: {response.text[:500]}"  # First 500 chars
                    )

                token_data = response.json()
                self._access_token = token_data["access_token"]

                # Set expiry 5 minutes before actual expiry for safety
                expires_in = token_data.get("expires_in", 3600)
                self._token_expiry = datetime.utcnow() + timedelta(
                    seconds=expires_in - 300
                )

                return self._access_token

            except requests.RequestException as e:
                raise RuntimeError(f"Token acquisition request failed: {str(e)}")

        def _make_request_with_retry(
            self, url: str, params: dict = None, max_retries: int = 3
        ) -> dict:
            """
            Make HTTP GET request to Microsoft Graph API with exponential backoff retry.

            Implements production-grade error handling:
            - Automatic retry on rate limiting (429) with Retry-After header
            - Exponential backoff on server errors (500, 502, 503)
            - Timeout handling

            Args:
                url: Full URL to request
                params: Query parameters (optional)
                max_retries: Maximum number of retry attempts (default: 3)

            Returns:
                dict: JSON response from API

            Raises:
                RuntimeError: If request fails after all retries
            """
            for attempt in range(max_retries):
                try:
                    headers = {
                        "Authorization": f"Bearer {self._get_access_token()}",
                        "Content-Type": "application/json",
                    }

                    response = requests.get(url, params=params, headers=headers, timeout=30)

                    if response.status_code == 200:
                        return response.json()

                    elif response.status_code == 401:
                        # Token may have expired, clear cache and retry once
                        self._access_token = None
                        self._token_expiry = None
                        if attempt < max_retries - 1:
                            continue
                        raise RuntimeError(
                            f"Authentication failed (401). Please verify credentials and permissions."
                        )

                    elif response.status_code == 403:
                        raise RuntimeError(
                            f"Forbidden (403). Please verify the app has required permissions: {response.text}"
                        )

                    elif response.status_code == 404:
                        raise RuntimeError(
                            f"Resource not found (404). Please verify team_id/channel_id: {response.text}"
                        )

                    elif response.status_code == 429:
                        # Rate limiting - respect Retry-After header
                        retry_after = int(response.headers.get("Retry-After", 60))
                        time.sleep(retry_after)
                        continue

                    elif response.status_code in [500, 502, 503]:
                        # Server errors - exponential backoff
                        if attempt < max_retries - 1:
                            time.sleep(2**attempt)
                            continue
                        raise RuntimeError(
                            f"Server error ({response.status_code}) after {max_retries} retries: {response.text}"
                        )

                    else:
                        raise RuntimeError(
                            f"Request failed with status {response.status_code}: {response.text}"
                        )

                except requests.Timeout:
                    if attempt < max_retries - 1:
                        time.sleep(2**attempt)
                        continue
                    raise RuntimeError(
                        f"Request timeout after {max_retries} attempts: {url}"
                    )

                except requests.RequestException as e:
                    if attempt < max_retries - 1:
                        time.sleep(2**attempt)
                        continue
                    raise RuntimeError(f"Request exception: {str(e)}")

            raise RuntimeError(f"Max retries ({max_retries}) exceeded for: {url}")

        # =========================================================================
        # Helper Methods for Auto-Discovery
        # =========================================================================

        def _fetch_all_team_ids(self, max_pages: int) -> List[str]:
            """
            Fetch all team IDs from the organization.

            Args:
                max_pages: Maximum number of pages to fetch

            Returns:
                List of team ID strings (GUIDs)
            """
            teams_url = f"{self.base_url}/groups?$filter=resourceProvisioningOptions/Any(x:x eq 'Team')"
            teams_params = {"$select": "id"}
            team_ids = []
            pages_fetched = 0
            next_url: str | None = teams_url

            while next_url and pages_fetched < max_pages:
                if pages_fetched == 0:
                    data = self._make_request_with_retry(teams_url, params=teams_params)
                else:
                    data = self._make_request_with_retry(next_url)

                teams = data.get("value", [])
                for team in teams:
                    team_id = team.get("id")
                    if team_id:
                        team_ids.append(team_id)

                next_url = data.get("@odata.nextLink")
                pages_fetched += 1

                if next_url:
                    time.sleep(0.1)

            return team_ids

        def _fetch_all_channel_ids(self, team_id: str, max_pages: int) -> List[str]:
            """
            Fetch all channel IDs for a specific team.

            Args:
                team_id: Team GUID
                max_pages: Maximum number of pages to fetch

            Returns:
                List of channel ID strings (GUIDs)

            Note:
                Returns empty list if team is inaccessible (404/403)
            """
            channels_url = f"{self.base_url}/teams/{team_id}/channels"
            channels_params = {"$select": "id"}
            channel_ids = []
            pages_fetched = 0
            next_url: str | None = channels_url

            try:
                while next_url and pages_fetched < max_pages:
                    if pages_fetched == 0:
                        data = self._make_request_with_retry(channels_url, params=channels_params)
                    else:
                        data = self._make_request_with_retry(next_url)

                    channels = data.get("value", [])
                    for channel in channels:
                        channel_id = channel.get("id")
                        if channel_id:
                            channel_ids.append(channel_id)

                    next_url = data.get("@odata.nextLink")
                    pages_fetched += 1

                    if next_url:
                        time.sleep(0.1)
            except Exception as e:
                # If team is inaccessible, return empty list
                if "404" in str(e) or "403" in str(e):
                    return []
                raise

            return channel_ids

        def _fetch_all_message_ids(self, team_id: str, channel_id: str, max_pages: int) -> List[str]:
            """
            Fetch all message IDs for a specific channel.

            Args:
                team_id: Team GUID
                channel_id: Channel GUID
                max_pages: Maximum number of pages to fetch

            Returns:
                List of message ID strings

            Note:
                Returns empty list if channel is inaccessible (404/403)
            """
            messages_url = f"{self.base_url}/teams/{team_id}/channels/{channel_id}/messages"
            messages_params = {"$top": 50}
            message_ids = []
            pages_fetched = 0
            next_url: str | None = messages_url

            try:
                while next_url and pages_fetched < max_pages:
                    if pages_fetched == 0:
                        data = self._make_request_with_retry(messages_url, params=messages_params)
                    else:
                        data = self._make_request_with_retry(next_url)

                    messages = data.get("value", [])
                    for message in messages:
                        message_id = message.get("id")
                        if message_id:
                            message_ids.append(message_id)

                    next_url = data.get("@odata.nextLink")
                    pages_fetched += 1

                    if next_url:
                        time.sleep(0.1)
            except Exception as e:
                # If channel is inaccessible, return empty list
                if "404" in str(e) or "403" in str(e):
                    return []
                raise

            return message_ids

        # =========================================================================
        # Public Interface Methods
        # =========================================================================

        def list_tables(self) -> list[str]:
            """
            List all supported Microsoft Teams tables.

            Note: Chats table is not supported because the Microsoft Graph API /chats endpoint
            does not support Application Permissions (only Delegated Permissions with user context).

            Reaction tracking is not implemented due to scalability concerns. See README for details.

            Returns:
                list[str]: Static list of table names
            """
            return [
                "teams",
                "channels",
                "messages",
                "members",
                "message_replies",
            ]

        def get_table_schema(
            self, table_name: str, table_options: dict[str, str]
        ) -> StructType:
            """
            Get the PySpark schema for a specific table.

            Args:
                table_name: Name of the table
                table_options: Table-specific options (not used for schema)

            Returns:
                StructType: PySpark schema definition

            Raises:
                ValueError: If table_name is not supported
            """
            if table_name not in self.list_tables():
                raise ValueError(
                    f"Unsupported table: {table_name}. Supported tables: {self.list_tables()}"
                )

            if table_name == "teams":
                return StructType(
                    [
                        StructField("id", StringType(), False),
                        StructField("displayName", StringType(), True),
                        StructField("description", StringType(), True),
                        StructField("classification", StringType(), True),
                        StructField("visibility", StringType(), True),
                        StructField("webUrl", StringType(), True),
                        StructField("isArchived", BooleanType(), True),
                        StructField("createdDateTime", StringType(), True),
                        StructField("internalId", StringType(), True),
                        StructField("tenantId", StringType(), True),
                        StructField("specialization", StringType(), True),
                        # Store complex settings objects as JSON strings for flexibility
                        StructField("memberSettings", StringType(), True),
                        StructField("guestSettings", StringType(), True),
                        StructField("messagingSettings", StringType(), True),
                        StructField("funSettings", StringType(), True),
                    ]
                )

            elif table_name == "channels":
                return StructType(
                    [
                        StructField("id", StringType(), False),
                        StructField("team_id", StringType(), False),  # Connector-derived
                        StructField("displayName", StringType(), True),
                        StructField("description", StringType(), True),
                        StructField("email", StringType(), True),
                        StructField("webUrl", StringType(), True),
                        StructField("membershipType", StringType(), True),
                        StructField("createdDateTime", StringType(), True),
                        StructField("isFavoriteByDefault", BooleanType(), True),
                        StructField("isArchived", BooleanType(), True),
                        StructField("tenantId", StringType(), True),
                    ]
                )

            elif table_name == "messages":
                return StructType(
                    [
                        StructField("id", StringType(), False),
                        StructField(
                            "team_id", StringType(), False
                        ),  # Connector-derived
                        StructField(
                            "channel_id", StringType(), False
                        ),  # Connector-derived
                        StructField("replyToId", StringType(), True),
                        StructField("etag", StringType(), True),
                        StructField("messageType", StringType(), True),
                        StructField("createdDateTime", StringType(), True),
                        StructField("lastModifiedDateTime", StringType(), True),
                        StructField("lastEditedDateTime", StringType(), True),
                        StructField("deletedDateTime", StringType(), True),
                        StructField("subject", StringType(), True),
                        StructField("summary", StringType(), True),
                        StructField("importance", StringType(), True),
                        StructField("locale", StringType(), True),
                        StructField("webUrl", StringType(), True),
                        StructField("from", self._identity_set_schema, True),
                        StructField("body", self._body_schema, True),
                        StructField(
                            "attachments", ArrayType(self._attachment_schema), True
                        ),
                        StructField("mentions", ArrayType(self._mention_schema), True),
                        StructField("reactions", ArrayType(self._reaction_schema), True),
                        StructField("channelIdentity", self._channel_identity_schema, True),
                        # Store complex/polymorphic objects as JSON strings
                        StructField("policyViolation", StringType(), True),
                        StructField("eventDetail", StringType(), True),
                        StructField("messageHistory", StringType(), True),
                    ]
                )

            elif table_name == "members":
                return StructType(
                    [
                        StructField("id", StringType(), False),
                        StructField("team_id", StringType(), False),  # Connector-derived
                        StructField("roles", ArrayType(StringType()), True),
                        StructField("displayName", StringType(), True),
                        StructField("userId", StringType(), True),
                        StructField("email", StringType(), True),
                        StructField("visibleHistoryStartDateTime", StringType(), True),
                        StructField("tenantId", StringType(), True),
                    ]
                )

            elif table_name == "message_replies":
                # Same schema as messages, plus parent_message_id
                return StructType(
                    [
                        StructField("id", StringType(), False),
                        StructField(
                            "parent_message_id", StringType(), False
                        ),  # Connector-derived (NEW!)
                        StructField(
                            "team_id", StringType(), False
                        ),  # Connector-derived
                        StructField(
                            "channel_id", StringType(), False
                        ),  # Connector-derived
                        StructField("replyToId", StringType(), True),
                        StructField("etag", StringType(), True),
                        StructField("messageType", StringType(), True),
                        StructField("createdDateTime", StringType(), True),
                        StructField("lastModifiedDateTime", StringType(), True),
                        StructField("lastEditedDateTime", StringType(), True),
                        StructField("deletedDateTime", StringType(), True),
                        StructField("subject", StringType(), True),
                        StructField("summary", StringType(), True),
                        StructField("importance", StringType(), True),
                        StructField("locale", StringType(), True),
                        StructField("webUrl", StringType(), True),
                        StructField("from", self._identity_set_schema, True),
                        StructField("body", self._body_schema, True),
                        StructField(
                            "attachments", ArrayType(self._attachment_schema), True
                        ),
                        StructField("mentions", ArrayType(self._mention_schema), True),
                        StructField("reactions", ArrayType(self._reaction_schema), True),
                        StructField("channelIdentity", self._channel_identity_schema, True),
                        # Store complex/polymorphic objects as JSON strings
                        StructField("policyViolation", StringType(), True),
                        StructField("eventDetail", StringType(), True),
                        StructField("messageHistory", StringType(), True),
                    ]
                )

        def read_table_metadata(
            self, table_name: str, table_options: dict[str, str]
        ) -> dict:
            """
            Get metadata for a table (primary keys, cursor field, ingestion type).

            Args:
                table_name: Name of the table
                table_options: Table-specific options (not used for metadata)

            Returns:
                dict: Metadata with keys: primary_keys, ingestion_type, cursor_field (if CDC)

            Raises:
                ValueError: If table_name is not supported
            """
            if table_name not in self._object_config:
                raise ValueError(
                    f"Unsupported table: {table_name}. Supported tables: {self.list_tables()}"
                )

            config = self._object_config[table_name]
            metadata = {
                "primary_keys": config["primary_keys"],
                "ingestion_type": config["ingestion_type"],
            }

            # Add cursor field for CDC tables
            if "cursor_field" in config:
                metadata["cursor_field"] = config["cursor_field"]

            return metadata

        def read_table(
            self, table_name: str, start_offset: dict, table_options: dict[str, str]
        ) -> Tuple[Iterator[dict], dict]:
            """
            Read data from a Microsoft Teams table.

            Args:
                table_name: Name of the table to read
                start_offset: Dictionary with cursor for incremental reads (e.g., {"cursor": "2025-01-15T10:30:00.000Z"})
                table_options: Table-specific options (team_id, channel_id, etc.)

            Returns:
                Tuple of (iterator of records, next_offset dict)

            Raises:
                ValueError: If table_name is not supported or required options are missing
            """
            if table_name not in self.list_tables():
                raise ValueError(
                    f"Unsupported table: {table_name}. Supported tables: {self.list_tables()}"
                )

            # Route to table-specific implementation
            if table_name == "teams":
                return self._read_teams(start_offset, table_options)
            elif table_name == "channels":
                return self._read_channels(start_offset, table_options)
            elif table_name == "messages":
                return self._read_messages(start_offset, table_options)
            elif table_name == "members":
                return self._read_members(start_offset, table_options)
            elif table_name == "message_replies":
                return self._read_message_replies(start_offset, table_options)

        def _read_teams(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> Tuple[Iterator[dict], dict]:
            """
            Read teams table (snapshot mode).

            Args:
                start_offset: Not used for snapshot tables
                table_options: Optional table options (top, max_pages_per_batch)

            Returns:
                Tuple of (iterator of team records, empty offset dict)
            """
            # Parse options
            try:
                top = int(table_options.get("top", 50))
            except (TypeError, ValueError):
                top = 50
            top = max(1, min(top, 999))  # Max 999 per Graph API

            try:
                max_pages = int(table_options.get("max_pages_per_batch", 100))
            except (TypeError, ValueError):
                max_pages = 100

            # Build initial request
            endpoint = self._object_config["teams"]["endpoint"]
            url = f"{self.base_url}/{endpoint}"
            params = {"$top": top}

            records: List[dict[str, Any]] = []
            pages_fetched = 0
            next_url: str | None = url

            while next_url and pages_fetched < max_pages:
                # Use URL directly for subsequent pages (contains pagination token)
                if pages_fetched == 0:
                    data = self._make_request_with_retry(url, params=params)
                else:
                    data = self._make_request_with_retry(next_url)

                teams = data.get("value", [])

                for team in teams:
                    # Convert complex nested objects to JSON strings
                    record: dict[str, Any] = dict(team)

                    # Serialize complex settings objects
                    if "memberSettings" in record and isinstance(
                        record["memberSettings"], dict
                    ):
                        record["memberSettings"] = json.dumps(record["memberSettings"])

                    if "guestSettings" in record and isinstance(
                        record["guestSettings"], dict
                    ):
                        record["guestSettings"] = json.dumps(record["guestSettings"])

                    if "messagingSettings" in record and isinstance(
                        record["messagingSettings"], dict
                    ):
                        record["messagingSettings"] = json.dumps(
                            record["messagingSettings"]
                        )

                    if "funSettings" in record and isinstance(record["funSettings"], dict):
                        record["funSettings"] = json.dumps(record["funSettings"])

                    records.append(record)

                # Handle pagination
                next_url = data.get("@odata.nextLink")
                pages_fetched += 1

                # Rate limiting - sleep between requests
                if next_url:
                    time.sleep(0.1)  # 100ms delay

            # Snapshot tables return empty offset
            return iter(records), {}

        def _read_channels(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> Tuple[Iterator[dict], dict]:
            """
            Read channels table (snapshot mode).

            Two modes:
            1. Specific team: Requires team_id
            2. Auto-discovery: Requires fetch_all_teams="true"

            Args:
                start_offset: Not used for snapshot tables
                table_options: Must include either team_id or fetch_all_teams

            Returns:
                Tuple of (iterator of channel records, empty offset dict)

            Raises:
                ValueError: If required options are missing
            """
            team_id = table_options.get("team_id")
            fetch_all_teams = table_options.get("fetch_all_teams", "").lower() == "true"

            # Validate inputs
            if not team_id and not fetch_all_teams:
                raise ValueError(
                    "table_options for 'channels' must include either 'team_id' "
                    "or 'fetch_all_teams=true'"
                )

            # Parse options
            try:
                max_pages = int(table_options.get("max_pages_per_batch", 100))
            except (TypeError, ValueError):
                max_pages = 100

            # Determine which teams to process
            if fetch_all_teams:
                team_ids_to_process = self._fetch_all_team_ids(max_pages)
            else:
                team_ids_to_process = [team_id]

            # Now fetch channels for all discovered teams
            records: List[dict[str, Any]] = []

            for current_team_id in team_ids_to_process:
                # Build request for this team's channels
                url = f"{self.base_url}/teams/{current_team_id}/channels"
                params = {}  # No query parameters needed - API returns all channels with pagination

                pages_fetched = 0
                next_url: str | None = url

                while next_url and pages_fetched < max_pages:
                    try:
                        if pages_fetched == 0:
                            data = self._make_request_with_retry(url, params=params)
                        else:
                            data = self._make_request_with_retry(next_url)

                        channels = data.get("value", [])

                        for channel in channels:
                            # Add connector-derived field
                            record: dict[str, Any] = dict(channel)
                            record["team_id"] = current_team_id
                            records.append(record)

                        next_url = data.get("@odata.nextLink")
                        pages_fetched += 1

                        if next_url:
                            time.sleep(0.1)

                    except Exception as e:
                        # If a team is inaccessible, log and continue
                        if "404" not in str(e) and "403" not in str(e):
                            # Only raise if it's not a 404/403 (inaccessible team)
                            raise
                        # Continue to next team on 404/403
                        break

            return iter(records), {}

        def _read_members(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> Tuple[Iterator[dict], dict]:
            """
            Read members table (snapshot mode).

            Two modes:
            1. Specific team: Requires team_id
            2. Auto-discovery: Requires fetch_all_teams="true"

            Args:
                start_offset: Not used for snapshot tables
                table_options: Must include either team_id or fetch_all_teams

            Returns:
                Tuple of (iterator of member records, empty offset dict)

            Raises:
                ValueError: If required options are missing
            """
            team_id = table_options.get("team_id")
            fetch_all_teams = table_options.get("fetch_all_teams", "").lower() == "true"

            # Validate inputs
            if not team_id and not fetch_all_teams:
                raise ValueError(
                    "table_options for 'members' must include either 'team_id' "
                    "or 'fetch_all_teams=true'"
                )

            try:
                max_pages = int(table_options.get("max_pages_per_batch", 100))
            except (TypeError, ValueError):
                max_pages = 100

            # Determine which teams to process
            if fetch_all_teams:
                team_ids_to_process = self._fetch_all_team_ids(max_pages)
            else:
                team_ids_to_process = [team_id]

            # Now fetch members for all discovered teams
            records: List[dict[str, Any]] = []

            for current_team_id in team_ids_to_process:
                # Note: /teams/{id}/members endpoint does NOT support $top parameter
                url = f"{self.base_url}/teams/{current_team_id}/members"
                params = {}  # No query parameters needed - API returns all members with pagination

                pages_fetched = 0
                next_url: str | None = url

                while next_url and pages_fetched < max_pages:
                    try:
                        if pages_fetched == 0:
                            data = self._make_request_with_retry(url, params=params)
                        else:
                            data = self._make_request_with_retry(next_url)

                        members = data.get("value", [])

                        for member in members:
                            record: dict[str, Any] = dict(member)
                            record["team_id"] = current_team_id
                            records.append(record)

                        next_url = data.get("@odata.nextLink")
                        pages_fetched += 1

                        if next_url:
                            time.sleep(0.1)

                    except Exception as e:
                        # If a team is inaccessible, log and continue
                        if "404" not in str(e) and "403" not in str(e):
                            # Only raise if it's not a 404/403 (inaccessible team)
                            raise
                        # Continue to next team on 404/403
                        break

            return iter(records), {}

        def _read_messages(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> Tuple[Iterator[dict], dict]:
            """
            Read messages table - routes to Delta API or legacy implementation.

            Delta API (default): Server-side filtering, O(changed_messages)
            Legacy: Client-side filtering, O(all_messages)

            Args:
                start_offset: Dictionary with cursor or deltaLink
                table_options: Must include team_id OR fetch_all_teams, plus channel_id or fetch_all_channels

            Returns:
                Tuple of (iterator of message records, next_offset dict)

            Configuration:
                use_delta_api: "true" (default) or "false"
            """
            use_delta_api = table_options.get("use_delta_api", "true").lower() == "true"

            if use_delta_api:
                return self._read_messages_delta(start_offset, table_options)
            else:
                return self._read_messages_legacy(start_offset, table_options)

        def _read_messages_delta(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> Tuple[Iterator[dict], dict]:
            """
            Read messages using Microsoft Graph Delta API (server-side filtering).

            Delta API provides efficient incremental sync:
            - Only returns new, modified, or deleted messages
            - O(changed_messages) instead of O(all_messages)
            - Handles deletions with @removed marker

            Note: Delta API does NOT track reaction changes. Reactions are not tracked by this connector.

            Args:
                start_offset: Dictionary with deltaLink from previous sync
                table_options: Same as legacy mode

            Returns:
                Tuple of (iterator of message records, offset dict with deltaLink)
            """
            team_id = table_options.get("team_id")
            channel_id = table_options.get("channel_id")
            fetch_all_channels = table_options.get("fetch_all_channels", "").lower() == "true"
            fetch_all_teams = table_options.get("fetch_all_teams", "").lower() == "true"

            # Validate inputs
            if not team_id and not fetch_all_teams:
                raise ValueError(
                    "table_options for 'messages' must include either 'team_id' "
                    "or 'fetch_all_teams=true'"
                )

            if not channel_id and not fetch_all_channels:
                raise ValueError(
                    "table_options for 'messages' must include either 'channel_id' "
                    "or 'fetch_all_channels=true'"
                )

            try:
                max_pages = int(table_options.get("max_pages_per_batch", 100))
            except (TypeError, ValueError):
                max_pages = 100

            # Determine which teams to process
            if fetch_all_teams:
                team_ids_to_process = self._fetch_all_team_ids(max_pages)
            else:
                team_ids_to_process = [team_id]

            # Build team-channel pairs
            all_team_channel_pairs = []
            for current_team_id in team_ids_to_process:
                if fetch_all_channels:
                    channel_ids_to_process = self._fetch_all_channel_ids(current_team_id, max_pages)
                    if not channel_ids_to_process:
                        continue
                else:
                    channel_ids_to_process = [channel_id]

                for ch_id in channel_ids_to_process:
                    all_team_channel_pairs.append((current_team_id, ch_id))

            # Fetch messages using Delta API for each channel
            records = []
            delta_links = {}

            for current_team_id, current_channel_id in all_team_channel_pairs:
                channel_key = f"{current_team_id}/{current_channel_id}"

                # Get deltaLink from previous sync (if exists)
                delta_link = None
                if start_offset and isinstance(start_offset, dict):
                    delta_links_data = start_offset.get("deltaLinks", {})
                    delta_link = delta_links_data.get(channel_key)

                if delta_link:
                    # Incremental sync - use saved deltaLink
                    url = delta_link
                else:
                    # Initial sync - use delta endpoint
                    url = f"{self.base_url}/teams/{current_team_id}/channels/{current_channel_id}/messages/delta"

                pages_fetched = 0
                next_url = url

                try:
                    while next_url and pages_fetched < max_pages:
                        data = self._make_request_with_retry(next_url)
                        messages = data.get("value", [])

                        for msg in messages:
                            # Handle deleted messages
                            if "@removed" in msg:
                                record = {
                                    "id": msg["id"],
                                    "team_id": current_team_id,
                                    "channel_id": current_channel_id,
                                    "_deleted": True,
                                    "lastModifiedDateTime": datetime.now().isoformat().replace("+00:00", "Z"),
                                }
                            else:
                                # Normal message
                                record = dict(msg)
                                record["team_id"] = current_team_id
                                record["channel_id"] = current_channel_id

                                # Serialize complex objects
                                if "policyViolation" in record and isinstance(record["policyViolation"], dict):
                                    record["policyViolation"] = json.dumps(record["policyViolation"])
                                if "eventDetail" in record and isinstance(record["eventDetail"], dict):
                                    record["eventDetail"] = json.dumps(record["eventDetail"])
                                if "messageHistory" in record and isinstance(record["messageHistory"], list):
                                    record["messageHistory"] = json.dumps(record["messageHistory"])

                            records.append(record)

                        # Check for deltaLink (end of sync) or nextLink (pagination)
                        delta_link_new = data.get("@odata.deltaLink")
                        next_link = data.get("@odata.nextLink")

                        if delta_link_new:
                            # Save deltaLink for next sync
                            delta_links[channel_key] = delta_link_new
                            break
                        elif next_link:
                            next_url = next_link
                            pages_fetched += 1
                            time.sleep(0.1)
                        else:
                            break

                except Exception as e:
                    # Skip inaccessible channels
                    if "404" not in str(e) and "403" not in str(e):
                        raise
                    continue

            # Return deltaLinks for next sync
            next_offset = {"deltaLinks": delta_links} if delta_links else {}
            return iter(records), next_offset

        def _read_messages_legacy(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> Tuple[Iterator[dict], dict]:
            """
            Read messages table (CDC mode with client-side timestamp filtering).

            LEGACY MODE: Fetches all messages and filters client-side.
            Use Delta API for better performance (O(changed) vs O(all)).

            Three modes:
            1. Specific channel: Requires team_id and channel_id
            2. Auto-discover channels: Requires team_id and fetch_all_channels="true"
            3. Auto-discover everything: Requires fetch_all_teams="true" and fetch_all_channels="true"

            Args:
                start_offset: Dictionary with cursor (e.g., {"cursor": "2025-01-15T10:30:00.000Z"})
                table_options: Must include team_id OR fetch_all_teams, plus either channel_id or fetch_all_channels

            Returns:
                Tuple of (iterator of message records, next_offset dict with updated cursor)

            Raises:
                ValueError: If required options are missing
            """
            team_id = table_options.get("team_id")
            channel_id = table_options.get("channel_id")
            fetch_all_channels = table_options.get("fetch_all_channels", "").lower() == "true"
            fetch_all_teams = table_options.get("fetch_all_teams", "").lower() == "true"

            # Validate inputs
            if not team_id and not fetch_all_teams:
                raise ValueError(
                    "table_options for 'messages' must include either 'team_id' "
                    "or 'fetch_all_teams=true'"
                )

            if not channel_id and not fetch_all_channels:
                raise ValueError(
                    "table_options for 'messages' must include either 'channel_id' "
                    "or 'fetch_all_channels=true'"
                )

            # Parse options
            try:
                top = int(table_options.get("top", 50))
            except (TypeError, ValueError):
                top = 50
            top = max(1, min(top, 50))  # Max 50 for messages endpoint

            try:
                max_pages = int(table_options.get("max_pages_per_batch", 100))
            except (TypeError, ValueError):
                max_pages = 100

            try:
                lookback_seconds = int(table_options.get("lookback_seconds", 300))
            except (TypeError, ValueError):
                lookback_seconds = 300

            # Determine starting cursor
            cursor = None
            if start_offset and isinstance(start_offset, dict):
                cursor = start_offset.get("cursor")
            if not cursor:
                cursor = table_options.get("start_date")

            # Determine which teams to process
            if fetch_all_teams:
                team_ids_to_process = self._fetch_all_team_ids(max_pages)
            else:
                team_ids_to_process = [team_id]

            # Now process each team to get channels
            all_team_channel_pairs = []
            for current_team_id in team_ids_to_process:
                # Determine which channels to process for this team
                if fetch_all_channels:
                    channel_ids_to_process = self._fetch_all_channel_ids(current_team_id, max_pages)
                    if not channel_ids_to_process:
                        # Team is inaccessible (404/403), skip it
                        continue
                else:
                    channel_ids_to_process = [channel_id]

                # Add all team-channel pairs
                for ch_id in channel_ids_to_process:
                    all_team_channel_pairs.append((current_team_id, ch_id))

            # Now fetch messages for all discovered team-channel pairs
            records: List[dict[str, Any]] = []
            max_modified: str | None = None

            for current_team_id, current_channel_id in all_team_channel_pairs:
                # Build request URL for this channel's messages
                url = f"{self.base_url}/teams/{current_team_id}/channels/{current_channel_id}/messages"
                params = {"$top": top}

                pages_fetched = 0
                next_url: str | None = url

                while next_url and pages_fetched < max_pages:
                    try:
                        if pages_fetched == 0:
                            data = self._make_request_with_retry(url, params=params)
                        else:
                            data = self._make_request_with_retry(next_url)

                        messages = data.get("value", [])

                        for msg in messages:
                            # Filter by cursor (client-side since API doesn't support $filter on /messages)
                            last_modified = msg.get("lastModifiedDateTime")
                            if cursor and last_modified and last_modified < cursor:
                                continue  # Skip messages before cursor

                            # Add connector-derived fields
                            record: dict[str, Any] = dict(msg)
                            record["team_id"] = current_team_id
                            record["channel_id"] = current_channel_id

                            # Serialize complex objects to JSON strings
                            if "policyViolation" in record and isinstance(
                                record["policyViolation"], dict
                            ):
                                record["policyViolation"] = json.dumps(record["policyViolation"])

                            if "eventDetail" in record and isinstance(record["eventDetail"], dict):
                                record["eventDetail"] = json.dumps(record["eventDetail"])

                            if "messageHistory" in record and isinstance(
                                record["messageHistory"], list
                            ):
                                record["messageHistory"] = json.dumps(record["messageHistory"])

                            records.append(record)

                            # Track max timestamp
                            if last_modified:
                                if max_modified is None or last_modified > max_modified:
                                    max_modified = last_modified

                        # Handle pagination
                        next_url = data.get("@odata.nextLink")
                        pages_fetched += 1

                        if next_url:
                            time.sleep(0.1)

                    except Exception as e:
                        # If a channel is inaccessible, log and continue
                        if "404" not in str(e) and "403" not in str(e):
                            # Only raise if it's not a 404/403 (inaccessible channel)
                            raise
                        # Continue to next channel on 404/403
                        break

            # Compute next cursor with lookback window
            next_cursor = cursor
            if max_modified:
                try:
                    # Parse ISO 8601 timestamp
                    dt = datetime.fromisoformat(max_modified.replace("Z", "+00:00"))
                    dt_with_lookback = dt - timedelta(seconds=lookback_seconds)
                    next_cursor = dt_with_lookback.isoformat().replace("+00:00", "Z")
                except Exception:
                    # Fallback: use max_modified as-is
                    next_cursor = max_modified

            next_offset = {"cursor": next_cursor} if next_cursor else {}
            return iter(records), next_offset

        def _read_message_replies(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> Tuple[Iterator[dict], dict]:
            """
            Read message replies (threads) for a specific message or auto-discover.

            Routes to Delta API or legacy implementation based on use_delta_api option.

            Args:
                start_offset: Dictionary with cursor or deltaLinks
                table_options: Must include use_delta_api option (default: "true")

            Returns:
                Tuple of (iterator of reply records, next_offset dict)

            Raises:
                ValueError: If required options are missing
            """
            use_delta_api = table_options.get("use_delta_api", "true").lower() == "true"

            if use_delta_api:
                return self._read_message_replies_delta(start_offset, table_options)
            else:
                return self._read_message_replies_legacy(start_offset, table_options)

        def _read_message_replies_delta(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> Tuple[Iterator[dict], dict]:
            """
            Read message replies using Microsoft Graph Delta API (server-side filtering).

            This method uses the Delta API for efficient incremental sync of replies.
            Only changed/new/deleted replies are returned from the server.

            Four modes:
            1. Specific message: Requires team_id, channel_id, and message_id
            2. Auto-discover messages: Requires team_id, channel_id, and fetch_all_messages="true"
            3. Auto-discover channels: Requires team_id, fetch_all_channels="true", and fetch_all_messages="true"
            4. Auto-discover everything: Requires fetch_all_teams="true", fetch_all_channels="true", and fetch_all_messages="true"

            Args:
                start_offset: Dictionary with deltaLinks (e.g., {"deltaLinks": {"team/channel/message": "deltaUrl"}})
                table_options: See modes above for required options

            Returns:
                Tuple of (iterator of reply records, next_offset dict with updated deltaLinks)

            Raises:
                ValueError: If required options are missing
            """
            team_id = table_options.get("team_id")
            channel_id = table_options.get("channel_id")
            message_id = table_options.get("message_id")
            fetch_all_messages = table_options.get("fetch_all_messages", "").lower() == "true"
            fetch_all_channels = table_options.get("fetch_all_channels", "").lower() == "true"
            fetch_all_teams = table_options.get("fetch_all_teams", "").lower() == "true"

            # Validate inputs
            if not team_id and not fetch_all_teams:
                raise ValueError(
                    "table_options for 'message_replies' must include either 'team_id' "
                    "or 'fetch_all_teams=true'"
                )

            if not channel_id and not fetch_all_channels:
                raise ValueError(
                    "table_options for 'message_replies' must include either 'channel_id' "
                    "or 'fetch_all_channels=true'"
                )

            if not message_id and not fetch_all_messages:
                raise ValueError(
                    "table_options for 'message_replies' must include either 'message_id' "
                    "or 'fetch_all_messages=true'"
                )

            # Parse options
            try:
                max_pages = int(table_options.get("max_pages_per_batch", 100))
            except (TypeError, ValueError):
                max_pages = 100

            # Determine which teams to process
            if fetch_all_teams:
                team_ids_to_process = self._fetch_all_team_ids(max_pages)
            else:
                team_ids_to_process = [team_id]

            # Build team-channel pairs
            all_team_channel_pairs = []
            for current_team_id in team_ids_to_process:
                if fetch_all_channels:
                    channel_ids_to_process = self._fetch_all_channel_ids(current_team_id, max_pages)
                    if not channel_ids_to_process:
                        continue
                else:
                    channel_ids_to_process = [channel_id]

                for ch_id in channel_ids_to_process:
                    all_team_channel_pairs.append((current_team_id, ch_id))

            # Build team-channel-message triples
            all_team_channel_message_triples = []
            for current_team_id, current_channel_id in all_team_channel_pairs:
                if fetch_all_messages:
                    message_ids_to_process = self._fetch_all_message_ids(current_team_id, current_channel_id, max_pages)
                    if not message_ids_to_process:
                        continue
                else:
                    message_ids_to_process = [message_id]

                for msg_id in message_ids_to_process:
                    all_team_channel_message_triples.append((current_team_id, current_channel_id, msg_id))

            # Fetch replies using Delta API for each message
            records: List[dict[str, Any]] = []
            delta_links = {}

            for current_team_id, current_channel_id, current_message_id in all_team_channel_message_triples:
                message_key = f"{current_team_id}/{current_channel_id}/{current_message_id}"

                # Get deltaLink from previous sync for this specific message
                delta_link = start_offset.get("deltaLinks", {}).get(message_key) if start_offset else None

                if delta_link:
                    # Incremental sync - use saved deltaLink
                    url = delta_link
                else:
                    # Initial sync - use delta endpoint
                    url = f"{self.base_url}/teams/{current_team_id}/channels/{current_channel_id}/messages/{current_message_id}/replies/delta"

                pages_fetched = 0

                try:
                    while url and pages_fetched < max_pages:
                        data = self._make_request_with_retry(url)
                        replies = data.get("value", [])

                        for reply in replies:
                            # Check if reply was deleted
                            if "@removed" in reply:
                                # Handle deleted reply
                                record = {
                                    "id": reply["id"],
                                    "parent_message_id": current_message_id,
                                    "team_id": current_team_id,
                                    "channel_id": current_channel_id,
                                    "_deleted": True,
                                    "lastModifiedDateTime": datetime.now().isoformat().replace("+00:00", "Z")
                                }
                            else:
                                # Normal reply processing
                                record: dict[str, Any] = dict(reply)
                                record["parent_message_id"] = current_message_id
                                record["team_id"] = current_team_id
                                record["channel_id"] = current_channel_id

                                # Serialize complex objects to JSON strings
                                if "policyViolation" in record and isinstance(
                                    record["policyViolation"], dict
                                ):
                                    record["policyViolation"] = json.dumps(record["policyViolation"])

                                if "eventDetail" in record and isinstance(record["eventDetail"], dict):
                                    record["eventDetail"] = json.dumps(record["eventDetail"])

                                if "messageHistory" in record and isinstance(
                                    record["messageHistory"], list
                                ):
                                    record["messageHistory"] = json.dumps(record["messageHistory"])

                            records.append(record)

                        # Check for deltaLink (end of sync for this message)
                        delta_link_new = data.get("@odata.deltaLink")
                        if delta_link_new:
                            delta_links[message_key] = delta_link_new
                            break  # Delta sync complete for this message

                        # Check for nextLink (more pages in this delta sync)
                        url = data.get("@odata.nextLink")
                        pages_fetched += 1

                        if url:
                            time.sleep(0.1)

                except Exception as e:
                    # If message has no replies or is inaccessible, continue
                    if "404" not in str(e) and "403" not in str(e):
                        raise
                    # Continue to next message on 404/403

            next_offset = {"deltaLinks": delta_links} if delta_links else {}
            return iter(records), next_offset

        def _read_message_replies_legacy(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> Tuple[Iterator[dict], dict]:
            """
            Read message replies (threads) for a specific message or auto-discover (CDC mode).

            LEGACY MODE: Fetches all replies and filters client-side.
            Use Delta API for better performance (O(changed) vs O(all)).

            Four modes:
            1. Specific message: Requires team_id, channel_id, and message_id
            2. Auto-discover messages: Requires team_id, channel_id, and fetch_all_messages="true"
            3. Auto-discover channels: Requires team_id, fetch_all_channels="true", and fetch_all_messages="true"
            4. Auto-discover everything: Requires fetch_all_teams="true", fetch_all_channels="true", and fetch_all_messages="true"

            Args:
                start_offset: Dictionary with cursor (e.g., {"cursor": "2025-01-15T10:30:00.000Z"})
                table_options: See modes above for required options

            Returns:
                Tuple of (iterator of reply records, next_offset dict with updated cursor)

            Raises:
                ValueError: If required options are missing
            """
            team_id = table_options.get("team_id")
            channel_id = table_options.get("channel_id")
            message_id = table_options.get("message_id")
            fetch_all_messages = table_options.get("fetch_all_messages", "").lower() == "true"
            fetch_all_channels = table_options.get("fetch_all_channels", "").lower() == "true"
            fetch_all_teams = table_options.get("fetch_all_teams", "").lower() == "true"

            # Validate inputs
            if not team_id and not fetch_all_teams:
                raise ValueError(
                    "table_options for 'message_replies' must include either 'team_id' "
                    "or 'fetch_all_teams=true'"
                )

            if not channel_id and not fetch_all_channels:
                raise ValueError(
                    "table_options for 'message_replies' must include either 'channel_id' "
                    "or 'fetch_all_channels=true'"
                )

            if not message_id and not fetch_all_messages:
                raise ValueError(
                    "table_options for 'message_replies' must include either 'message_id' "
                    "or 'fetch_all_messages=true'"
                )

            # Parse options
            try:
                top = int(table_options.get("top", 50))
            except (TypeError, ValueError):
                top = 50
            top = max(1, min(top, 50))  # Max 50 for replies endpoint

            try:
                max_pages = int(table_options.get("max_pages_per_batch", 100))
            except (TypeError, ValueError):
                max_pages = 100

            try:
                lookback_seconds = int(table_options.get("lookback_seconds", 300))
            except (TypeError, ValueError):
                lookback_seconds = 300

            # Determine starting cursor
            cursor = None
            if start_offset and isinstance(start_offset, dict):
                cursor = start_offset.get("cursor")
            if not cursor:
                cursor = table_options.get("start_date")

            # Determine which teams to process
            if fetch_all_teams:
                team_ids_to_process = self._fetch_all_team_ids(max_pages)
            else:
                team_ids_to_process = [team_id]

            # Now process each team to get channels
            all_team_channel_pairs = []
            for current_team_id in team_ids_to_process:
                # Determine which channels to process for this team
                if fetch_all_channels:
                    channel_ids_to_process = self._fetch_all_channel_ids(current_team_id, max_pages)
                    if not channel_ids_to_process:
                        # Team is inaccessible (404/403), skip it
                        continue
                else:
                    channel_ids_to_process = [channel_id]

                # Add all team-channel pairs
                for ch_id in channel_ids_to_process:
                    all_team_channel_pairs.append((current_team_id, ch_id))

            # Now process each team-channel pair to get messages
            all_team_channel_message_triples = []
            for current_team_id, current_channel_id in all_team_channel_pairs:
                # Determine which messages to process for this channel
                if fetch_all_messages:
                    message_ids_to_process = self._fetch_all_message_ids(current_team_id, current_channel_id, max_pages)
                    if not message_ids_to_process:
                        # Channel is inaccessible (404/403), skip it
                        continue
                else:
                    message_ids_to_process = [message_id]

                # Add all team-channel-message triples
                for msg_id in message_ids_to_process:
                    all_team_channel_message_triples.append((current_team_id, current_channel_id, msg_id))

            # Parse max_concurrent_threads option
            try:
                max_workers = int(table_options.get("max_concurrent_threads", 10))
            except (TypeError, ValueError):
                max_workers = 10

            # Now fetch replies for all discovered team-channel-message triples using ThreadPoolExecutor
            records: List[dict[str, Any]] = []
            max_modified: str | None = None

            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = {
                    executor.submit(
                        self._fetch_replies_for_message,
                        team_id,
                        channel_id,
                        message_id,
                        cursor,
                        top,
                        max_pages
                    ): (team_id, channel_id, message_id)
                    for team_id, channel_id, message_id in all_team_channel_message_triples
                }

                for future in as_completed(futures):
                    try:
                        reply_records, reply_max_modified = future.result()
                        records.extend(reply_records)

                        # Track max timestamp across all messages
                        if reply_max_modified:
                            if max_modified is None or reply_max_modified > max_modified:
                                max_modified = reply_max_modified

                    except Exception as e:
                        # Log but continue on errors from individual message fetches
                        if "404" not in str(e) and "403" not in str(e):
                            # Only raise if it's not a 404/403 (inaccessible message)
                            raise
                        # Continue to next message on 404/403

            # Compute next cursor with lookback window
            next_cursor = cursor
            if max_modified:
                try:
                    # Parse ISO 8601 timestamp
                    dt = datetime.fromisoformat(max_modified.replace("Z", "+00:00"))
                    dt_with_lookback = dt - timedelta(seconds=lookback_seconds)
                    next_cursor = dt_with_lookback.isoformat().replace("+00:00", "Z")
                except Exception:
                    # Fallback: use max_modified as-is
                    next_cursor = max_modified

            next_offset = {"cursor": next_cursor} if next_cursor else {}
            return iter(records), next_offset

        def _fetch_replies_for_message(
            self,
            team_id: str,
            channel_id: str,
            message_id: str,
            cursor: str | None,
            top: int,
            max_pages: int
        ) -> Tuple[List[dict[str, Any]], str | None]:
            """
            Fetch replies for a single message (helper for parallel execution).

            Args:
                team_id: Team ID
                channel_id: Channel ID
                message_id: Message ID
                cursor: Timestamp cursor for filtering
                top: Page size
                max_pages: Max pages to fetch

            Returns:
                Tuple of (list of reply records, max_modified timestamp)
            """
            records: List[dict[str, Any]] = []
            max_modified: str | None = None

            url = f"{self.base_url}/teams/{team_id}/channels/{channel_id}/messages/{message_id}/replies"
            params = {"$top": top}

            pages_fetched = 0
            next_url: str | None = url

            try:
                while next_url and pages_fetched < max_pages:
                    if pages_fetched == 0:
                        data = self._make_request_with_retry(url, params=params)
                    else:
                        data = self._make_request_with_retry(next_url)

                    replies = data.get("value", [])

                    for reply in replies:
                        # Filter by cursor (client-side since API doesn't support $filter on /replies)
                        last_modified = reply.get("lastModifiedDateTime")
                        if cursor and last_modified and last_modified < cursor:
                            continue  # Skip replies before cursor

                        # Add connector-derived fields
                        record: dict[str, Any] = dict(reply)
                        record["parent_message_id"] = message_id
                        record["team_id"] = team_id
                        record["channel_id"] = channel_id

                        # Serialize complex objects to JSON strings
                        if "policyViolation" in record and isinstance(record["policyViolation"], dict):
                            record["policyViolation"] = json.dumps(record["policyViolation"])

                        if "eventDetail" in record and isinstance(record["eventDetail"], dict):
                            record["eventDetail"] = json.dumps(record["eventDetail"])

                        if "messageHistory" in record and isinstance(record["messageHistory"], list):
                            record["messageHistory"] = json.dumps(record["messageHistory"])

                        records.append(record)

                        # Track max timestamp
                        if last_modified:
                            if max_modified is None or last_modified > max_modified:
                                max_modified = last_modified

                    # Handle pagination
                    next_url = data.get("@odata.nextLink")
                    pages_fetched += 1

                    if next_url:
                        time.sleep(0.1)

            except Exception as e:
                # If a message has no replies or is inaccessible, return empty
                if "404" not in str(e):
                    # Only raise if it's not a 404 (no replies)
                    raise
                # Return empty on 404

            return records, max_modified


    def register_lakeflow_source(spark):
        """
        Register the Microsoft Teams connector with the Lakeflow framework.

        Note: This is a placeholder function for compatibility with the source_loader.
        The actual registration happens through the ingestion pipeline.

        Args:
            spark: The Spark session
        """
        # No-op: The connector is used directly by the ingestion pipeline
        # which doesn't require explicit registration
        pass


    ########################################################
    # pipeline/lakeflow_python_source.py
    ########################################################

    METADATA_TABLE = "_lakeflow_metadata"
    TABLE_NAME = "tableName"
    TABLE_NAME_LIST = "tableNameList"
    TABLE_CONFIGS = "tableConfigs"


    class LakeflowStreamReader(SimpleDataSourceStreamReader):
        """
        Implements a data source stream reader for Lakeflow Connect.
        Currently, only the simpleStreamReader is implemented, which uses a
        more generic protocol suitable for most data sources that support
        incremental loading.
        """

        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.lakeflow_connect = lakeflow_connect
            self.schema = schema

        def initialOffset(self):
            return {}

        def read(self, start: dict) -> (Iterator[tuple], dict):
            records, offset = self.lakeflow_connect.read_table(
                self.options[TABLE_NAME], start, self.options
            )
            rows = map(lambda x: parse_value(x, self.schema), records)
            return rows, offset

        def readBetweenOffsets(self, start: dict, end: dict) -> Iterator[tuple]:
            # TODO: This does not ensure the records returned are identical across repeated calls.
            # For append-only tables, the data source must guarantee that reading from the same
            # start offset will always yield the same set of records.
            # For tables ingested as incremental CDC, it is only necessary that no new changes
            # are missed in the returned records.
            return self.read(start)[0]


    class LakeflowBatchReader(DataSourceReader):
        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.schema = schema
            self.lakeflow_connect = lakeflow_connect
            self.table_name = options[TABLE_NAME]

        def read(self, partition):
            all_records = []
            if self.table_name == METADATA_TABLE:
                all_records = self._read_table_metadata()
            else:
                all_records, _ = self.lakeflow_connect.read_table(
                    self.table_name, None, self.options
                )

            rows = map(lambda x: parse_value(x, self.schema), all_records)
            return iter(rows)

        def _read_table_metadata(self):
            table_name_list = self.options.get(TABLE_NAME_LIST, "")
            table_names = [o.strip() for o in table_name_list.split(",") if o.strip()]
            all_records = []
            table_configs = json.loads(self.options.get(TABLE_CONFIGS, "{}"))
            for table in table_names:
                metadata = self.lakeflow_connect.read_table_metadata(
                    table, table_configs.get(table, {})
                )
                all_records.append({TABLE_NAME: table, **metadata})
            return all_records


    class LakeflowSource(DataSource):
        def __init__(self, options):
            self.options = options
            self.lakeflow_connect = LakeflowConnect(options)

        @classmethod
        def name(cls):
            return "lakeflow_connect"

        def schema(self):
            table = self.options[TABLE_NAME]
            if table == METADATA_TABLE:
                return StructType(
                    [
                        StructField(TABLE_NAME, StringType(), False),
                        StructField("primary_keys", ArrayType(StringType()), True),
                        StructField("cursor_field", StringType(), True),
                        StructField("ingestion_type", StringType(), True),
                    ]
                )
            else:
                # Assuming the LakeflowConnect interface uses get_table_schema, not get_table_details
                return self.lakeflow_connect.get_table_schema(table, self.options)

        def reader(self, schema: StructType):
            return LakeflowBatchReader(self.options, schema, self.lakeflow_connect)

        def simpleStreamReader(self, schema: StructType):
            return LakeflowStreamReader(self.options, schema, self.lakeflow_connect)


    spark.dataSource.register(LakeflowSource)  # pylint: disable=undefined-variable
