# ==============================================================================
# Merged Lakeflow Source: elasticsearch
# ==============================================================================
# This file is auto-generated by tools/scripts/merge_python_source.py
# Do not edit manually. Make changes to the source files instead.
# ==============================================================================

from datetime import datetime
from decimal import Decimal
from typing import Any, Iterator
import json

from pyspark.sql import Row
from pyspark.sql.datasource import DataSource, DataSourceReader, SimpleDataSourceStreamReader
from requests.adapters import HTTPAdapter
from urllib3.util import Retry
from pyspark.sql.types import *
import base64
import requests


def register_lakeflow_source(spark):
    """Register the Lakeflow Python source with Spark."""

    ########################################################
    # libs/utils.py
    ########################################################

    def _parse_struct(value: Any, field_type: StructType) -> Row:
        """Parse a dictionary into a PySpark Row based on StructType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for StructType, got {type(value)}")
        # Spark Python -> Arrow conversion require missing StructType fields to be assigned None.
        if value == {}:
            raise ValueError(
                "field in StructType cannot be an empty dict. "
                "Please assign None as the default value instead."
            )
        field_dict = {}
        for field in field_type.fields:
            if field.name in value:
                field_dict[field.name] = parse_value(value.get(field.name), field.dataType)
            elif field.nullable:
                field_dict[field.name] = None
            else:
                raise ValueError(f"Field {field.name} is not nullable but not found in the input")
        return Row(**field_dict)


    def _parse_array(value: Any, field_type: ArrayType) -> list:
        """Parse a list into a PySpark array based on ArrayType schema."""
        if not isinstance(value, list):
            if field_type.containsNull:
                return [parse_value(value, field_type.elementType)]
            raise ValueError(f"Expected a list for ArrayType, got {type(value)}")
        return [parse_value(v, field_type.elementType) for v in value]


    def _parse_map(value: Any, field_type: MapType) -> dict:
        """Parse a dictionary into a PySpark map based on MapType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for MapType, got {type(value)}")
        return {
            parse_value(k, field_type.keyType): parse_value(v, field_type.valueType)
            for k, v in value.items()
        }


    def _parse_string(value: Any) -> str:
        """Convert value to string."""
        return str(value)


    def _parse_integer(value: Any) -> int:
        """Convert value to integer."""
        if isinstance(value, str) and value.strip():
            return int(float(value)) if "." in value else int(value)
        if isinstance(value, (int, float)):
            return int(value)
        raise ValueError(f"Cannot convert {value} to integer")


    def _parse_float(value: Any) -> float:
        """Convert value to float."""
        return float(value)


    def _parse_decimal(value: Any) -> Decimal:
        """Convert value to Decimal."""
        return Decimal(value) if isinstance(value, str) and value.strip() else Decimal(str(value))


    def _parse_boolean(value: Any) -> bool:
        """Convert value to boolean."""
        if isinstance(value, str):
            lowered = value.lower()
            if lowered in ("true", "t", "yes", "y", "1"):
                return True
            if lowered in ("false", "f", "no", "n", "0"):
                return False
        return bool(value)


    def _parse_date(value: Any) -> datetime.date:
        """Convert value to date."""
        if isinstance(value, str):
            for fmt in ("%Y-%m-%d", "%m/%d/%Y", "%d-%m-%Y", "%Y/%m/%d"):
                try:
                    return datetime.strptime(value, fmt).date()
                except ValueError:
                    continue
            return datetime.fromisoformat(value).date()
        if isinstance(value, datetime):
            return value.date()
        raise ValueError(f"Cannot convert {value} to date")


    def _parse_timestamp(value: Any) -> datetime:
        """Convert value to timestamp."""
        if isinstance(value, str):
            ts_value = value.replace("Z", "+00:00") if value.endswith("Z") else value
            try:
                return datetime.fromisoformat(ts_value)
            except ValueError:
                for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S"):
                    try:
                        return datetime.strptime(ts_value, fmt)
                    except ValueError:
                        continue
        elif isinstance(value, (int, float)):
            return datetime.fromtimestamp(value)
        elif isinstance(value, datetime):
            return value
        raise ValueError(f"Cannot convert {value} to timestamp")


    def _decode_string_to_bytes(value: str) -> bytes:
        """Try to decode a string as base64, then hex, then UTF-8."""
        try:
            return base64.b64decode(value)
        except Exception:
            pass
        try:
            return bytes.fromhex(value)
        except Exception:
            pass
        return value.encode("utf-8")


    def _parse_binary(value: Any) -> bytes:
        """Convert value to bytes. Tries base64, then hex, then UTF-8 for strings."""
        if isinstance(value, bytes):
            return value
        if isinstance(value, bytearray):
            return bytes(value)
        if isinstance(value, str):
            return _decode_string_to_bytes(value)
        if isinstance(value, list):
            return bytes(value)
        return str(value).encode("utf-8")


    # Mapping of primitive types to their parser functions
    _PRIMITIVE_PARSERS = {
        StringType: _parse_string,
        IntegerType: _parse_integer,
        LongType: _parse_integer,
        FloatType: _parse_float,
        DoubleType: _parse_float,
        DecimalType: _parse_decimal,
        BooleanType: _parse_boolean,
        DateType: _parse_date,
        TimestampType: _parse_timestamp,
        BinaryType: _parse_binary,
    }


    def parse_value(value: Any, field_type: DataType) -> Any:
        """
        Converts a JSON value into a PySpark-compatible data type based on the provided field type.
        """
        if value is None:
            return None

        # Handle complex types
        if isinstance(field_type, StructType):
            return _parse_struct(value, field_type)
        if isinstance(field_type, ArrayType):
            return _parse_array(value, field_type)
        if isinstance(field_type, MapType):
            return _parse_map(value, field_type)

        # Handle primitive types via type-based lookup
        try:
            field_type_class = type(field_type)
            if field_type_class in _PRIMITIVE_PARSERS:
                return _PRIMITIVE_PARSERS[field_type_class](value)

            # Check for custom UDT handling
            if hasattr(field_type, "fromJson"):
                return field_type.fromJson(value)

            raise TypeError(f"Unsupported field type: {field_type}")
        except (ValueError, TypeError) as e:
            raise ValueError(f"Error converting '{value}' ({type(value)}) to {field_type}: {str(e)}")


    ########################################################
    # sources/elasticsearch/elasticsearch.py
    ########################################################

    class _AuthMethod:
        """Authentication method for producing auth headers for Elasticsearch API requests."""

        def headers(self) -> dict[str, str]:
            raise NotImplementedError

        @classmethod
        def keys(cls) -> list[str]:
            raise NotImplementedError

        @staticmethod
        def kind() -> str:
            raise NotImplementedError


    class _ApiKeyAuth(_AuthMethod):
        def __init__(self, api_key: str):
            self._api_key = api_key

        def headers(self) -> dict[str, str]:
            return {"Authorization": f"ApiKey {self._api_key}"}

        @staticmethod
        def keys() -> list[str]:
            return ["api_key"]

        @staticmethod
        def kind() -> str:
            return "API_KEY"


    class _BearerAuth(_AuthMethod):
        def __init__(self, token: str):
            self._token = token

        def headers(self) -> dict[str, str]:
            return {"Authorization": f"Bearer {self._token}"}

        @classmethod
        def keys(cls) -> list[str]:
            return ["token"]

        @staticmethod
        def kind() -> str:
            return "BEARER"


    class _ElasticsearchClient:
        def __init__(
            self,
            endpoint: str,
            auth: _AuthMethod,
            verify_ssl: bool = True,
            timeout: float | tuple[float, float] = (5.0, 30.0),
            max_retries: int = 3,
            backoff_factor: float = 0.5,
        ):
            self._endpoint = endpoint.rstrip("/")
            self._auth = auth
            self._verify_ssl = verify_ssl
            self._timeout = timeout

            # Configure a session with basic retry/backoff for transient failures.
            retry = Retry(
                total=max_retries,
                connect=max_retries,
                read=max_retries,
                status=max_retries,
                backoff_factor=backoff_factor,
                status_forcelist=[429, 500, 502, 503, 504],
                allowed_methods=frozenset(["GET", "POST"]),
                respect_retry_after_header=True,
                raise_on_status=False,
            )
            adapter = HTTPAdapter(max_retries=retry)
            session = requests.Session()
            session.mount("http://", adapter)
            session.mount("https://", adapter)
            self._session = session

        def _build_headers(self, extra: dict[str, str] | None = None) -> dict[str, str]:
            headers = {"Content-Type": "application/json"}
            headers.update(self._auth.headers())
            if extra:
                headers.update(extra)
            return headers

        def get(self, path: str, params: dict | None = None) -> dict:
            resp = self._session.get(
                f"{self._endpoint}{path}",
                headers=self._build_headers(),
                params=params,
                verify=self._verify_ssl,
                timeout=self._timeout,
            )
            resp.raise_for_status()
            return resp.json()

        def post(self, path: str, json: dict, params: dict | None = None) -> dict:
            resp = self._session.post(
                f"{self._endpoint}{path}",
                headers=self._build_headers(),
                json=json,
                params=params,
                verify=self._verify_ssl,
                timeout=self._timeout,
            )
            resp.raise_for_status()
            return resp.json()

        def delete(self, path: str, json: dict | None = None, params: dict | None = None) -> dict:
            resp = self._session.delete(
                f"{self._endpoint}{path}",
                headers=self._build_headers(),
                json=json,
                params=params,
                verify=self._verify_ssl,
                timeout=self._timeout,
            )
            resp.raise_for_status()
            return resp.json()


    class LakeflowConnect:

        _supported_auth_strategies = [
            _ApiKeyAuth,
            _BearerAuth,
        ]

        def __init__(self, options: dict[str, str]) -> None:
            """
            Initialize the Elasticsearch connector.

            Required options:
              - endpoint: base URL (e.g., https://elasticsearch-host:9200)
              - authentication: one of [API_KEY, BEARER]
                - api_key when authentication=API_KEY
                - token when authentication=BEARER
            Optional:
              - verify_ssl: "true"/"false" to control TLS verification (default: true)
            """
            endpoint = options.get("endpoint")
            authentication = options.get("authentication", "").upper()

            if not endpoint or authentication == "":
                raise ValueError("Missing required options: endpoint, authentication")

            auth_method = next((method for method in self._supported_auth_strategies if method.kind() == authentication), None)

            if auth_method is None:
                raise ValueError(f"Unsupported authentication method: {authentication}")

            auth_opts: dict[str, str] = {}

            for k in auth_method.keys():
                if k not in options or options.get(k, None) is None:
                    raise ValueError(f"Missing required authentication option: {k}")
                auth_opts[k] = options.get(k)

            verify_ssl = str(options.get("verify_ssl", "true")).lower() != "false"

            self._client = _ElasticsearchClient(
                endpoint=endpoint,
                auth=auth_method(**auth_opts),
                verify_ssl=verify_ssl
            )

            self._default_cursor_fields = ["timestamp", "updated_at"]  # evaluated in order!

            # Cache for discovered and available indices to avoid repeated API calls
            self._indices_cache: list[str] = []

            # Cache for index properties to avoid repeated API calls
            self._index_properties_cache: dict[str, dict] = {}

            # Cache for table metadata to avoid recomputation
            self._metadata_cache: dict[str, dict[str, Any]] = {}

            # Cache for discovered schemas to avoid recomputation
            self._schema_cache: dict[str, StructType] = {}

        @staticmethod
        def _validate_index(index: str) -> None:
            """Reject wildcard/pattern inputs; only single index/alias names are allowed."""
            if any(ch in index for ch in ["*", "?", ","]):
                raise ValueError(
                    f"Unsupported index name '{index}'. Wildcards or comma-separated patterns are not allowed; "
                    "specify a single index or alias."
                )

        def _discover_indices(self) -> list[str]:
            """Fetch indices and aliases visible to the provided credentials."""
            candidates: set[str] = set()

            # _cat/indices returns a list of dicts with "index"
            cat = self._client.get(path="/_cat/indices", params={"format": "json", "expand_wildcards": "all"})
            if isinstance(cat, list):
                for entry in cat:
                    idx = entry.get("index")
                    if idx:
                        candidates.add(idx)

            # aliases: include alias names too
            aliases_resp = self._client.get(path="/_aliases")
            if isinstance(aliases_resp, dict):
                for idx, data in aliases_resp.items():
                    candidates.add(idx)
                    aliases = data.get("aliases") or {}
                    for alias in aliases.keys():
                        candidates.add(alias)

            accessible: set[str] = set()

            # Probe each candidate with a lightweight search to ensure accessibility
            for idx in candidates:
                try:
                    self._client.post(path=f"/{idx}/_search", json={"size": 0}, params={"size": 0})
                    accessible.add(idx)
                except Exception:
                    continue

            return sorted(accessible)

        def _fetch_index_properties(self, index: str) -> dict:
            """
            Fetch and cache index properties for Elasticsearch 8.x+ (typeless mappings).
            Expects `mappings.properties` to be present; raises if the structure is not found.
            """
            if index in self._index_properties_cache:
                return self._index_properties_cache[index]

            # Fetch mapping from API
            mapping = self._client.get(f"/{index}/_mapping")

            # mapping can be keyed by concrete index even when requested via alias
            if not isinstance(mapping, dict):
                raise ValueError(f"Unexpected mapping response for index/alias {index}")

            # Resolve the mapping item:
            # - direct match when key exists
            # - single concrete index when len == 1
            # - multi-index alias/data stream: ensure properties are identical, else raise
            selected_item = None

            if index in mapping:
                selected_item = mapping[index]
            elif len(mapping) == 1:
                _, selected_item = next(iter(mapping.items()))
            else:
                # multi-index response: compare properties across all backing indices
                properties_list = []
                for _, item in mapping.items():
                    props = item.get("mappings", {}).get("properties")
                    if not isinstance(props, dict):
                        raise ValueError(
                            "Expected typeless mapping with 'mappings.properties' (Elasticsearch 8.x+). "
                            f"Received keys: {list(item.get('mappings', {}).keys())}"
                        )
                    properties_list.append(props)

                first_props = properties_list[0]
                if all(props == first_props for props in properties_list[1:]):
                    selected_item = next(iter(mapping.values()))
                else:
                    raise ValueError(
                        f"Alias or pattern '{index}' resolves to multiple indices with different mappings; "
                        "ensure backing indices share the same schema or target a specific index."
                    )

            properties = selected_item.get("mappings", {}).get("properties")
            if not isinstance(properties, dict):
                raise ValueError(
                    "Expected typeless mapping with 'mappings.properties' (Elasticsearch 8.x+). "
                    f"Received keys: {list(selected_item.get('mappings', {}).keys())}"
                )

            # Cache the result
            self._index_properties_cache[index] = properties

            return properties

        @staticmethod
        def _is_field_available(properties: dict, field_path: str | None) -> bool:
            """
            Determine whether a (possibly dotted) field path exists in the mapping. Supports
            nested objects (properties) and multi-fields (fields).
            """
            parts = (field_path or "").split(".")
            current = properties

            for part in parts:
                next_level = None
                if part in current:
                    next_level = current[part]
                elif "properties" in current and isinstance(current["properties"], dict) and part in current["properties"]:
                    next_level = current["properties"][part]
                elif "fields" in current and isinstance(current["fields"], dict) and part in current["fields"]:
                    next_level = current["fields"][part]
                else:
                    return False
                current = next_level

            return True

        def _metadata_for_index(self, index: str, options: dict[str, str]) -> dict[str, Any]:
            """Compute metadata (PKs, ingestion type, cursor) for a given index."""

            properties = self._fetch_index_properties(index=index)

            # Determine cursor field
            if "cursor_field" in options:
                cursor_field = options["cursor_field"]
                if cursor_field.startswith("_"):
                    raise ValueError(
                        f"Unsupported cursor_field '{cursor_field}'. Fields starting with '_' (meta fields) are not allowed as cursors."
                    )
                if not self._is_field_available(properties=properties, field_path=cursor_field):
                    raise ValueError(f"Configured cursor_field '{cursor_field}' not found in index '{index}'")

            else:
                # fallback to default cursor fields, None if no default field is available
                cursor_field = next(
                    (
                        field for field in self._default_cursor_fields
                        if self._is_field_available(properties=properties, field_path=field)
                    ),
                    None
                )

            # Determine ingestion type with optional override (requires a cursor)
            ingestion_override = str(options.get("ingestion_type", "")).lower()
            has_cursor = cursor_field is not None
            ingestion_type = "cdc" if has_cursor else "snapshot"
            if ingestion_override in {"append", "cdc"}:
                ingestion_type = ingestion_override if has_cursor else "snapshot"

            metadata: dict[str, Any] = {
                "primary_keys": ["_id"],
                "ingestion_type": ingestion_type,
            }
            if cursor_field:
                metadata["cursor_field"] = cursor_field
            return metadata

        def _map_elasticsearch_field_to_spark(self, name: str, field: dict) -> StructField:
            """
            Convert an Elasticsearch field definition to a StructField.
            Unknown or dynamic types are mapped to StringType.
            """
            field_type = field.get("type")

            # object → struct; nested → array<struct>
            if "properties" in field and field_type != "nested":
                return StructField(
                    name=name,
                    dataType=self._properties_to_struct(properties=field.get("properties", {})),
                    nullable=True,
                )

            if field_type == "nested":
                return StructField(
                    name=name,
                    dataType=ArrayType(
                        self._properties_to_struct(properties=field.get("properties", {})),
                        containsNull=True
                    ),
                    nullable=True
                )

            type_map = {
                "keyword": StringType(),
                "text": StringType(),
                "date": TimestampType(),
                "date_nanos": TimestampType(),
                "boolean": BooleanType(),
                "binary": BinaryType(),
                "long": LongType(),
                "integer": LongType(),
                "short": LongType(),
                "byte": LongType(),
                "unsigned_long": LongType(),
                "double": DoubleType(),
                "float": DoubleType(),
                "half_float": DoubleType(),
                "scaled_float": DoubleType(),
                "ip": StringType(),
                "flattened": MapType(StringType(), StringType(), True),
                "version": StringType(),
                "completion": StringType(),
                "geo_shape": StringType(),
                "geo_point": StructType(
                    [
                        StructField("lat", DoubleType(), True),
                        StructField("lon", DoubleType(), True),
                    ]
                ),
            }

            spark_type = type_map.get(field_type, StringType())
            return StructField(name=name, dataType=spark_type, nullable=True)

        def _properties_to_struct(self, properties: dict) -> StructType:
            fields = []
            for name, field in properties.items():
                fields.append(self._map_elasticsearch_field_to_spark(name=name, field=field))
            return StructType(fields)

        def _schema_for_index(self, index: str) -> StructType:
            """Get table schema for a given index."""
            properties = self._fetch_index_properties(index=index)
            return self._properties_to_struct(properties=properties)

        def _open_point_in_time(self, index: str, keep_alive: str) -> str:
            resp = self._client.post(path=f"/{index}/_pit", json={}, params={"keep_alive": keep_alive})
            pit_id = resp.get("id")
            if not pit_id:
                raise ValueError(f"Failed to open point-in-time for index {index}")
            return pit_id

        def _close_point_in_time(self, pit_id: str) -> None:
            """Best-effort PIT close to release server-side resources."""
            try:
                self._client.delete(path="/_pit", json={"id": pit_id})
            except Exception:
                # Ignore close failures; PIT will expire on its own
                pass

        def _build_search_request(
            self,
            index: str,
            cursor_field: str | None,
            start_offset: dict | None,
            size: int,
            keep_alive: str,
        ) -> tuple[str, dict]:
            offset = start_offset or {}
            pit_id = offset.get("pit_id") or self._open_point_in_time(index=index, keep_alive=keep_alive)
            search_after = offset.get("search_after")

            query: dict[str, Any] = {"match_all": {}}
            # Backward compatibility when only a cursor value is available
            if cursor_field and not search_after and offset.get("cursor") is not None:
                query = {"range": {cursor_field: {"gt": offset["cursor"]}}}

            if cursor_field:
                # Use _shard_doc as a stable tiebreaker
                sort_order = [{cursor_field: "asc"}, {"_shard_doc": "asc"}]
            else:
                # For snapshot reads without a cursor, use a deterministic shard order
                sort_order = [{"_shard_doc": "asc"}]

            body: dict[str, Any] = {
                "pit": {"id": pit_id, "keep_alive": keep_alive},
                "size": size,
                "sort": sort_order,
                "query": query,
                "track_total_hits": False,
            }

            if search_after:
                body["search_after"] = search_after

            return pit_id, body

        def _read_index(self, index: str, start_offset: dict | None, options: dict[str, str]) -> tuple[Iterator[dict], dict]:
            options = options or {}
            start_offset = start_offset or {}

            metadata = self.read_table_metadata(table_name=index, table_options=options)
            schema = self.get_table_schema(table_name=index, table_options=options)

            # Page size per Elasticsearch request (default 1000)
            size = int(options.get("page_size", 1000))
            keep_alive = str(options.get("pit_keep_alive", "1m"))
            cursor_field = options.get("cursor_field") or metadata.get("cursor_field")

            def _normalize_record(record: dict, schema: StructType) -> dict:
                """Coerce list values into structs when schema expects a StructType."""
                normalized = dict(record)
                for field in schema.fields:
                    value = normalized.get(field.name)
                    if isinstance(field.dataType, StructType) and isinstance(value, list):
                        if value:
                            normalized[field.name] = value[0] if isinstance(value[0], dict) else None
                        else:
                            normalized[field.name] = None
                return normalized

            # Eager pagination: loop over all pages (search_after + PIT) and compute the
            # final cursor before returning
            records: list[dict[str, Any]] = []
            pit_id: str | None = None
            search_after: list | None = start_offset.get("search_after")
            cursor_value = start_offset.get("cursor")
            last_sort: list | None = None

            try:
                # Open PIT up-front so keep_alive overrides always apply.
                pit_id = start_offset.get("pit_id") or self._open_point_in_time(
                    index=index, keep_alive=keep_alive
                )

                while True:
                    prev_search_after = search_after
                    effective_offset: dict[str, Any] = {"pit_id": pit_id} if pit_id else {}
                    if search_after:
                        effective_offset["search_after"] = search_after
                    if cursor_value:
                        effective_offset["cursor"] = cursor_value

                    pit_id, search_body = self._build_search_request(
                        index=index,
                        cursor_field=cursor_field,
                        start_offset=effective_offset,
                        size=size,
                        keep_alive=keep_alive,
                    )

                    response = self._client.post(path="/_search", json=search_body)
                    hits = response.get("hits", {}).get("hits", [])
                    if not hits:
                        break

                    last_sort = hits[-1].get("sort", [])
                    # Protect against infinite loops if ES repeats the same sort key.
                    if prev_search_after is not None and last_sort == prev_search_after:
                        break

                    for hit in hits:
                        record = _normalize_record(dict(hit.get("_source", {})), schema)
                        record["_id"] = hit.get("_id")
                        records.append(record)

                    search_after = last_sort
                    if cursor_field and last_sort:
                        cursor_value = last_sort[0]
            finally:
                if pit_id:
                    self._close_point_in_time(pit_id)

            next_offset: dict[str, Any] = {}
            if cursor_field and last_sort:
                next_offset = {"cursor": cursor_value or last_sort[0], "cursor_field": cursor_field}

            return iter(records), next_offset

        def list_tables(self) -> list[str]:
            """List available indices/aliases."""
            if not self._indices_cache:
                indices = self._discover_indices()
                # Cache the result
                self._indices_cache = indices
            return self._indices_cache

        def get_table_schema(self, table_name: str, table_options: dict[str, str]) -> StructType:
            self._validate_index(table_name)
            if table_name not in self.list_tables():
                raise ValueError(f"Unsupported index: {table_name}. Index is not available or not accessible.")

            # Check cache first
            if table_name in self._schema_cache:
                return self._schema_cache[table_name]

            # Get schema for index and add _id column for downstream consumers
            schema = self._schema_for_index(index=table_name)
            fields = schema.fields + [StructField("_id", StringType(), False)]
            augmented = StructType(fields)

            # Cache the result
            self._schema_cache[table_name] = augmented

            return augmented

        def read_table_metadata(self, table_name: str, table_options: dict[str, str]) -> dict[str, Any]:
            table_options = table_options or {}
            self._validate_index(table_name)
            if table_name not in self.list_tables():
                raise ValueError(f"Unsupported index: {table_name}. Index is not available or not accessible.")

            # Check cache first
            if table_name in self._metadata_cache:
                return self._metadata_cache[table_name]

            # Get metadata for index
            metadata = self._metadata_for_index(index=table_name, options=table_options)

            # Cache the result
            self._metadata_cache[table_name] = metadata

            return metadata

        def read_table(
            self, table_name: str, start_offset: dict, table_options: dict[str, str]
        ) -> tuple[Iterator[dict], dict]:

            self._validate_index(table_name)
            if table_name not in self.list_tables():
                raise ValueError(f"Unsupported index: {table_name}. Index is not available or not accessible.")

            return self._read_index(index=table_name, start_offset=start_offset, options=table_options)


    ########################################################
    # pipeline/lakeflow_python_source.py
    ########################################################

    METADATA_TABLE = "_lakeflow_metadata"
    TABLE_NAME = "tableName"
    TABLE_NAME_LIST = "tableNameList"
    TABLE_CONFIGS = "tableConfigs"
    IS_DELETE_FLOW = "isDeleteFlow"


    class LakeflowStreamReader(SimpleDataSourceStreamReader):
        """
        Implements a data source stream reader for Lakeflow Connect.
        Currently, only the simpleStreamReader is implemented, which uses a
        more generic protocol suitable for most data sources that support
        incremental loading.
        """

        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.lakeflow_connect = lakeflow_connect
            self.schema = schema

        def initialOffset(self):
            return {}

        def read(self, start: dict) -> (Iterator[tuple], dict):
            is_delete_flow = self.options.get(IS_DELETE_FLOW) == "true"
            # Strip delete flow options before passing to connector
            table_options = {
                k: v for k, v in self.options.items() if k != IS_DELETE_FLOW
            }

            if is_delete_flow:
                records, offset = self.lakeflow_connect.read_table_deletes(
                    self.options[TABLE_NAME], start, table_options
                )
            else:
                records, offset = self.lakeflow_connect.read_table(
                    self.options[TABLE_NAME], start, table_options
                )
            rows = map(lambda x: parse_value(x, self.schema), records)
            return rows, offset

        def readBetweenOffsets(self, start: dict, end: dict) -> Iterator[tuple]:
            # TODO: This does not ensure the records returned are identical across repeated calls.
            # For append-only tables, the data source must guarantee that reading from the same
            # start offset will always yield the same set of records.
            # For tables ingested as incremental CDC, it is only necessary that no new changes
            # are missed in the returned records.
            return self.read(start)[0]


    class LakeflowBatchReader(DataSourceReader):
        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.schema = schema
            self.lakeflow_connect = lakeflow_connect
            self.table_name = options[TABLE_NAME]

        def read(self, partition):
            all_records = []
            if self.table_name == METADATA_TABLE:
                all_records = self._read_table_metadata()
            else:
                all_records, _ = self.lakeflow_connect.read_table(
                    self.table_name, None, self.options
                )

            rows = map(lambda x: parse_value(x, self.schema), all_records)
            return iter(rows)

        def _read_table_metadata(self):
            table_name_list = self.options.get(TABLE_NAME_LIST, "")
            table_names = [o.strip() for o in table_name_list.split(",") if o.strip()]
            all_records = []
            table_configs = json.loads(self.options.get(TABLE_CONFIGS, "{}"))
            for table in table_names:
                metadata = self.lakeflow_connect.read_table_metadata(
                    table, table_configs.get(table, {})
                )
                all_records.append({TABLE_NAME: table, **metadata})
            return all_records


    class LakeflowSource(DataSource):
        def __init__(self, options):
            self.options = options
            self.lakeflow_connect = LakeflowConnect(options)

        @classmethod
        def name(cls):
            return "lakeflow_connect"

        def schema(self):
            table = self.options[TABLE_NAME]
            if table == METADATA_TABLE:
                return StructType(
                    [
                        StructField(TABLE_NAME, StringType(), False),
                        StructField("primary_keys", ArrayType(StringType()), True),
                        StructField("cursor_field", StringType(), True),
                        StructField("ingestion_type", StringType(), True),
                    ]
                )
            else:
                # Assuming the LakeflowConnect interface uses get_table_schema, not get_table_details
                return self.lakeflow_connect.get_table_schema(table, self.options)

        def reader(self, schema: StructType):
            return LakeflowBatchReader(self.options, schema, self.lakeflow_connect)

        def simpleStreamReader(self, schema: StructType):
            return LakeflowStreamReader(self.options, schema, self.lakeflow_connect)


    spark.dataSource.register(LakeflowSource)  # pylint: disable=undefined-variable
