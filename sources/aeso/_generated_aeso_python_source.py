# ==============================================================================
# Merged Lakeflow Source: aeso
# ==============================================================================
# This file is auto-generated by scripts/merge_python_source.py
# Do not edit manually. Make changes to the source files instead.
# ==============================================================================

from datetime import datetime, timedelta
from decimal import Decimal
from typing import (
    Any,
    Dict,
    Iterator,
    List,
)
import time

from pyspark.sql import Row
from pyspark.sql.datasource import DataSource, DataSourceReader, SimpleDataSourceStreamReader
from pyspark.sql.types import *


def register_lakeflow_source(spark):
    """Register the Lakeflow Python source with Spark."""

    ########################################################
    # libs/utils.py
    ########################################################

    def parse_value(value: Any, field_type: DataType) -> Any:
        """
        Converts a JSON value into a PySpark-compatible data type based on the provided field type.
        """
        if value is None:
            return None
        # Handle complex types
        if isinstance(field_type, StructType):
            # Validate input for StructType
            if not isinstance(value, dict):
                raise ValueError(f"Expected a dictionary for StructType, got {type(value)}")
            # Spark Python -> Arrow conversion require missing StructType fields to be assigned None.
            if value == {}:
                raise ValueError(
                    f"field in StructType cannot be an empty dict. Please assign None as the default value instead."
                )
            # For StructType, recursively parse fields into a Row
            field_dict = {}
            for field in field_type.fields:
                # When a field does not exist in the input:
                # 1. set it to None when schema marks it as nullable
                # 2. Otherwise, raise an error.
                if field.name in value:
                    field_dict[field.name] = parse_value(
                        value.get(field.name), field.dataType
                    )
                elif field.nullable:
                    field_dict[field.name] = None
                else:
                    raise ValueError(
                        f"Field {field.name} is not nullable but not found in the input"
                    )

            return Row(**field_dict)
        elif isinstance(field_type, ArrayType):
            # For ArrayType, parse each element in the array
            if not isinstance(value, list):
                # Handle edge case: single value that should be an array
                if field_type.containsNull:
                    # Try to convert to a single-element array if nulls are allowed
                    return [parse_value(value, field_type.elementType)]
                else:
                    raise ValueError(f"Expected a list for ArrayType, got {type(value)}")
            return [parse_value(v, field_type.elementType) for v in value]
        elif isinstance(field_type, MapType):
            # Handle MapType - new support
            if not isinstance(value, dict):
                raise ValueError(f"Expected a dictionary for MapType, got {type(value)}")
            return {
                parse_value(k, field_type.keyType): parse_value(v, field_type.valueType)
                for k, v in value.items()
            }
        # Handle primitive types with more robust error handling and type conversion
        try:
            if isinstance(field_type, StringType):
                # Don't convert None to "None" string
                return str(value) if value is not None else None
            elif isinstance(field_type, (IntegerType, LongType)):
                # Convert numeric strings and floats to integers
                if isinstance(value, str) and value.strip():
                    # Handle numeric strings
                    if "." in value:
                        return int(float(value))
                    return int(value)
                elif isinstance(value, (int, float)):
                    return int(value)
                raise ValueError(f"Cannot convert {value} to integer")
            elif isinstance(field_type, FloatType) or isinstance(field_type, DoubleType):
                # New support for floating point types
                if isinstance(value, str) and value.strip():
                    return float(value)
                return float(value)
            elif isinstance(field_type, DecimalType):
                # New support for Decimal type

                if isinstance(value, str) and value.strip():
                    return Decimal(value)
                return Decimal(str(value))
            elif isinstance(field_type, BooleanType):
                # Enhanced boolean conversion
                if isinstance(value, str):
                    lowered = value.lower()
                    if lowered in ("true", "t", "yes", "y", "1"):
                        return True
                    elif lowered in ("false", "f", "no", "n", "0"):
                        return False
                return bool(value)
            elif isinstance(field_type, DateType):
                # New support for DateType
                if isinstance(value, str):
                    # Try multiple date formats
                    for fmt in ("%Y-%m-%d", "%m/%d/%Y", "%d-%m-%Y", "%Y/%m/%d"):
                        try:
                            return datetime.strptime(value, fmt).date()
                        except ValueError:
                            continue
                    # ISO format as fallback
                    return datetime.fromisoformat(value).date()
                elif isinstance(value, datetime):
                    return value.date()
                raise ValueError(f"Cannot convert {value} to date")
            elif isinstance(field_type, TimestampType):
                # Enhanced timestamp handling
                if isinstance(value, str):
                    # Handle multiple timestamp formats including Z and timezone offsets
                    if value.endswith("Z"):
                        value = value.replace("Z", "+00:00")
                    try:
                        return datetime.fromisoformat(value)
                    except ValueError:
                        # Try additional formats if ISO format fails
                        for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S"):
                            try:
                                return datetime.strptime(value, fmt)
                            except ValueError:
                                continue
                elif isinstance(value, (int, float)):
                    # Handle Unix timestamps
                    return datetime.fromtimestamp(value)
                elif isinstance(value, datetime):
                    return value
                raise ValueError(f"Cannot convert {value} to timestamp")
            else:
                # Check for custom UDT handling
                if hasattr(field_type, "fromJson"):
                    # Support for User Defined Types that implement fromJson
                    return field_type.fromJson(value)
                raise TypeError(f"Unsupported field type: {field_type}")
        except (ValueError, TypeError) as e:
            # Add context to the error
            raise ValueError(
                f"Error converting '{value}' ({type(value)}) to {field_type}: {str(e)}"
            )


    ########################################################
    # sources/aeso/aeso.py
    ########################################################

    class LakeflowConnect:
        """AESO connector - provides access to Alberta electricity pool price data."""

        def __init__(self, options: Dict[str, str]) -> None:
            """Initialize AESO connector.

            Args:
                options: Configuration with api_key (required), start_date (optional), 
                        lookback_hours (optional, default 24)
            """
            if "api_key" not in options:
                raise ValueError("api_key is required in options")

            self.api_key = options["api_key"]
            self.start_date = options.get("start_date")
            self.lookback_hours = int(options.get("lookback_hours", 24))

            if self.start_date:
                try:
                    datetime.strptime(self.start_date, "%Y-%m-%d")
                except ValueError:
                    raise ValueError("start_date must be in YYYY-MM-DD format")

            # Initialize the AESO API client
            try:
                from aeso import AESOAPI
                self.aeso_client = AESOAPI(self.api_key)
            except ImportError:
                raise ImportError(
                    "aeso-python-api package is required. "
                    "Install it with: pip install aeso-python-api"
                )
            except Exception as e:
                raise RuntimeError(f"Failed to initialize AESO API client: {e}")

        def list_tables(self) -> List[str]:
            """List available tables."""
            return ["pool_price"]

        def get_table_schema(
            self, table_name: str, table_options: Dict[str, str]
        ) -> StructType:
            """Get table schema."""
            if table_name not in self.list_tables():
                raise ValueError(
                    f"Table '{table_name}' is not supported. "
                    f"Supported tables: {', '.join(self.list_tables())}"
                )

            if table_name == "pool_price":
                return StructType([
                    StructField("begin_datetime_utc", TimestampType(), False),
                    StructField("pool_price", DoubleType(), True),  # Nullable - null for future hours with only forecasts
                    StructField("forecast_pool_price", DoubleType(), True),
                    StructField("rolling_30day_avg", DoubleType(), True),
                    StructField("ingestion_time", TimestampType(), False),  # UTC timestamp when row was ingested
                ])

            raise ValueError(f"Unknown table: {table_name}")

        def read_table_metadata(
            self, table_name: str, table_options: Dict[str, str]
        ) -> Dict[str, any]:
            """Get table metadata (primary keys, cursor, ingestion type)."""
            if table_name not in self.list_tables():
                raise ValueError(
                    f"Table '{table_name}' is not supported. "
                    f"Supported tables: {', '.join(self.list_tables())}"
                )

            if table_name == "pool_price":
                return {
                    "primary_keys": ["begin_datetime_utc"],
                    "cursor_field": "begin_datetime_utc",
                    "sequence_by": "ingestion_time",
                    "ingestion_type": "cdc"
                }

            raise ValueError(f"Unknown table: {table_name}")

        def read_table(
            self, table_name: str, start_offset: dict, table_options: Dict[str, str]
        ) -> (Iterator[dict], dict):
            """Read table data with CDC incremental support."""
            if table_name not in self.list_tables():
                raise ValueError(
                    f"Table '{table_name}' is not supported. "
                    f"Supported tables: {', '.join(self.list_tables())}"
                )

            if table_name == "pool_price":
                return self._read_pool_price_table(start_offset)

            raise ValueError(f"Unknown table: {table_name}")

        def _read_pool_price_table(self, start_offset: dict) -> (Iterator[dict], dict):
            """Read pool price data with CDC incremental loading.

            Uses lookback window to recapture recently updated records due to
            late-arriving data and settlement adjustments.
            """
            if not start_offset:
                start_offset = {}

            high_watermark = start_offset.get("high_watermark")

            if high_watermark:
                # Incremental: Apply lookback window for CDC
                try:
                    hwm_dt = datetime.fromisoformat(high_watermark.replace('Z', '+00:00'))
                    fetch_start_dt = hwm_dt - timedelta(hours=self.lookback_hours)
                    fetch_start_date = fetch_start_dt.strftime("%Y-%m-%d")
                    print(f"CDC: watermark={high_watermark}, lookback={self.lookback_hours}h, start={fetch_start_date}")
                except (ValueError, AttributeError) as e:
                    raise ValueError(f"Invalid high_watermark: {high_watermark}, error: {e}")
            else:
                # Initial: use start_date or default to 30 days ago
                fetch_start_date = self.start_date or (datetime.now() - timedelta(days=30)).strftime("%Y-%m-%d")
                print(f"Initial load from {fetch_start_date}")

            # End date always set to today for continuous ingestion
            today = datetime.now().strftime("%Y-%m-%d")

            if fetch_start_date > today:
                print(f"Start date {fetch_start_date} is in future, no data to fetch")
                return iter([]), start_offset

            all_records = []
            current_date = fetch_start_date
            fetch_end_date = today
            max_batch_days = 30  # Fetch in 7-day batches

            print(f"Fetching: {fetch_start_date} to {fetch_end_date}")

            # Capture ingestion timestamp once for this batch
            ingestion_timestamp = datetime.utcnow()

            while current_date <= fetch_end_date:
                batch_end_dt = datetime.strptime(current_date, "%Y-%m-%d") + timedelta(days=max_batch_days - 1)
                batch_end_date = min(batch_end_dt.strftime("%Y-%m-%d"), fetch_end_date)

                try:
                    print(f"Batch: {current_date} to {batch_end_date}")

                    # Both start_date and end_date required by AESO API
                    pool_prices = self.aeso_client.get_pool_price_report(
                        start_date=current_date,
                        end_date=batch_end_date
                    )

                    for price_obj in pool_prices:
                        # Get UTC datetime from source (natively UTC)
                        utc_dt = price_obj.begin_datetime_utc

                        # Remove timezone info if present (keep as naive UTC datetime)
                        if hasattr(utc_dt, 'tzinfo') and utc_dt.tzinfo is not None:
                            utc_dt = utc_dt.replace(tzinfo=None)

                        all_records.append({
                            "begin_datetime_utc": utc_dt,
                            "pool_price": float(price_obj.pool_price) if price_obj.pool_price is not None else None,
                            "forecast_pool_price": float(price_obj.forecast_pool_price) if price_obj.forecast_pool_price is not None else None,
                            "rolling_30day_avg": float(price_obj.rolling_30day_avg) if price_obj.rolling_30day_avg is not None else None,
                            "ingestion_time": ingestion_timestamp,
                        })

                    print(f"Fetched {len(pool_prices)} records")
                    time.sleep(0.1)  # Rate limit protection

                except Exception as e:
                    print(f"Error: {e}")
                    if all_records:
                        latest_timestamp = max(r["begin_datetime_utc"] for r in all_records)
                        next_offset = {"high_watermark": latest_timestamp.isoformat()}
                        print(f"Partial success: {len(all_records)} records, watermark: {next_offset['high_watermark']}")
                        return iter(all_records), next_offset
                    else:
                        print("No records fetched, retrying with same offset")
                        return iter([]), start_offset

                current_date = (datetime.strptime(batch_end_date, "%Y-%m-%d") + timedelta(days=1)).strftime("%Y-%m-%d")

            # Calculate new high watermark
            if all_records:
                latest_timestamp = max(r["begin_datetime_utc"] for r in all_records)
                next_high_watermark = latest_timestamp.isoformat()
            else:
                next_high_watermark = high_watermark or datetime.strptime(fetch_end_date, "%Y-%m-%d").replace(
                    hour=23, minute=59, second=59
                ).isoformat()

            print(f"Success: {len(all_records)} records, watermark: {next_high_watermark}")
            return iter(all_records), {"high_watermark": next_high_watermark}


    ########################################################
    # pipeline/lakeflow_python_source.py
    ########################################################

    METADATA_TABLE = "_lakeflow_metadata"
    TABLE_NAME = "tableName"
    TABLE_NAME_LIST = "tableNameList"


    class LakeflowStreamReader(SimpleDataSourceStreamReader):
        """
        Implements a data source stream reader for Lakeflow Connect.
        Currently, only the simpleStreamReader is implemented, which uses a
        more generic protocol suitable for most data sources that support
        incremental loading.
        """

        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.lakeflow_connect = lakeflow_connect
            self.schema = schema

        def initialOffset(self):
            return {}

        def read(self, start: dict) -> (Iterator[tuple], dict):
            records, offset = self.lakeflow_connect.read_table(
                self.options["tableName"], start, self.options
            )
            rows = map(lambda x: parse_value(x, self.schema), records)
            return rows, offset

        def readBetweenOffsets(self, start: dict, end: dict) -> Iterator[tuple]:
            # TODO: This does not ensure the records returned are identical across repeated calls.
            # For append-only tables, the data source must guarantee that reading from the same
            # start offset will always yield the same set of records.
            # For tables ingested as incremental CDC, it is only necessary that no new changes
            # are missed in the returned records.
            return self.read(start)[0]


    class LakeflowBatchReader(DataSourceReader):
        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.schema = schema
            self.lakeflow_connect = lakeflow_connect
            self.table_name = options[TABLE_NAME]

        def read(self, partition):
            all_records = []
            if self.table_name == METADATA_TABLE:
                all_records = self._read_table_metadata()
            else:
                all_records, _ = self.lakeflow_connect.read_table(
                    self.table_name, None, self.options
                )

            rows = map(lambda x: parse_value(x, self.schema), all_records)
            return iter(rows)

        def _read_table_metadata(self):
            table_name_list = self.options.get(TABLE_NAME_LIST, "")
            table_names = [o.strip() for o in table_name_list.split(",") if o.strip()]
            all_records = []
            for table in table_names:
                metadata = self.lakeflow_connect.read_table_metadata(table, self.options)
                all_records.append({"tableName": table, **metadata})
            return all_records


    class LakeflowSource(DataSource):
        def __init__(self, options):
            self.options = options
            self.lakeflow_connect = LakeflowConnect(options)

        @classmethod
        def name(cls):
            return "lakeflow_connect"

        def schema(self):
            table = self.options["tableName"]
            if table == METADATA_TABLE:
                return StructType(
                    [
                        StructField("tableName", StringType(), False),
                        StructField("primary_keys", ArrayType(StringType()), True),
                        StructField("cursor_field", StringType(), True),
                        StructField("ingestion_type", StringType(), True),
                    ]
                )
            else:
                # Assuming the LakeflowConnect interface uses get_table_schema, not get_table_details
                return self.lakeflow_connect.get_table_schema(table, self.options)

        def reader(self, schema: StructType):
            return LakeflowBatchReader(self.options, schema, self.lakeflow_connect)

        def simpleStreamReader(self, schema: StructType):
            return LakeflowStreamReader(self.options, schema, self.lakeflow_connect)


    spark.dataSource.register(LakeflowSource)
