# ==============================================================================
# Merged Lakeflow Source: segment
# ==============================================================================
# This file is auto-generated by tools/scripts/merge_python_source.py
# Do not edit manually. Make changes to the source files instead.
# ==============================================================================

from datetime import datetime
from decimal import Decimal
from typing import Any, Iterator
import json

from pyspark.sql import Row
from pyspark.sql.datasource import DataSource, DataSourceReader, SimpleDataSourceStreamReader
from pyspark.sql.types import *
import base64


def register_lakeflow_source(spark):
    """Register the Lakeflow Python source with Spark."""

    ########################################################
    # libs/utils.py
    ########################################################

    def _parse_struct(value: Any, field_type: StructType) -> Row:
        """Parse a dictionary into a PySpark Row based on StructType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for StructType, got {type(value)}")
        # Spark Python -> Arrow conversion require missing StructType fields to be assigned None.
        if value == {}:
            raise ValueError(
                "field in StructType cannot be an empty dict. "
                "Please assign None as the default value instead."
            )
        field_dict = {}
        for field in field_type.fields:
            if field.name in value:
                field_dict[field.name] = parse_value(value.get(field.name), field.dataType)
            elif field.nullable:
                field_dict[field.name] = None
            else:
                raise ValueError(f"Field {field.name} is not nullable but not found in the input")
        return Row(**field_dict)


    def _parse_array(value: Any, field_type: ArrayType) -> list:
        """Parse a list into a PySpark array based on ArrayType schema."""
        if not isinstance(value, list):
            if field_type.containsNull:
                return [parse_value(value, field_type.elementType)]
            raise ValueError(f"Expected a list for ArrayType, got {type(value)}")
        return [parse_value(v, field_type.elementType) for v in value]


    def _parse_map(value: Any, field_type: MapType) -> dict:
        """Parse a dictionary into a PySpark map based on MapType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for MapType, got {type(value)}")
        return {
            parse_value(k, field_type.keyType): parse_value(v, field_type.valueType)
            for k, v in value.items()
        }


    def _parse_string(value: Any) -> str:
        """Convert value to string."""
        return str(value)


    def _parse_integer(value: Any) -> int:
        """Convert value to integer."""
        if isinstance(value, str) and value.strip():
            return int(float(value)) if "." in value else int(value)
        if isinstance(value, (int, float)):
            return int(value)
        raise ValueError(f"Cannot convert {value} to integer")


    def _parse_float(value: Any) -> float:
        """Convert value to float."""
        return float(value)


    def _parse_decimal(value: Any) -> Decimal:
        """Convert value to Decimal."""
        return Decimal(value) if isinstance(value, str) and value.strip() else Decimal(str(value))


    def _parse_boolean(value: Any) -> bool:
        """Convert value to boolean."""
        if isinstance(value, str):
            lowered = value.lower()
            if lowered in ("true", "t", "yes", "y", "1"):
                return True
            if lowered in ("false", "f", "no", "n", "0"):
                return False
        return bool(value)


    def _parse_date(value: Any) -> datetime.date:
        """Convert value to date."""
        if isinstance(value, str):
            for fmt in ("%Y-%m-%d", "%m/%d/%Y", "%d-%m-%Y", "%Y/%m/%d"):
                try:
                    return datetime.strptime(value, fmt).date()
                except ValueError:
                    continue
            return datetime.fromisoformat(value).date()
        if isinstance(value, datetime):
            return value.date()
        raise ValueError(f"Cannot convert {value} to date")


    def _parse_timestamp(value: Any) -> datetime:
        """Convert value to timestamp."""
        if isinstance(value, str):
            ts_value = value.replace("Z", "+00:00") if value.endswith("Z") else value
            try:
                return datetime.fromisoformat(ts_value)
            except ValueError:
                for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S"):
                    try:
                        return datetime.strptime(ts_value, fmt)
                    except ValueError:
                        continue
        elif isinstance(value, (int, float)):
            return datetime.fromtimestamp(value)
        elif isinstance(value, datetime):
            return value
        raise ValueError(f"Cannot convert {value} to timestamp")


    def _decode_string_to_bytes(value: str) -> bytes:
        """Try to decode a string as base64, then hex, then UTF-8."""
        try:
            return base64.b64decode(value)
        except Exception:
            pass
        try:
            return bytes.fromhex(value)
        except Exception:
            pass
        return value.encode("utf-8")


    def _parse_binary(value: Any) -> bytes:
        """Convert value to bytes. Tries base64, then hex, then UTF-8 for strings."""
        if isinstance(value, bytes):
            return value
        if isinstance(value, bytearray):
            return bytes(value)
        if isinstance(value, str):
            return _decode_string_to_bytes(value)
        if isinstance(value, list):
            return bytes(value)
        return str(value).encode("utf-8")


    # Mapping of primitive types to their parser functions
    _PRIMITIVE_PARSERS = {
        StringType: _parse_string,
        IntegerType: _parse_integer,
        LongType: _parse_integer,
        FloatType: _parse_float,
        DoubleType: _parse_float,
        DecimalType: _parse_decimal,
        BooleanType: _parse_boolean,
        DateType: _parse_date,
        TimestampType: _parse_timestamp,
        BinaryType: _parse_binary,
    }


    def parse_value(value: Any, field_type: DataType) -> Any:
        """
        Converts a JSON value into a PySpark-compatible data type based on the provided field type.
        """
        if value is None:
            return None

        # Handle complex types
        if isinstance(field_type, StructType):
            return _parse_struct(value, field_type)
        if isinstance(field_type, ArrayType):
            return _parse_array(value, field_type)
        if isinstance(field_type, MapType):
            return _parse_map(value, field_type)

        # Handle primitive types via type-based lookup
        try:
            field_type_class = type(field_type)
            if field_type_class in _PRIMITIVE_PARSERS:
                return _PRIMITIVE_PARSERS[field_type_class](value)

            # Check for custom UDT handling
            if hasattr(field_type, "fromJson"):
                return field_type.fromJson(value)

            raise TypeError(f"Unsupported field type: {field_type}")
        except (ValueError, TypeError) as e:
            raise ValueError(f"Error converting '{value}' ({type(value)}) to {field_type}: {str(e)}")


    ########################################################
    # sources/segment/segment.py
    ########################################################

    """Segment Public API Connector for Lakeflow Connect.

    This connector implements the LakeflowConnect interface to ingest data from
    Segment's Public API, which provides access to workspace configuration and metadata.
    """
    from typing import Iterator, Any

    import requests
    from pyspark.sql.types import (
        StructType,
        StructField,
        LongType,
        StringType,
        BooleanType,
        ArrayType,
        MapType,
    )


    class LakeflowConnect:
        """Segment Public API connector implementation."""

        # Supported tables
        SUPPORTED_TABLES = [
            "sources",
            "destinations",
            "warehouses",
            "catalog_sources",
            "catalog_destinations",
            "catalog_warehouses",
            "tracking_plans",
            "users",
            "labels",
            "audit_events",
            "spaces",
            "reverse_etl_models",
            "transformations",
            "usage_api_calls_daily",
            "usage_mtu_daily",
        ]

        def __init__(self, options: dict[str, str]) -> None:
            """
            Initialize the Segment connector with connection-level options.

            Expected options:
                - api_token (required): Public API token from Segment's Workspace settings.
                - region (optional): API region - 'api' for US or 'eu1' for EU (default: 'api').
            """
            api_token = options.get("api_token")
            if not api_token:
                raise ValueError("Segment connector requires 'api_token' in options")

            region = options.get("region", "api")
            if region not in ("api", "eu1"):
                raise ValueError(
                    f"Invalid region '{region}'. Must be 'api' (US) or 'eu1' (EU)"
                )

            self.base_url = f"https://{region}.segmentapis.com"

            # Configure a session with proper headers for Segment Public API
            self._session = requests.Session()
            self._session.headers.update(
                {
                    "Authorization": f"Bearer {api_token}",
                    "Content-Type": "application/json",
                }
            )

        def list_tables(self) -> list[str]:
            """
            List names of all tables supported by this connector.
            """
            return self.SUPPORTED_TABLES.copy()

        def _validate_table(self, table_name: str) -> None:
            """Validate that the table is supported."""
            if table_name not in self.SUPPORTED_TABLES:
                raise ValueError(
                    f"Unsupported table: {table_name!r}. "
                    f"Supported tables: {self.SUPPORTED_TABLES}"
                )

        # -------------------------------------------------------------------------
        # Schema definitions
        # -------------------------------------------------------------------------

        def _get_logos_struct(self) -> StructType:
            """Return the logos struct schema."""
            return StructType(
                [
                    StructField("default", StringType(), True),
                    StructField("mark", StringType(), True),
                    StructField("alt", StringType(), True),
                ]
            )

        def _get_metadata_struct(self) -> StructType:
            """Return the common metadata struct schema for sources/destinations/warehouses."""
            return StructType(
                [
                    StructField("id", StringType(), True),
                    StructField("slug", StringType(), True),
                    StructField("name", StringType(), True),
                    StructField("description", StringType(), True),
                    StructField("categories", ArrayType(StringType(), True), True),
                    StructField("logos", self._get_logos_struct(), True),
                ]
            )

        def _get_label_struct(self) -> StructType:
            """Return the label struct schema."""
            return StructType(
                [
                    StructField("key", StringType(), True),
                    StructField("value", StringType(), True),
                ]
            )

        def _get_sources_schema(self) -> StructType:
            """Return the sources table schema."""
            return StructType(
                [
                    StructField("id", StringType(), False),
                    StructField("slug", StringType(), True),
                    StructField("name", StringType(), True),
                    StructField("workspaceId", StringType(), True),
                    StructField("enabled", BooleanType(), True),
                    StructField("writeKeys", ArrayType(StringType(), True), True),
                    StructField("metadata", self._get_metadata_struct(), True),
                    StructField("settings", MapType(StringType(), StringType(), True), True),
                    StructField("labels", ArrayType(self._get_label_struct(), True), True),
                    StructField("createdAt", StringType(), True),
                    StructField("updatedAt", StringType(), True),
                ]
            )

        def _get_destinations_schema(self) -> StructType:
            """Return the destinations table schema."""
            return StructType(
                [
                    StructField("id", StringType(), False),
                    StructField("name", StringType(), True),
                    StructField("enabled", BooleanType(), True),
                    StructField("workspaceId", StringType(), True),
                    StructField("sourceId", StringType(), True),
                    StructField("metadata", self._get_metadata_struct(), True),
                    StructField("settings", MapType(StringType(), StringType(), True), True),
                    StructField("createdAt", StringType(), True),
                    StructField("updatedAt", StringType(), True),
                ]
            )

        def _get_warehouses_schema(self) -> StructType:
            """Return the warehouses table schema."""
            return StructType(
                [
                    StructField("id", StringType(), False),
                    StructField("name", StringType(), True),
                    StructField("workspaceId", StringType(), True),
                    StructField("enabled", BooleanType(), True),
                    StructField("metadata", self._get_metadata_struct(), True),
                    StructField("settings", MapType(StringType(), StringType(), True), True),
                    StructField("createdAt", StringType(), True),
                    StructField("updatedAt", StringType(), True),
                ]
            )

        def _get_catalog_sources_schema(self) -> StructType:
            """Return the catalog_sources table schema."""
            return StructType(
                [
                    StructField("id", StringType(), False),
                    StructField("slug", StringType(), True),
                    StructField("name", StringType(), True),
                    StructField("description", StringType(), True),
                    StructField("categories", ArrayType(StringType(), True), True),
                    StructField("logos", self._get_logos_struct(), True),
                    StructField("isCloudEventSource", BooleanType(), True),
                ]
            )

        def _get_catalog_destinations_schema(self) -> StructType:
            """Return the catalog_destinations table schema."""
            return StructType(
                [
                    StructField("id", StringType(), False),
                    StructField("slug", StringType(), True),
                    StructField("name", StringType(), True),
                    StructField("description", StringType(), True),
                    StructField("categories", ArrayType(StringType(), True), True),
                    StructField("logos", self._get_logos_struct(), True),
                    StructField("status", StringType(), True),
                    StructField(
                        "supportedMethods", MapType(StringType(), BooleanType(), True), True
                    ),
                ]
            )

        def _get_catalog_warehouses_schema(self) -> StructType:
            """Return the catalog_warehouses table schema."""
            return StructType(
                [
                    StructField("id", StringType(), False),
                    StructField("slug", StringType(), True),
                    StructField("name", StringType(), True),
                    StructField("description", StringType(), True),
                    StructField("logos", self._get_logos_struct(), True),
                ]
            )

        def _get_tracking_plans_schema(self) -> StructType:
            """Return the tracking_plans table schema."""
            return StructType(
                [
                    StructField("id", StringType(), False),
                    StructField("slug", StringType(), True),
                    StructField("name", StringType(), True),
                    StructField("description", StringType(), True),
                    StructField("type", StringType(), True),
                    StructField("workspaceId", StringType(), True),
                    StructField("createdAt", StringType(), True),
                    StructField("updatedAt", StringType(), True),
                ]
            )

        def _get_users_schema(self) -> StructType:
            """Return the users table schema."""
            permission_struct = StructType(
                [
                    StructField("roleId", StringType(), True),
                    StructField("roleName", StringType(), True),
                    StructField("resources", ArrayType(MapType(StringType(), StringType(), True), True), True),
                ]
            )
            return StructType(
                [
                    StructField("id", StringType(), False),
                    StructField("name", StringType(), True),
                    StructField("email", StringType(), True),
                    StructField("permissions", ArrayType(permission_struct, True), True),
                ]
            )

        def _get_labels_schema(self) -> StructType:
            """Return the labels table schema."""
            return StructType(
                [
                    StructField("key", StringType(), False),
                    StructField("value", StringType(), False),
                    StructField("description", StringType(), True),
                ]
            )

        def _get_audit_events_schema(self) -> StructType:
            """Return the audit_events table schema."""
            actor_struct = StructType(
                [
                    StructField("id", StringType(), True),
                    StructField("type", StringType(), True),
                    StructField("email", StringType(), True),
                ]
            )
            resource_struct = StructType(
                [
                    StructField("id", StringType(), True),
                    StructField("type", StringType(), True),
                    StructField("name", StringType(), True),
                ]
            )
            return StructType(
                [
                    StructField("id", StringType(), False),
                    StructField("timestamp", StringType(), True),
                    StructField("type", StringType(), True),
                    StructField("actor", actor_struct, True),
                    StructField("resource", resource_struct, True),
                ]
            )

        def _get_spaces_schema(self) -> StructType:
            """Return the spaces table schema."""
            return StructType(
                [
                    StructField("id", StringType(), False),
                    StructField("slug", StringType(), True),
                    StructField("name", StringType(), True),
                    StructField("createdAt", StringType(), True),
                    StructField("updatedAt", StringType(), True),
                ]
            )

        def _get_reverse_etl_models_schema(self) -> StructType:
            """Return the reverse_etl_models table schema."""
            return StructType(
                [
                    StructField("id", StringType(), False),
                    StructField("name", StringType(), True),
                    StructField("description", StringType(), True),
                    StructField("enabled", BooleanType(), True),
                    StructField("sourceId", StringType(), True),
                    StructField("scheduleStrategy", StringType(), True),
                    StructField("query", StringType(), True),
                    StructField("queryIdentifierColumn", StringType(), True),
                    StructField("createdAt", StringType(), True),
                    StructField("updatedAt", StringType(), True),
                ]
            )

        def _get_transformations_schema(self) -> StructType:
            """Return the transformations table schema."""
            return StructType(
                [
                    StructField("id", StringType(), False),
                    StructField("name", StringType(), True),
                    StructField("sourceId", StringType(), True),
                    StructField("destinationMetadataId", StringType(), True),
                    StructField("enabled", BooleanType(), True),
                    StructField("code", StringType(), True),
                    StructField("createdAt", StringType(), True),
                    StructField("updatedAt", StringType(), True),
                ]
            )

        def _get_usage_api_calls_daily_schema(self) -> StructType:
            """Return the usage_api_calls_daily table schema."""
            return StructType(
                [
                    StructField("timestamp", StringType(), False),
                    StructField("apiCalls", LongType(), True),
                    StructField("sourceId", StringType(), True),
                    StructField("workspaceId", StringType(), True),
                ]
            )

        def _get_usage_mtu_daily_schema(self) -> StructType:
            """Return the usage_mtu_daily table schema."""
            return StructType(
                [
                    StructField("timestamp", StringType(), False),
                    StructField("mtu", LongType(), True),
                    StructField("sourceId", StringType(), True),
                    StructField("workspaceId", StringType(), True),
                ]
            )

        def get_table_schema(
            self, table_name: str, table_options: dict[str, str]
        ) -> StructType:
            """
            Fetch the schema of a table.
            """
            self._validate_table(table_name)

            schema_map = {
                "sources": self._get_sources_schema,
                "destinations": self._get_destinations_schema,
                "warehouses": self._get_warehouses_schema,
                "catalog_sources": self._get_catalog_sources_schema,
                "catalog_destinations": self._get_catalog_destinations_schema,
                "catalog_warehouses": self._get_catalog_warehouses_schema,
                "tracking_plans": self._get_tracking_plans_schema,
                "users": self._get_users_schema,
                "labels": self._get_labels_schema,
                "audit_events": self._get_audit_events_schema,
                "spaces": self._get_spaces_schema,
                "reverse_etl_models": self._get_reverse_etl_models_schema,
                "transformations": self._get_transformations_schema,
                "usage_api_calls_daily": self._get_usage_api_calls_daily_schema,
                "usage_mtu_daily": self._get_usage_mtu_daily_schema,
            }

            return schema_map[table_name]()

        def read_table_metadata(
            self, table_name: str, table_options: dict[str, str]
        ) -> dict:
            """
            Fetch metadata for the given table.
            """
            self._validate_table(table_name)

            metadata_map = {
                "sources": {
                    "primary_keys": ["id"],
                    "ingestion_type": "snapshot",
                },
                "destinations": {
                    "primary_keys": ["id"],
                    "ingestion_type": "snapshot",
                },
                "warehouses": {
                    "primary_keys": ["id"],
                    "ingestion_type": "snapshot",
                },
                "catalog_sources": {
                    "primary_keys": ["id"],
                    "ingestion_type": "snapshot",
                },
                "catalog_destinations": {
                    "primary_keys": ["id"],
                    "ingestion_type": "snapshot",
                },
                "catalog_warehouses": {
                    "primary_keys": ["id"],
                    "ingestion_type": "snapshot",
                },
                "tracking_plans": {
                    "primary_keys": ["id"],
                    "ingestion_type": "snapshot",
                },
                "users": {
                    "primary_keys": ["id"],
                    "ingestion_type": "snapshot",
                },
                "labels": {
                    "primary_keys": ["key", "value"],
                    "ingestion_type": "snapshot",
                },
                "audit_events": {
                    "primary_keys": ["id"],
                    "ingestion_type": "snapshot",
                },
                "spaces": {
                    "primary_keys": ["id"],
                    "ingestion_type": "snapshot",
                },
                "reverse_etl_models": {
                    "primary_keys": ["id"],
                    "ingestion_type": "snapshot",
                },
                "transformations": {
                    "primary_keys": ["id"],
                    "ingestion_type": "snapshot",
                },
                "usage_api_calls_daily": {
                    "primary_keys": ["timestamp"],
                    "cursor_field": "timestamp",
                    "ingestion_type": "cdc",
                },
                "usage_mtu_daily": {
                    "primary_keys": ["timestamp"],
                    "cursor_field": "timestamp",
                    "ingestion_type": "cdc",
                },
            }

            return metadata_map[table_name]

        def read_table(
            self, table_name: str, start_offset: dict, table_options: dict[str, str]
        ) -> tuple[Iterator[dict], dict]:
            """
            Read records from a table and return raw JSON-like dictionaries.
            """
            self._validate_table(table_name)

            reader_map = {
                "sources": self._read_sources,
                "destinations": self._read_destinations,
                "warehouses": self._read_warehouses,
                "catalog_sources": self._read_catalog_sources,
                "catalog_destinations": self._read_catalog_destinations,
                "catalog_warehouses": self._read_catalog_warehouses,
                "tracking_plans": self._read_tracking_plans,
                "users": self._read_users,
                "labels": self._read_labels,
                "audit_events": self._read_audit_events,
                "spaces": self._read_spaces,
                "reverse_etl_models": self._read_reverse_etl_models,
                "transformations": self._read_transformations,
                "usage_api_calls_daily": self._read_usage_api_calls_daily,
                "usage_mtu_daily": self._read_usage_mtu_daily,
            }

            return reader_map[table_name](start_offset, table_options)

        # -------------------------------------------------------------------------
        # Helper methods for API requests
        # -------------------------------------------------------------------------

        def _paginated_get(
            self,
            endpoint: str,
            data_key: str,
            table_options: dict[str, str],
            params: dict[str, Any] | None = None,
        ) -> list[dict[str, Any]]:
            """
            Perform a paginated GET request and return all records.

            Args:
                endpoint: API endpoint path (e.g., '/sources')
                data_key: Key in response.data containing the array of records
                table_options: Table options that may contain pagination settings
                params: Optional query parameters

            Returns:
                List of all records from all pages
            """
            try:
                max_pages = int(table_options.get("max_pages", 100))
            except (TypeError, ValueError):
                max_pages = 100

            try:
                page_size = int(table_options.get("page_size", 100))
            except (TypeError, ValueError):
                page_size = 100

            url = f"{self.base_url}{endpoint}"
            records: list[dict[str, Any]] = []
            cursor: str | None = None
            pages_fetched = 0

            while pages_fetched < max_pages:
                request_params = params.copy() if params else {}
                request_params["pagination.count"] = page_size
                if cursor:
                    request_params["pagination.cursor"] = cursor

                response = self._session.get(url, params=request_params, timeout=60)
                if response.status_code == 403:
                    # 403 Forbidden typically means the feature is not enabled for this workspace
                    # (e.g., Audit Trail requires enterprise plan). Return empty results gracefully.
                    return records
                if response.status_code != 200:
                    raise RuntimeError(
                        f"Segment API error for {endpoint}: "
                        f"{response.status_code} {response.text}"
                    )

                response_data = response.json()
                data = response_data.get("data", {})

                # Extract records from the response
                items = data.get(data_key, [])
                if isinstance(items, list):
                    records.extend(items)
                elif isinstance(items, dict):
                    # Some endpoints return a single object, wrap in list
                    records.append(items)

                # Check for next page
                pagination = response_data.get("pagination", {})
                next_cursor = pagination.get("next")
                if not next_cursor or next_cursor == cursor:
                    break

                cursor = next_cursor
                pages_fetched += 1

            return records

        # -------------------------------------------------------------------------
        # Table reader implementations
        # -------------------------------------------------------------------------

        def _read_sources(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> tuple[Iterator[dict], dict]:
            """Read the sources table."""
            records = self._paginated_get("/sources", "sources", table_options)
            return iter(records), {}

        def _read_destinations(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> tuple[Iterator[dict], dict]:
            """Read the destinations table."""
            records = self._paginated_get("/destinations", "destinations", table_options)
            return iter(records), {}

        def _read_warehouses(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> tuple[Iterator[dict], dict]:
            """Read the warehouses table."""
            records = self._paginated_get("/warehouses", "warehouses", table_options)
            return iter(records), {}

        def _read_catalog_sources(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> tuple[Iterator[dict], dict]:
            """Read the catalog_sources table."""
            records = self._paginated_get(
                "/catalog/sources", "sourcesCatalog", table_options
            )
            return iter(records), {}

        def _read_catalog_destinations(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> tuple[Iterator[dict], dict]:
            """Read the catalog_destinations table."""
            records = self._paginated_get(
                "/catalog/destinations", "destinationsCatalog", table_options
            )
            return iter(records), {}

        def _read_catalog_warehouses(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> tuple[Iterator[dict], dict]:
            """Read the catalog_warehouses table."""
            records = self._paginated_get(
                "/catalog/warehouses", "warehousesCatalog", table_options
            )
            return iter(records), {}

        def _read_tracking_plans(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> tuple[Iterator[dict], dict]:
            """Read the tracking_plans table."""
            records = self._paginated_get(
                "/tracking-plans", "trackingPlans", table_options
            )
            return iter(records), {}

        def _read_users(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> tuple[Iterator[dict], dict]:
            """Read the users table."""
            records = self._paginated_get("/users", "users", table_options)
            return iter(records), {}

        def _read_labels(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> tuple[Iterator[dict], dict]:
            """Read the labels table."""
            records = self._paginated_get("/labels", "labels", table_options)
            return iter(records), {}

        def _read_audit_events(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> tuple[Iterator[dict], dict]:
            """Read the audit_events table."""
            records = self._paginated_get("/audit-events", "auditEvents", table_options)
            return iter(records), {}

        def _read_spaces(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> tuple[Iterator[dict], dict]:
            """Read the spaces table."""
            records = self._paginated_get("/spaces", "spaces", table_options)
            return iter(records), {}

        def _read_reverse_etl_models(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> tuple[Iterator[dict], dict]:
            """Read the reverse_etl_models table."""
            records = self._paginated_get(
                "/reverse-etl-models", "reverseEtlModels", table_options
            )
            return iter(records), {}

        def _read_transformations(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> tuple[Iterator[dict], dict]:
            """Read the transformations table."""
            records = self._paginated_get(
                "/transformations", "transformations", table_options
            )
            return iter(records), {}

        def _read_usage_api_calls_daily(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> tuple[Iterator[dict], dict]:
            """
            Read the usage_api_calls_daily table.

            This stream supports incremental sync using timestamp as the cursor.
            """
            # Determine the start date from offset or table_options
            cursor = None
            if start_offset and isinstance(start_offset, dict):
                cursor = start_offset.get("cursor")
            if not cursor:
                cursor = table_options.get("start_date")

            params = {}
            if cursor:
                params["period"] = cursor

            try:
                max_pages = int(table_options.get("max_pages", 100))
            except (TypeError, ValueError):
                max_pages = 100

            url = f"{self.base_url}/usage/api-calls/daily"
            records: list[dict[str, Any]] = []
            max_timestamp: str | None = None
            pages_fetched = 0
            next_cursor: str | None = None

            while pages_fetched < max_pages:
                request_params = params.copy()
                if next_cursor:
                    request_params["pagination.cursor"] = next_cursor

                response = self._session.get(url, params=request_params, timeout=60)
                if response.status_code != 200:
                    raise RuntimeError(
                        f"Segment API error for /usage/api-calls/daily: "
                        f"{response.status_code} {response.text}"
                    )

                response_data = response.json()
                data = response_data.get("data", {})
                items = data.get("dailyPerSourceAPICallsUsage", [])

                if isinstance(items, list):
                    for item in items:
                        records.append(item)
                        timestamp = item.get("timestamp")
                        if isinstance(timestamp, str):
                            if max_timestamp is None or timestamp > max_timestamp:
                                max_timestamp = timestamp

                pagination = response_data.get("pagination", {})
                next_cursor = pagination.get("next")
                if not next_cursor:
                    break

                pages_fetched += 1

            # Compute next offset
            if max_timestamp:
                next_offset = {"cursor": max_timestamp}
            elif start_offset:
                next_offset = start_offset
            else:
                next_offset = {}

            return iter(records), next_offset

        def _read_usage_mtu_daily(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> tuple[Iterator[dict], dict]:
            """
            Read the usage_mtu_daily table.

            This stream supports incremental sync using timestamp as the cursor.
            """
            # Determine the start date from offset or table_options
            cursor = None
            if start_offset and isinstance(start_offset, dict):
                cursor = start_offset.get("cursor")
            if not cursor:
                cursor = table_options.get("start_date")

            params = {}
            if cursor:
                params["period"] = cursor

            try:
                max_pages = int(table_options.get("max_pages", 100))
            except (TypeError, ValueError):
                max_pages = 100

            url = f"{self.base_url}/usage/mtu/daily"
            records: list[dict[str, Any]] = []
            max_timestamp: str | None = None
            pages_fetched = 0
            next_cursor: str | None = None

            while pages_fetched < max_pages:
                request_params = params.copy()
                if next_cursor:
                    request_params["pagination.cursor"] = next_cursor

                response = self._session.get(url, params=request_params, timeout=60)
                if response.status_code != 200:
                    raise RuntimeError(
                        f"Segment API error for /usage/mtu/daily: "
                        f"{response.status_code} {response.text}"
                    )

                response_data = response.json()
                data = response_data.get("data", {})
                items = data.get("dailyPerSourceMTUUsage", [])

                if isinstance(items, list):
                    for item in items:
                        records.append(item)
                        timestamp = item.get("timestamp")
                        if isinstance(timestamp, str):
                            if max_timestamp is None or timestamp > max_timestamp:
                                max_timestamp = timestamp

                pagination = response_data.get("pagination", {})
                next_cursor = pagination.get("next")
                if not next_cursor:
                    break

                pages_fetched += 1

            # Compute next offset
            if max_timestamp:
                next_offset = {"cursor": max_timestamp}
            elif start_offset:
                next_offset = start_offset
            else:
                next_offset = {}

            return iter(records), next_offset


    ########################################################
    # pipeline/lakeflow_python_source.py
    ########################################################

    METADATA_TABLE = "_lakeflow_metadata"
    TABLE_NAME = "tableName"
    TABLE_NAME_LIST = "tableNameList"
    TABLE_CONFIGS = "tableConfigs"
    IS_DELETE_FLOW = "isDeleteFlow"


    class LakeflowStreamReader(SimpleDataSourceStreamReader):
        """
        Implements a data source stream reader for Lakeflow Connect.
        Currently, only the simpleStreamReader is implemented, which uses a
        more generic protocol suitable for most data sources that support
        incremental loading.
        """

        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.lakeflow_connect = lakeflow_connect
            self.schema = schema

        def initialOffset(self):
            return {}

        def read(self, start: dict) -> (Iterator[tuple], dict):
            is_delete_flow = self.options.get(IS_DELETE_FLOW) == "true"
            # Strip delete flow options before passing to connector
            table_options = {
                k: v for k, v in self.options.items() if k != IS_DELETE_FLOW
            }

            if is_delete_flow:
                records, offset = self.lakeflow_connect.read_table_deletes(
                    self.options[TABLE_NAME], start, table_options
                )
            else:
                records, offset = self.lakeflow_connect.read_table(
                    self.options[TABLE_NAME], start, table_options
                )
            rows = map(lambda x: parse_value(x, self.schema), records)
            return rows, offset

        def readBetweenOffsets(self, start: dict, end: dict) -> Iterator[tuple]:
            # TODO: This does not ensure the records returned are identical across repeated calls.
            # For append-only tables, the data source must guarantee that reading from the same
            # start offset will always yield the same set of records.
            # For tables ingested as incremental CDC, it is only necessary that no new changes
            # are missed in the returned records.
            return self.read(start)[0]


    class LakeflowBatchReader(DataSourceReader):
        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.schema = schema
            self.lakeflow_connect = lakeflow_connect
            self.table_name = options[TABLE_NAME]

        def read(self, partition):
            all_records = []
            if self.table_name == METADATA_TABLE:
                all_records = self._read_table_metadata()
            else:
                all_records, _ = self.lakeflow_connect.read_table(
                    self.table_name, None, self.options
                )

            rows = map(lambda x: parse_value(x, self.schema), all_records)
            return iter(rows)

        def _read_table_metadata(self):
            table_name_list = self.options.get(TABLE_NAME_LIST, "")
            table_names = [o.strip() for o in table_name_list.split(",") if o.strip()]
            all_records = []
            table_configs = json.loads(self.options.get(TABLE_CONFIGS, "{}"))
            for table in table_names:
                metadata = self.lakeflow_connect.read_table_metadata(
                    table, table_configs.get(table, {})
                )
                all_records.append({TABLE_NAME: table, **metadata})
            return all_records


    class LakeflowSource(DataSource):
        def __init__(self, options):
            self.options = options
            self.lakeflow_connect = LakeflowConnect(options)

        @classmethod
        def name(cls):
            return "lakeflow_connect"

        def schema(self):
            table = self.options[TABLE_NAME]
            if table == METADATA_TABLE:
                return StructType(
                    [
                        StructField(TABLE_NAME, StringType(), False),
                        StructField("primary_keys", ArrayType(StringType()), True),
                        StructField("cursor_field", StringType(), True),
                        StructField("ingestion_type", StringType(), True),
                    ]
                )
            else:
                # Assuming the LakeflowConnect interface uses get_table_schema, not get_table_details
                return self.lakeflow_connect.get_table_schema(table, self.options)

        def reader(self, schema: StructType):
            return LakeflowBatchReader(self.options, schema, self.lakeflow_connect)

        def simpleStreamReader(self, schema: StructType):
            return LakeflowStreamReader(self.options, schema, self.lakeflow_connect)


    spark.dataSource.register(LakeflowSource)  # pylint: disable=undefined-variable
