# ==============================================================================
# Merged Lakeflow Source: open_meteo
# ==============================================================================
# This file is auto-generated by scripts/merge_python_source.py
# Do not edit manually. Make changes to the source files instead.
# ==============================================================================

from datetime import datetime, timedelta
from decimal import Decimal
from typing import (
    Any,
    Dict,
    Iterator,
    List,
    Optional,
)

from pyspark.sql import Row
from pyspark.sql.datasource import DataSource, DataSourceReader, SimpleDataSourceStreamReader
from pyspark.sql.types import *
import requests


def register_lakeflow_source(spark):
    """Register the Lakeflow Python source with Spark."""

    ########################################################
    # libs/utils.py
    ########################################################

    def parse_value(value: Any, field_type: DataType) -> Any:
        """
        Converts a JSON value into a PySpark-compatible data type based on the provided field type.
        """
        if value is None:
            return None
        # Handle complex types
        if isinstance(field_type, StructType):
            # Validate input for StructType
            if not isinstance(value, dict):
                raise ValueError(f"Expected a dictionary for StructType, got {type(value)}")
            # Spark Python -> Arrow conversion require missing StructType fields to be assigned None.
            if value == {}:
                raise ValueError(
                    f"field in StructType cannot be an empty dict. Please assign None as the default value instead."
                )
            # For StructType, recursively parse fields into a Row
            field_dict = {}
            for field in field_type.fields:
                # When a field does not exist in the input:
                # 1. set it to None when schema marks it as nullable
                # 2. Otherwise, raise an error.
                if field.name in value:
                    field_dict[field.name] = parse_value(
                        value.get(field.name), field.dataType
                    )
                elif field.nullable:
                    field_dict[field.name] = None
                else:
                    raise ValueError(
                        f"Field {field.name} is not nullable but not found in the input"
                    )

            return Row(**field_dict)
        elif isinstance(field_type, ArrayType):
            # For ArrayType, parse each element in the array
            if not isinstance(value, list):
                # Handle edge case: single value that should be an array
                if field_type.containsNull:
                    # Try to convert to a single-element array if nulls are allowed
                    return [parse_value(value, field_type.elementType)]
                else:
                    raise ValueError(f"Expected a list for ArrayType, got {type(value)}")
            return [parse_value(v, field_type.elementType) for v in value]
        elif isinstance(field_type, MapType):
            # Handle MapType - new support
            if not isinstance(value, dict):
                raise ValueError(f"Expected a dictionary for MapType, got {type(value)}")
            return {
                parse_value(k, field_type.keyType): parse_value(v, field_type.valueType)
                for k, v in value.items()
            }
        # Handle primitive types with more robust error handling and type conversion
        try:
            if isinstance(field_type, StringType):
                # Don't convert None to "None" string
                return str(value) if value is not None else None
            elif isinstance(field_type, (IntegerType, LongType)):
                # Convert numeric strings and floats to integers
                if isinstance(value, str) and value.strip():
                    # Handle numeric strings
                    if "." in value:
                        return int(float(value))
                    return int(value)
                elif isinstance(value, (int, float)):
                    return int(value)
                raise ValueError(f"Cannot convert {value} to integer")
            elif isinstance(field_type, FloatType) or isinstance(field_type, DoubleType):
                # New support for floating point types
                if isinstance(value, str) and value.strip():
                    return float(value)
                return float(value)
            elif isinstance(field_type, DecimalType):
                # New support for Decimal type

                if isinstance(value, str) and value.strip():
                    return Decimal(value)
                return Decimal(str(value))
            elif isinstance(field_type, BooleanType):
                # Enhanced boolean conversion
                if isinstance(value, str):
                    lowered = value.lower()
                    if lowered in ("true", "t", "yes", "y", "1"):
                        return True
                    elif lowered in ("false", "f", "no", "n", "0"):
                        return False
                return bool(value)
            elif isinstance(field_type, DateType):
                # New support for DateType
                if isinstance(value, str):
                    # Try multiple date formats
                    for fmt in ("%Y-%m-%d", "%m/%d/%Y", "%d-%m-%Y", "%Y/%m/%d"):
                        try:
                            return datetime.strptime(value, fmt).date()
                        except ValueError:
                            continue
                    # ISO format as fallback
                    return datetime.fromisoformat(value).date()
                elif isinstance(value, datetime):
                    return value.date()
                raise ValueError(f"Cannot convert {value} to date")
            elif isinstance(field_type, TimestampType):
                # Enhanced timestamp handling
                if isinstance(value, str):
                    # Handle multiple timestamp formats including Z and timezone offsets
                    if value.endswith("Z"):
                        value = value.replace("Z", "+00:00")
                    try:
                        return datetime.fromisoformat(value)
                    except ValueError:
                        # Try additional formats if ISO format fails
                        for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S"):
                            try:
                                return datetime.strptime(value, fmt)
                            except ValueError:
                                continue
                elif isinstance(value, (int, float)):
                    # Handle Unix timestamps
                    return datetime.fromtimestamp(value)
                elif isinstance(value, datetime):
                    return value
                raise ValueError(f"Cannot convert {value} to timestamp")
            else:
                # Check for custom UDT handling
                if hasattr(field_type, "fromJson"):
                    # Support for User Defined Types that implement fromJson
                    return field_type.fromJson(value)
                raise TypeError(f"Unsupported field type: {field_type}")
        except (ValueError, TypeError) as e:
            # Add context to the error
            raise ValueError(
                f"Error converting '{value}' ({type(value)}) to {field_type}: {str(e)}"
            )


    ########################################################
    # sources/open_meteo/open_meteo.py
    ########################################################

    class LakeflowConnect:
        """
        Open-Meteo connector for weather forecast and historical data.

        Open-Meteo is a free weather API that provides:
        - Weather forecasts up to 16 days ahead
        - Historical weather data (80+ years)
        - Geocoding for location search

        No authentication required for non-commercial use.
        """

        # Supported tables
        TABLES = ["forecast", "historical", "geocoding"]

        # API base URLs
        FORECAST_API_URL = "https://api.open-meteo.com/v1/forecast"
        ARCHIVE_API_URL = "https://archive-api.open-meteo.com/v1/archive"
        GEOCODING_API_URL = "https://geocoding-api.open-meteo.com/v1/search"

        # Default hourly variables to fetch if none specified
        DEFAULT_HOURLY_VARIABLES = [
            "temperature_2m",
            "relative_humidity_2m",
            "precipitation",
            "weather_code",
            "cloud_cover",
            "wind_speed_10m",
            "wind_direction_10m",
        ]

        def __init__(self, options: Dict[str, str]) -> None:
            """
            Initialize the Open-Meteo connector.

            Args:
                options: Connection options. Optional keys:
                    - base_url: Override the base API URL (for testing)
                    - timeout: Request timeout in seconds (default: 30)
            """
            self.options = options
            self.base_url = options.get("base_url", "https://api.open-meteo.com")

            try:
                self.timeout = int(options.get("timeout", "30"))
            except (TypeError, ValueError):
                self.timeout = 30

            # Create a session for connection pooling
            self._session = requests.Session()

        def list_tables(self) -> List[str]:
            """
            Return the list of available tables.

            Returns:
                List of table names: forecast, historical, geocoding
            """
            return self.TABLES.copy()

        def get_table_schema(
            self, table_name: str, table_options: Dict[str, str]
        ) -> StructType:
            """
            Return the schema for a table.

            Args:
                table_name: Name of the table
                table_options: Additional options (not used for schema)

            Returns:
                StructType schema for the table
            """
            if table_name not in self.TABLES:
                raise ValueError(f"Unsupported table: {table_name!r}. Supported tables: {self.TABLES}")

            if table_name == "forecast":
                return self._get_weather_schema()
            elif table_name == "historical":
                return self._get_weather_schema()
            elif table_name == "geocoding":
                return self._get_geocoding_schema()

            raise ValueError(f"Unsupported table: {table_name!r}")

        def _get_weather_schema(self) -> StructType:
            """
            Return the schema for weather data (forecast and historical).

            The schema includes location metadata and all common hourly variables.
            """
            return StructType([
                # Location and time identifiers
                StructField("latitude", DoubleType(), False),
                StructField("longitude", DoubleType(), False),
                StructField("time", StringType(), False),

                # Location metadata
                StructField("elevation", DoubleType(), True),
                StructField("timezone", StringType(), True),
                StructField("timezone_abbreviation", StringType(), True),

                # Temperature variables
                StructField("temperature_2m", DoubleType(), True),
                StructField("apparent_temperature", DoubleType(), True),
                StructField("dewpoint_2m", DoubleType(), True),

                # Humidity
                StructField("relative_humidity_2m", DoubleType(), True),

                # Precipitation
                StructField("precipitation", DoubleType(), True),
                StructField("rain", DoubleType(), True),
                StructField("showers", DoubleType(), True),
                StructField("snowfall", DoubleType(), True),
                StructField("snow_depth", DoubleType(), True),

                # Weather condition
                StructField("weather_code", LongType(), True),

                # Cloud cover
                StructField("cloud_cover", DoubleType(), True),
                StructField("cloud_cover_low", DoubleType(), True),
                StructField("cloud_cover_mid", DoubleType(), True),
                StructField("cloud_cover_high", DoubleType(), True),

                # Pressure
                StructField("pressure_msl", DoubleType(), True),
                StructField("surface_pressure", DoubleType(), True),

                # Wind
                StructField("wind_speed_10m", DoubleType(), True),
                StructField("wind_direction_10m", DoubleType(), True),
                StructField("wind_gusts_10m", DoubleType(), True),

                # Visibility and UV
                StructField("visibility", DoubleType(), True),
                StructField("uv_index", DoubleType(), True),
                StructField("uv_index_clear_sky", DoubleType(), True),

                # Day/night and sunshine
                StructField("is_day", LongType(), True),
                StructField("sunshine_duration", DoubleType(), True),

                # Radiation
                StructField("shortwave_radiation", DoubleType(), True),
                StructField("direct_radiation", DoubleType(), True),
                StructField("diffuse_radiation", DoubleType(), True),

                # Evapotranspiration
                StructField("et0_fao_evapotranspiration", DoubleType(), True),
            ])

        def _get_geocoding_schema(self) -> StructType:
            """Return the schema for geocoding results."""
            return StructType([
                StructField("id", LongType(), False),
                StructField("name", StringType(), False),
                StructField("latitude", DoubleType(), False),
                StructField("longitude", DoubleType(), False),
                StructField("elevation", DoubleType(), True),
                StructField("feature_code", StringType(), True),
                StructField("country_code", StringType(), True),
                StructField("country", StringType(), True),
                StructField("admin1", StringType(), True),
                StructField("admin2", StringType(), True),
                StructField("admin3", StringType(), True),
                StructField("admin4", StringType(), True),
                StructField("timezone", StringType(), True),
                StructField("population", LongType(), True),
                StructField("postcodes", ArrayType(StringType(), True), True),
            ])

        def read_table_metadata(
            self, table_name: str, table_options: Dict[str, str]
        ) -> Dict[str, Any]:
            """
            Return metadata for a table.

            Args:
                table_name: Name of the table
                table_options: Additional options (not used)

            Returns:
                Dict with primary_keys, cursor_field (if applicable), and ingestion_type
            """
            if table_name not in self.TABLES:
                raise ValueError(f"Unsupported table: {table_name!r}. Supported tables: {self.TABLES}")

            if table_name == "forecast":
                return {
                    "primary_keys": ["latitude", "longitude", "time"],
                    "ingestion_type": "snapshot",
                }
            elif table_name == "historical":
                return {
                    "primary_keys": ["latitude", "longitude", "time"],
                    "ingestion_type": "append",
                }
            elif table_name == "geocoding":
                return {
                    "primary_keys": ["id"],
                    "ingestion_type": "snapshot",
                }

            raise ValueError(f"Unsupported table: {table_name!r}")

        def read_table(
            self, table_name: str, start_offset: dict, table_options: Dict[str, str]
        ) -> tuple:
            """
            Read data from a table.

            Args:
                table_name: Name of the table to read
                start_offset: Starting offset for incremental reads (used for historical)
                table_options: Table-specific options:
                    - For forecast/historical:
                        - latitude (required): Latitude in decimal degrees
                        - longitude (required): Longitude in decimal degrees
                        - hourly: Comma-separated list of variables (optional)
                        - forecast_days: Number of forecast days (forecast only, default: 7)
                        - start_date: Start date YYYY-MM-DD (historical only)
                        - end_date: End date YYYY-MM-DD (historical only)
                        - timezone: Timezone name (optional)
                    - For geocoding:
                        - name (required): Location name to search
                        - count: Number of results (default: 10)
                        - language: Language code (default: en)

            Returns:
                Tuple of (Iterator[dict], dict) - records iterator and next offset
            """
            if table_name not in self.TABLES:
                raise ValueError(f"Unsupported table: {table_name!r}. Supported tables: {self.TABLES}")

            if table_name == "forecast":
                return self._read_forecast(start_offset, table_options)
            elif table_name == "historical":
                return self._read_historical(start_offset, table_options)
            elif table_name == "geocoding":
                return self._read_geocoding(start_offset, table_options)

            raise ValueError(f"Unsupported table: {table_name!r}")

        def _read_forecast(
            self, start_offset: dict, table_options: Dict[str, str]
        ) -> tuple:
            """Read weather forecast data."""
            latitude = table_options.get("latitude")
            longitude = table_options.get("longitude")

            if not latitude or not longitude:
                raise ValueError(
                    "table_options for 'forecast' must include 'latitude' and 'longitude'"
                )

            # Parse hourly variables
            hourly_str = table_options.get("hourly")
            if hourly_str:
                hourly_vars = [v.strip() for v in hourly_str.split(",") if v.strip()]
            else:
                hourly_vars = self.DEFAULT_HOURLY_VARIABLES.copy()

            # Build request parameters
            params = {
                "latitude": latitude,
                "longitude": longitude,
                "hourly": ",".join(hourly_vars),
                "timeformat": "iso8601",
            }

            # Optional parameters
            if table_options.get("forecast_days"):
                params["forecast_days"] = table_options["forecast_days"]
            if table_options.get("timezone"):
                params["timezone"] = table_options["timezone"]
            if table_options.get("temperature_unit"):
                params["temperature_unit"] = table_options["temperature_unit"]
            if table_options.get("wind_speed_unit"):
                params["wind_speed_unit"] = table_options["wind_speed_unit"]
            if table_options.get("precipitation_unit"):
                params["precipitation_unit"] = table_options["precipitation_unit"]

            # Make API request
            response = self._session.get(
                self.FORECAST_API_URL,
                params=params,
                timeout=self.timeout
            )

            if response.status_code != 200:
                raise RuntimeError(
                    f"Open-Meteo API error: {response.status_code} {response.text}"
                )

            data = response.json()
            records = self._pivot_weather_response(data)

            # Forecast is snapshot - no incremental offset
            return iter(records), {}

        def _read_historical(
            self, start_offset: dict, table_options: Dict[str, str]
        ) -> tuple:
            """Read historical weather data."""
            latitude = table_options.get("latitude")
            longitude = table_options.get("longitude")

            if not latitude or not longitude:
                raise ValueError(
                    "table_options for 'historical' must include 'latitude' and 'longitude'"
                )

            # Determine date range
            start_date = table_options.get("start_date")
            end_date = table_options.get("end_date")

            # Use offset for incremental reads if available
            if start_offset and isinstance(start_offset, dict):
                cursor_date = start_offset.get("cursor")
                if cursor_date:
                    start_date = cursor_date

            if not start_date or not end_date:
                raise ValueError(
                    "table_options for 'historical' must include 'start_date' and 'end_date' (YYYY-MM-DD)"
                )

            # Parse hourly variables
            hourly_str = table_options.get("hourly")
            if hourly_str:
                hourly_vars = [v.strip() for v in hourly_str.split(",") if v.strip()]
            else:
                hourly_vars = self.DEFAULT_HOURLY_VARIABLES.copy()

            # Build request parameters
            params = {
                "latitude": latitude,
                "longitude": longitude,
                "start_date": start_date,
                "end_date": end_date,
                "hourly": ",".join(hourly_vars),
                "timeformat": "iso8601",
            }

            # Optional parameters
            if table_options.get("timezone"):
                params["timezone"] = table_options["timezone"]
            if table_options.get("temperature_unit"):
                params["temperature_unit"] = table_options["temperature_unit"]
            if table_options.get("wind_speed_unit"):
                params["wind_speed_unit"] = table_options["wind_speed_unit"]
            if table_options.get("precipitation_unit"):
                params["precipitation_unit"] = table_options["precipitation_unit"]

            # Make API request
            response = self._session.get(
                self.ARCHIVE_API_URL,
                params=params,
                timeout=self.timeout
            )

            if response.status_code != 200:
                raise RuntimeError(
                    f"Open-Meteo Archive API error: {response.status_code} {response.text}"
                )

            data = response.json()
            records = self._pivot_weather_response(data)

            # Calculate next offset for incremental reads
            # Use end_date + 1 day as the next cursor
            next_cursor = end_date
            if records:
                try:
                    end_dt = datetime.strptime(end_date, "%Y-%m-%d")
                    next_dt = end_dt + timedelta(days=1)
                    next_cursor = next_dt.strftime("%Y-%m-%d")
                except ValueError:
                    pass

            next_offset = {"cursor": next_cursor}

            # If we got no records and had a start_offset, return same offset to signal end
            if not records and start_offset:
                next_offset = start_offset

            return iter(records), next_offset

        def _read_geocoding(
            self, start_offset: dict, table_options: Dict[str, str]
        ) -> tuple:
            """Read geocoding search results."""
            name = table_options.get("name")

            if not name:
                raise ValueError(
                    "table_options for 'geocoding' must include 'name' (location to search)"
                )

            # Build request parameters
            params = {
                "name": name,
                "format": "json",
            }

            if table_options.get("count"):
                params["count"] = table_options["count"]
            if table_options.get("language"):
                params["language"] = table_options["language"]

            # Make API request
            response = self._session.get(
                self.GEOCODING_API_URL,
                params=params,
                timeout=self.timeout
            )

            if response.status_code != 200:
                raise RuntimeError(
                    f"Open-Meteo Geocoding API error: {response.status_code} {response.text}"
                )

            data = response.json()
            results = data.get("results", [])

            # Transform results to match schema
            records = []
            for result in results:
                record = {
                    "id": result.get("id"),
                    "name": result.get("name"),
                    "latitude": result.get("latitude"),
                    "longitude": result.get("longitude"),
                    "elevation": result.get("elevation"),
                    "feature_code": result.get("feature_code"),
                    "country_code": result.get("country_code"),
                    "country": result.get("country"),
                    "admin1": result.get("admin1"),
                    "admin2": result.get("admin2"),
                    "admin3": result.get("admin3"),
                    "admin4": result.get("admin4"),
                    "timezone": result.get("timezone"),
                    "population": result.get("population"),
                    "postcodes": result.get("postcodes"),
                }
                records.append(record)

            # Geocoding is snapshot - no incremental offset
            return iter(records), {}

        def _pivot_weather_response(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:
            """
            Transform Open-Meteo's parallel array response into row-based records.

            Open-Meteo returns data like:
            {
                "hourly": {
                    "time": ["2024-01-01T00:00", "2024-01-01T01:00", ...],
                    "temperature_2m": [2.5, 2.3, ...],
                    "precipitation": [0.0, 0.1, ...]
                }
            }

            We transform this into:
            [
                {"time": "2024-01-01T00:00", "temperature_2m": 2.5, "precipitation": 0.0, ...},
                {"time": "2024-01-01T01:00", "temperature_2m": 2.3, "precipitation": 0.1, ...},
                ...
            ]
            """
            records = []

            # Extract location metadata
            latitude = data.get("latitude")
            longitude = data.get("longitude")
            elevation = data.get("elevation")
            timezone = data.get("timezone")
            timezone_abbr = data.get("timezone_abbreviation")

            hourly = data.get("hourly", {})
            if not hourly:
                return records

            times = hourly.get("time", [])
            if not times:
                return records

            # Get all variable names except 'time'
            variable_names = [k for k in hourly.keys() if k != "time"]

            # Pivot: create one record per timestamp
            for i, time_val in enumerate(times):
                record = {
                    "latitude": latitude,
                    "longitude": longitude,
                    "time": time_val,
                    "elevation": elevation,
                    "timezone": timezone,
                    "timezone_abbreviation": timezone_abbr,
                }

                # Add each variable's value at this index
                for var_name in variable_names:
                    var_values = hourly.get(var_name, [])
                    if i < len(var_values):
                        record[var_name] = var_values[i]
                    else:
                        record[var_name] = None

                records.append(record)

            return records


    ########################################################
    # pipeline/lakeflow_python_source.py
    ########################################################

    METADATA_TABLE = "_lakeflow_metadata"
    TABLE_NAME = "tableName"
    TABLE_NAME_LIST = "tableNameList"


    class LakeflowStreamReader(SimpleDataSourceStreamReader):
        """
        Implements a data source stream reader for Lakeflow Connect.
        Currently, only the simpleStreamReader is implemented, which uses a
        more generic protocol suitable for most data sources that support
        incremental loading.
        """

        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.lakeflow_connect = lakeflow_connect
            self.schema = schema

        def initialOffset(self):
            return {}

        def read(self, start: dict) -> (Iterator[tuple], dict):
            records, offset = self.lakeflow_connect.read_table(
                self.options["tableName"], start, self.options
            )
            rows = map(lambda x: parse_value(x, self.schema), records)
            return rows, offset

        def readBetweenOffsets(self, start: dict, end: dict) -> Iterator[tuple]:
            # TODO: This does not ensure the records returned are identical across repeated calls.
            # For append-only tables, the data source must guarantee that reading from the same
            # start offset will always yield the same set of records.
            # For tables ingested as incremental CDC, it is only necessary that no new changes
            # are missed in the returned records.
            return self.read(start)[0]


    class LakeflowBatchReader(DataSourceReader):
        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.schema = schema
            self.lakeflow_connect = lakeflow_connect
            self.table_name = options[TABLE_NAME]

        def read(self, partition):
            all_records = []
            if self.table_name == METADATA_TABLE:
                all_records = self._read_table_metadata()
            else:
                all_records, _ = self.lakeflow_connect.read_table(
                    self.table_name, None, self.options
                )

            rows = map(lambda x: parse_value(x, self.schema), all_records)
            return iter(rows)

        def _read_table_metadata(self):
            table_name_list = self.options.get(TABLE_NAME_LIST, "")
            table_names = [o.strip() for o in table_name_list.split(",") if o.strip()]
            all_records = []
            for table in table_names:
                metadata = self.lakeflow_connect.read_table_metadata(table, self.options)
                all_records.append({"tableName": table, **metadata})
            return all_records


    class LakeflowSource(DataSource):
        def __init__(self, options):
            self.options = options
            self.lakeflow_connect = LakeflowConnect(options)

        @classmethod
        def name(cls):
            return "lakeflow_connect"

        def schema(self):
            table = self.options["tableName"]
            if table == METADATA_TABLE:
                return StructType(
                    [
                        StructField("tableName", StringType(), False),
                        StructField("primary_keys", ArrayType(StringType()), True),
                        StructField("cursor_field", StringType(), True),
                        StructField("ingestion_type", StringType(), True),
                    ]
                )
            else:
                # Assuming the LakeflowConnect interface uses get_table_schema, not get_table_details
                return self.lakeflow_connect.get_table_schema(table, self.options)

        def reader(self, schema: StructType):
            return LakeflowBatchReader(self.options, schema, self.lakeflow_connect)

        def simpleStreamReader(self, schema: StructType):
            return LakeflowStreamReader(self.options, schema, self.lakeflow_connect)


    spark.dataSource.register(LakeflowSource)
