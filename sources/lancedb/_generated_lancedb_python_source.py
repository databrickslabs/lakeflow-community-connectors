# ==============================================================================
# Merged Lakeflow Source: lancedb
# ==============================================================================
# This file is auto-generated by tools/scripts/merge_python_source.py
# Do not edit manually. Make changes to the source files instead.
# ==============================================================================

from abc import ABC, abstractmethod
from datetime import datetime
from decimal import Decimal
from typing import (
    Any,
    Callable,
    Dict,
    Iterator,
    List,
    Optional,
)
import io
import json
import time

from pydantic import (
    BaseModel,
    ConfigDict,
    Field,
    field_validator,
)
from pyspark.sql import Row
from pyspark.sql.datasource import DataSource, DataSourceReader, SimpleDataSourceStreamReader
from urllib.parse import quote, urljoin
from pyspark.sql.types import *
import base64
import logging
import requests


def register_lakeflow_source(spark):
    """Register the Lakeflow Python source with Spark."""

    ########################################################
    # libs/utils.py
    ########################################################

    def _parse_struct(value: Any, field_type: StructType) -> Row:
        """Parse a dictionary into a PySpark Row based on StructType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for StructType, got {type(value)}")
        # Spark Python -> Arrow conversion require missing StructType fields to be assigned None.
        if value == {}:
            raise ValueError(
                "field in StructType cannot be an empty dict. "
                "Please assign None as the default value instead."
            )
        field_dict = {}
        for field in field_type.fields:
            if field.name in value:
                field_dict[field.name] = parse_value(value.get(field.name), field.dataType)
            elif field.nullable:
                field_dict[field.name] = None
            else:
                raise ValueError(f"Field {field.name} is not nullable but not found in the input")
        return Row(**field_dict)


    def _parse_array(value: Any, field_type: ArrayType) -> list:
        """Parse a list into a PySpark array based on ArrayType schema."""
        if not isinstance(value, list):
            if field_type.containsNull:
                return [parse_value(value, field_type.elementType)]
            raise ValueError(f"Expected a list for ArrayType, got {type(value)}")
        return [parse_value(v, field_type.elementType) for v in value]


    def _parse_map(value: Any, field_type: MapType) -> dict:
        """Parse a dictionary into a PySpark map based on MapType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for MapType, got {type(value)}")
        return {
            parse_value(k, field_type.keyType): parse_value(v, field_type.valueType)
            for k, v in value.items()
        }


    def _parse_string(value: Any) -> str:
        """Convert value to string."""
        return str(value)


    def _parse_integer(value: Any) -> int:
        """Convert value to integer."""
        if isinstance(value, str) and value.strip():
            return int(float(value)) if "." in value else int(value)
        if isinstance(value, (int, float)):
            return int(value)
        raise ValueError(f"Cannot convert {value} to integer")


    def _parse_float(value: Any) -> float:
        """Convert value to float."""
        return float(value)


    def _parse_decimal(value: Any) -> Decimal:
        """Convert value to Decimal."""
        return Decimal(value) if isinstance(value, str) and value.strip() else Decimal(str(value))


    def _parse_boolean(value: Any) -> bool:
        """Convert value to boolean."""
        if isinstance(value, str):
            lowered = value.lower()
            if lowered in ("true", "t", "yes", "y", "1"):
                return True
            if lowered in ("false", "f", "no", "n", "0"):
                return False
        return bool(value)


    def _parse_date(value: Any) -> datetime.date:
        """Convert value to date."""
        if isinstance(value, str):
            for fmt in ("%Y-%m-%d", "%m/%d/%Y", "%d-%m-%Y", "%Y/%m/%d"):
                try:
                    return datetime.strptime(value, fmt).date()
                except ValueError:
                    continue
            return datetime.fromisoformat(value).date()
        if isinstance(value, datetime):
            return value.date()
        raise ValueError(f"Cannot convert {value} to date")


    def _parse_timestamp(value: Any) -> datetime:
        """Convert value to timestamp."""
        if isinstance(value, str):
            ts_value = value.replace("Z", "+00:00") if value.endswith("Z") else value
            try:
                return datetime.fromisoformat(ts_value)
            except ValueError:
                for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S"):
                    try:
                        return datetime.strptime(ts_value, fmt)
                    except ValueError:
                        continue
        elif isinstance(value, (int, float)):
            return datetime.fromtimestamp(value)
        elif isinstance(value, datetime):
            return value
        raise ValueError(f"Cannot convert {value} to timestamp")


    def _decode_string_to_bytes(value: str) -> bytes:
        """Try to decode a string as base64, then hex, then UTF-8."""
        try:
            return base64.b64decode(value)
        except Exception:
            pass
        try:
            return bytes.fromhex(value)
        except Exception:
            pass
        return value.encode("utf-8")


    def _parse_binary(value: Any) -> bytes:
        """Convert value to bytes. Tries base64, then hex, then UTF-8 for strings."""
        if isinstance(value, bytes):
            return value
        if isinstance(value, bytearray):
            return bytes(value)
        if isinstance(value, str):
            return _decode_string_to_bytes(value)
        if isinstance(value, list):
            return bytes(value)
        return str(value).encode("utf-8")


    # Mapping of primitive types to their parser functions
    _PRIMITIVE_PARSERS = {
        StringType: _parse_string,
        IntegerType: _parse_integer,
        LongType: _parse_integer,
        FloatType: _parse_float,
        DoubleType: _parse_float,
        DecimalType: _parse_decimal,
        BooleanType: _parse_boolean,
        DateType: _parse_date,
        TimestampType: _parse_timestamp,
        BinaryType: _parse_binary,
    }


    def parse_value(value: Any, field_type: DataType) -> Any:
        """
        Converts a JSON value into a PySpark-compatible data type based on the provided field type.
        """
        if value is None:
            return None

        # Handle complex types
        if isinstance(field_type, StructType):
            return _parse_struct(value, field_type)
        if isinstance(field_type, ArrayType):
            return _parse_array(value, field_type)
        if isinstance(field_type, MapType):
            return _parse_map(value, field_type)

        # Handle primitive types via type-based lookup
        try:
            field_type_class = type(field_type)
            if field_type_class in _PRIMITIVE_PARSERS:
                return _PRIMITIVE_PARSERS[field_type_class](value)

            # Check for custom UDT handling
            if hasattr(field_type, "fromJson"):
                return field_type.fromJson(value)

            raise TypeError(f"Unsupported field type: {field_type}")
        except (ValueError, TypeError) as e:
            raise ValueError(f"Error converting '{value}' ({type(value)}) to {field_type}: {str(e)}")


    ########################################################
    # sources/lancedb/lancedb.py
    ########################################################

    logger = logging.getLogger(__name__)


    # ============================================================================
    # BASE CLASSES (Shared Infrastructure)
    # ============================================================================

    class BaseTableOptions(BaseModel):
        """
        Base configuration options for reading from any vector database table.

        This model provides common validation and sanitization that applies across
        all vector database implementations. Subclasses can extend with DB-specific options.

        Security Features:
        - SQL injection prevention through column name validation
        - Resource protection through batch size limits
        - Type safety through Pydantic validation

        Attributes:
            primary_keys: List of column names that form the primary key
            cursor_field: Column name used for incremental reads
            batch_size: Number of rows to fetch per API request (1-10000)
            filter_expression: Optional filter expression (implementation-specific)
            columns: Optional list of specific columns to retrieve
        """

        model_config = ConfigDict(extra="allow")  # Allow DB-specific extensions

        primary_keys: Optional[List[str]] = Field(
            default=None, description="Primary key columns")
        cursor_field: Optional[str] = Field(
            default=None, description="Cursor field for incremental reads")
        batch_size: int = Field(
            default=1000,
            ge=1,
            le=10000,
            description="Rows per batch")
        filter_expression: Optional[str] = Field(
            default=None, description="Filter expression")
        columns: Optional[List[str]] = Field(
            default=None, description="Specific columns to retrieve")

        @field_validator("primary_keys", "columns", mode="before")
        @classmethod
        def parse_and_validate_column_lists(cls, v):
            """
            Parse and validate column names (handles JSON strings from Databricks).

            Databricks sends list options as JSON strings, so we need to parse them.
            Also validates column names to prevent SQL injection.
            """
            if v is None:
                return v

            # If it's a string, try to parse it as JSON
            if isinstance(v, str):
                import json
                try:
                    v = json.loads(v)
                except json.JSONDecodeError:
                    # If it's not JSON, treat as single column name
                    v = [v]

            # Now validate each column name
            if not isinstance(v, list):
                raise ValueError(f"Expected list of column names, got {type(v)}")

            for col in v:
                if not isinstance(col, str):
                    raise ValueError(
                        f"Column name must be string, got {
                            type(col)}")
                if not col.replace("_", "").replace(".", "").isalnum():
                    raise ValueError(
                        f"Invalid column name: {col}. "
                        "Only alphanumeric, underscore, and dot allowed."
                    )
            return v

        @field_validator("cursor_field")
        @classmethod
        def validate_cursor_field(cls, v: Optional[str]) -> Optional[str]:
            """
            Validate cursor field name to prevent SQL injection.

            Args:
                v: Cursor field name to validate

            Returns:
                Validated cursor field name

            Raises:
                ValueError: If cursor field name contains invalid characters
            """
            if v is None:
                return v
            if not v.replace("_", "").replace(".", "").isalnum():
                raise ValueError(
                    f"Invalid cursor field: {v}. "
                    "Only alphanumeric, underscore, and dot allowed."
                )
            return v


    class BaseVectorDBConnector(ABC):
        """
        Abstract base class for vector database connectors.

        This class provides reusable infrastructure for building vector database connectors:
        - Secure HTTP communication with retry logic
        - Input validation and sanitization
        - Thread-safe session management
        - Error handling and logging
        - Resource cleanup

        Subclasses must implement:
        - _build_base_url(): Construct DB-specific base URL
        - _setup_authentication(): Configure DB-specific auth headers
        - _build_list_tables_request(): Prepare table listing request
        - _parse_list_tables_response(): Extract table names from response
        - _build_schema_request(): Prepare schema retrieval request
        - _parse_schema_response(): Convert DB schema to Spark StructType
        - _build_metadata_request(): Prepare metadata retrieval request
        - _parse_metadata_response(): Extract metadata from response
        - _build_query_request(): Prepare data query request
        - _parse_query_response(): Extract records from response

        Thread Safety:
        - Configuration is immutable after initialization
        - HTTP session is thread-safe by design
        - No shared mutable state between method calls

        Performance:
        - Connection pooling via requests.Session
        - Iterator-based data streaming
        - Configurable batch sizes and retry behavior
        """

        def __init__(
            self,
            options: Dict[str, str],
            max_retries: int = 5,
            initial_retry_delay: float = 1.0,
            request_timeout: int = 30
        ) -> None:
            """
            Initialize the vector database connector.

            Args:
                options: Dictionary containing connection parameters (DB-specific)
                max_retries: Maximum number of retry attempts for failed requests (default: 5)
                initial_retry_delay: Initial delay in seconds between retries,
                                   doubles with each attempt (default: 1.0)
                request_timeout: Request timeout in seconds (default: 30)

            Raises:
                ValueError: If required parameters are missing or invalid
            """
            # Validate required parameters (subclass-specific)
            self._validate_options(options)

            # Store configuration (immutable after initialization)
            self._options = options

            # Configure retry and timeout parameters (externalized for flexibility)
            self.max_retries = max_retries
            self.retry_delay = initial_retry_delay
            self.request_timeout = request_timeout

            # Build base URL (DB-specific implementation)
            self.base_url = self._build_base_url(options)

            # Initialize thread-safe HTTP session with connection pooling
            self.session = requests.Session()

            # Setup authentication (DB-specific implementation)
            self._setup_authentication(self.session, options)

            logger.info("Initialized %s connector", self.__class__.__name__)

        def __del__(self):
            """Clean up resources when connector is destroyed."""
            if hasattr(self, 'session'):
                self.session.close()
                logger.debug("Closed HTTP session")

        # ========================================================================
        # REUSABLE SECURITY METHODS (No changes needed for new DBs)
        # ========================================================================

        def _validate_options(self, options: Dict[str, str]) -> None:
            """
            Validate required connection options.

            Subclasses should override to add DB-specific validation.

            Args:
                options: Connection options dictionary

            Raises:
                ValueError: If required options are missing or invalid
            """
            # Subclass implements validation

        def _sanitize_identifier(self, identifier: str) -> str:
            """
            Sanitize identifiers to prevent injection attacks.

            Validates that identifiers (table names, project names, etc.) contain
            only safe characters to prevent URL injection and other security issues.

            Args:
                identifier: String identifier to sanitize

            Returns:
                Sanitized identifier

            Raises:
                ValueError: If identifier contains invalid characters
            """
            # Allow alphanumeric, hyphens, and underscores only
            if not identifier.replace("-", "").replace("_", "").isalnum():
                raise ValueError(
                    f"Invalid identifier: {identifier}. "
                    "Only alphanumeric, hyphen, and underscore allowed."
                )
            return identifier

        # ========================================================================
        # REUSABLE HTTP METHODS (No changes needed for new DBs)
        # ========================================================================

        def _make_request(
            self,
            method: str,
            endpoint: str,
            json_data: Optional[Dict[str, Any]] = None,
            params: Optional[Dict[str, Any]] = None
        ) -> requests.Response:
            """
            Make a secure HTTP request with retry logic and rate limiting.

            This method implements exponential backoff for transient failures and
            handles rate limiting gracefully. It sanitizes inputs and logs errors
            without exposing sensitive information.

            Args:
                method: HTTP method (GET, POST, etc.)
                endpoint: API endpoint path (e.g., '/v1/table/')
                json_data: Optional JSON payload for POST requests
                params: Optional query parameters

            Returns:
                Response object from the API

            Raises:
                requests.exceptions.HTTPError: For non-retryable HTTP errors
                requests.exceptions.RequestException: For connection errors after max retries
            """
            url = urljoin(self.base_url, endpoint)
            retry_count = 0

            while retry_count <= self.max_retries:
                try:
                    response = self.session.request(
                        method=method,
                        url=url,
                        json=json_data,
                        params=params,
                        timeout=self.request_timeout
                    )

                    # Handle rate limiting (HTTP 429)
                    if response.status_code == 429:
                        retry_after = int(response.headers.get(
                            "Retry-After", self.retry_delay * (2 ** retry_count)))
                        logger.warning(
                            "Rate limited. Retrying after %d seconds...",
                            retry_after)
                        time.sleep(retry_after)
                        retry_count += 1
                        continue

                    # Raise for other HTTP errors
                    if response.status_code >= 400:
                        error_msg = f"{
                            response.status_code} {
                            response.reason} for url: {
                            response.url}"
                        try:
                            error_detail = response.json()
                            logger.error("API Error Response: %s", error_detail)
                            error_msg += f" - Detail: {error_detail}"
                        except Exception:  # pylint: disable=broad-except
                            error_body = response.text[:500]  # First 500 chars
                            logger.error("API Error Response: %s", error_body)
                            error_msg += f" - Response: {error_body}"
                        raise requests.exceptions.HTTPError(
                            error_msg, response=response)

                    return response

                except requests.exceptions.RequestException as e:
                    retry_count += 1
                    if retry_count > self.max_retries:
                        logger.error(
                            "Request failed after %d retries: %s",
                            self.max_retries,
                            e)
                        raise

                    # Exponential backoff
                    delay = self.retry_delay * (2 ** (retry_count - 1))
                    logger.warning(
                        "Request failed, retrying in %s seconds... (attempt %s/%s)",
                        delay, retry_count, self.max_retries
                    )
                    time.sleep(delay)

            raise requests.exceptions.RequestException("Max retries exceeded")

        # ========================================================================
        # REUSABLE ITERATOR PATTERN (No changes needed for new DBs)
        # ========================================================================

        def _create_batch_iterator(
            self,
            fetch_batch_func: Callable,
            initial_offset: dict,
            **kwargs
        ) -> tuple[Iterator[dict], dict]:
            """
            Create a memory-efficient iterator for batch data retrieval.

            This method implements the iterator pattern for streaming large datasets
            with minimal memory footprint. It calls the provided fetch function
            repeatedly until all data is retrieved.

            Args:
                fetch_batch_func: Function that fetches a batch of records
                initial_offset: Starting offset for pagination
                **kwargs: Additional arguments passed to fetch_batch_func

            Returns:
                Tuple of (iterator of records, final offset)
            """
            current_offset = initial_offset
            end_offset = initial_offset

            def _iterator():
                nonlocal current_offset, end_offset

                while True:
                    # Fetch batch using provided function
                    records, new_offset = fetch_batch_func(
                        current_offset, **kwargs)

                    # Yield records
                    for record in records:
                        yield record

                    # Update offset
                    end_offset = new_offset

                    # Check if we're done (offset unchanged = no more data)
                    if new_offset == current_offset or not records:
                        break

                    current_offset = new_offset

            return _iterator(), end_offset

        # ========================================================================
        # ABSTRACT METHODS (Must be implemented by subclasses)
        # ========================================================================

        @abstractmethod
        def _build_base_url(self, options: Dict[str, str]) -> str:
            """
            Construct the base URL for API requests.

            Args:
                options: Connection options

            Returns:
                Base URL string (e.g., "https://api.vectordb.com")
            """

        @abstractmethod
        def _setup_authentication(
                self, session: requests.Session, options: Dict[str, str]) -> None:
            """
            Configure authentication headers for the session.

            Args:
                session: HTTP session to configure
                options: Connection options containing credentials
            """

        @abstractmethod
        def _build_list_tables_request(
                self) -> tuple[str, str, Optional[dict], Optional[dict]]:
            """
            Build request parameters for listing tables.

            Returns:
                Tuple of (method, endpoint, json_data, params)
            """

        @abstractmethod
        def _parse_list_tables_response(
                self, response: requests.Response) -> List[str]:
            """
            Parse table names from list tables response.

            Args:
                response: HTTP response

            Returns:
                List of table names
            """

        @abstractmethod
        def _build_schema_request(
            self, table_name: str, table_options: Dict[str, str]
        ) -> tuple[str, str, Optional[dict], Optional[dict]]:
            """
            Build request parameters for retrieving table schema.

            Args:
                table_name: Name of the table
                table_options: Table-specific options

            Returns:
                Tuple of (method, endpoint, json_data, params)
            """

        @abstractmethod
        def _parse_schema_response(
            self, response: requests.Response, table_name: str
        ) -> StructType:
            """
            Parse schema from response and convert to Spark StructType.

            Args:
                response: HTTP response
                table_name: Name of the table

            Returns:
                Spark StructType representing the table schema
            """

        @abstractmethod
        def _build_metadata_request(
            self, table_name: str, table_options: Dict[str, str]
        ) -> tuple[str, str, Optional[dict], Optional[dict]]:
            """
            Build request parameters for retrieving table metadata.

            Args:
                table_name: Name of the table
                table_options: Table-specific options

            Returns:
                Tuple of (method, endpoint, json_data, params)
            """

        @abstractmethod
        def _parse_metadata_response(
            self, response: requests.Response, table_name: str
        ) -> Dict[str, Any]:
            """
            Parse metadata from response.

            Args:
                response: HTTP response
                table_name: Name of the table

            Returns:
                Dictionary with keys: primary_keys, cursor_field, ingestion_type
            """

        @abstractmethod
        def _build_query_request(
            self,
            table_name: str,
            offset: int,
            batch_size: int,
            filter_expr: Optional[str],
            columns: Optional[List[str]],
            cursor_value: Optional[Any],
            **kwargs
        ) -> tuple[str, str, Optional[dict], Optional[dict]]:
            """
            Build request parameters for querying table data.

            Args:
                table_name: Name of the table
                offset: Pagination offset
                batch_size: Number of rows to fetch
                filter_expr: Optional filter expression
                columns: Optional list of columns to retrieve
                cursor_value: Optional cursor value for incremental reads
                **kwargs: Additional DB-specific parameters

            Returns:
                Tuple of (method, endpoint, json_data, params)
            """

        @abstractmethod
        def _parse_query_response(
            self, response: requests.Response
        ) -> List[dict]:
            """
            Parse records from query response.

            Args:
                response: HTTP response

            Returns:
                List of records as dictionaries
            """

        # ========================================================================
        # LAKEFLOW INTERFACE IMPLEMENTATION (Uses abstract methods above)
        # ========================================================================

        def list_tables(self) -> List[str]:
            """
            List all tables in the vector database.

            Returns:
                List of table names
            """
            method, endpoint, json_data, params = self._build_list_tables_request()
            response = self._make_request(method, endpoint, json_data, params)
            return self._parse_list_tables_response(response)

        def get_table_schema(
            self, table_name: str, table_options: Dict[str, str]
        ) -> StructType:
            """
            Fetch the schema of a table.

            Args:
                table_name: Name of the table
                table_options: Table-specific options (columns supported for filtering)

            Returns:
                Spark StructType representing the schema (filtered if columns specified)
            """
            table_name = self._sanitize_identifier(table_name)
            method, endpoint, json_data, params = self._build_schema_request(
                table_name, table_options
            )
            response = self._make_request(method, endpoint, json_data, params)
            schema = self._parse_schema_response(response, table_name)

            # Filter schema based on columns parameter if specified
            # This ensures schema matches the actual data returned (important for
            # Spark)
            try:
                # Parse table options to check if columns filtering is requested
                options = self._get_table_options(table_options)

                if hasattr(options, 'columns') and options.columns:
                    # Filter schema to only include requested columns
                    filtered_fields = []
                    requested_columns_set = set(options.columns)

                    for field in schema.fields:
                        if field.name in requested_columns_set:
                            filtered_fields.append(field)

                    # Log the filtering
                    logger.info(
                        "Filtered schema for table '%s': %d columns requested (from %d total)",
                        table_name, len(filtered_fields), len(schema.fields)
                    )

                    schema = StructType(filtered_fields)
            except Exception as e:  # pylint: disable=broad-exception-caught
                # If anything goes wrong with filtering, return full schema
                logger.debug(
                    "Could not filter schema (returning full schema): %s", e
                )

            return schema

        def _get_table_options(self, table_options: Dict[str, str]):
            """
            Parse table options into appropriate options class.
            Can be overridden by subclasses to use their specific options class.

            Args:
                table_options: Dictionary of table options

            Returns:
                Parsed options object (BaseTableOptions or subclass)
            """
            return BaseTableOptions(**table_options)

        def read_table_metadata(
            self, table_name: str, table_options: Dict[str, str]
        ) -> Dict[str, Any]:
            """
            Fetch the metadata of a table.

            Args:
                table_name: Name of the table
                table_options: Table-specific options

            Returns:
                Dictionary containing: primary_keys, cursor_field, ingestion_type
            """
            table_name = self._sanitize_identifier(table_name)
            method, endpoint, json_data, params = self._build_metadata_request(
                table_name, table_options
            )
            response = self._make_request(method, endpoint, json_data, params)
            return self._parse_metadata_response(response, table_name)

        def read_table(self,
                       table_name: str,
                       start_offset: dict,
                       table_options: Dict[str,
                                           str]) -> tuple[Iterator[dict],
                                                          dict]:
            """
            Read records from a table with pagination support.

            Args:
                table_name: Name of the table
                start_offset: Starting offset for pagination
                table_options: Table-specific options

            Returns:
                Tuple of (iterator of records, end offset)
            """
            # This method should be overridden by subclasses for DB-specific implementation
            # But provides a framework using the abstract methods
            raise NotImplementedError(
                "Subclass must implement read_table() using the provided abstract methods"
            )


    # ============================================================================
    # LANCEDB-SPECIFIC IMPLEMENTATION
    # ============================================================================


    class LanceDBTableOptions(BaseTableOptions):
        """
        LanceDB-specific table options extending base options.

        Supports LanceDB REST API query parameters:
        https://docs.lancedb.com/api-reference/rest/table/query-a-table

        Vector Search:
        - query_vector: Query vector for similarity search
        - use_full_scan: Enable full scans (generates dummy vector if needed)
        - vector_column: Name of vector column to search (default: auto-detect)
        - distance_type: Distance metric (e.g., "cosine", "l2", "dot")

        Search Optimization:
        - nprobes: Number of probes for IVF index (higher = more accurate, slower)
        - ef: Search effort parameter for HNSW index (higher = more accurate, slower)
        - refine_factor: Refine factor for search quality
        - fast_search: Use fast search mode
        - bypass_vector_index: Skip vector index (full scan)

        Filtering:
        - prefilter: Apply filter before vector search (default: true, recommended)
        - lower_bound: Lower bound for distance filtering
        - upper_bound: Upper bound for distance filtering

        Results:
        - with_row_id: Include _rowid column in results
        - offset: Skip first N results (for pagination)
        - version: Query specific table version
        """
        # Vector search parameters
        query_vector: Optional[List[float]] = Field(
            default=None,
            description="Query vector for similarity search"
        )
        use_full_scan: bool = Field(
            default=True,
            description="Enable full scan mode (generates dummy vector if needed)"
        )
        vector_column: Optional[str] = Field(
            default=None,
            description="Name of the vector column to search"
        )
        distance_type: Optional[str] = Field(
            default=None,
            description="Distance metric: cosine, l2, dot"
        )

        # Index optimization parameters
        nprobes: Optional[int] = Field(
            default=None,
            ge=0,
            description="Number of probes for IVF index (higher = more accurate)"
        )
        ef: Optional[int] = Field(
            default=None,
            ge=0,
            description="Search effort for HNSW index (higher = more accurate)"
        )
        refine_factor: Optional[int] = Field(
            default=None,
            ge=0,
            description="Refine factor for search quality"
        )
        fast_search: Optional[bool] = Field(
            default=None,
            description="Use fast search mode"
        )
        bypass_vector_index: Optional[bool] = Field(
            default=None,
            description="Bypass vector index (full scan)"
        )

        # Filtering parameters
        prefilter: Optional[bool] = Field(
            default=True,
            description="Apply filter before vector search (recommended for large datasets)"
        )
        lower_bound: Optional[float] = Field(
            default=None,
            description="Lower bound for distance filtering"
        )
        upper_bound: Optional[float] = Field(
            default=None,
            description="Upper bound for distance filtering"
        )

        # Result parameters
        with_row_id: Optional[bool] = Field(
            default=None,
            description="Include _rowid column in results"
        )
        offset: Optional[int] = Field(
            default=None,
            ge=0,
            description="Number of results to skip (for pagination)"
        )
        version: Optional[int] = Field(
            default=None,
            ge=0,
            description="Table version to query"
        )


    class LakeflowConnect(BaseVectorDBConnector):
        """
        LanceDB Cloud Connector for Lakeflow Community Connectors.

        This connector provides secure, high-performance access to LanceDB Cloud
        instances, with built-in support for:
        - Security validation (SQL injection prevention)
        - HTTP retry logic with exponential backoff
        - Rate limiting handling
        - Iterator patterns
        - Error handling
        - Logging
        - Thread safety

        LanceDB-specific implementation:
        - API endpoints and authentication
        - Apache Arrow IPC parsing
        - Vector dimension detection
        - Schema mapping (Arrow â†’ Spark)
        """

        def __init__(
            self,
            options: Dict[str, str],
            max_retries: int = 5,
            initial_retry_delay: float = 1.0,
            request_timeout: int = 30
        ) -> None:
            """
            Initialize LanceDB connector.

            Args:
                options: Connection parameters:
                    - api_key: LanceDB Cloud API key
                    - project_name: Project/database name
                    - region: Cloud region (e.g., 'us-east-1')
                max_retries: Maximum number of retry attempts (default: 5)
                initial_retry_delay: Initial retry delay in seconds (default: 1.0)
                request_timeout: Request timeout in seconds (default: 30)
            """
            # Store LanceDB-specific options before calling super().__init__()
            self._project_name = options.get("project_name", "")
            self._region = options.get("region", "")
            self._api_key = options.get("api_key", "")

            # Call parent initializer (handles validation, session setup, etc.)
            super().__init__(options, max_retries, initial_retry_delay, request_timeout)

            logger.info(
                "Initialized LanceDB connector for project %s",
                self._project_name)

        # ========================================================================
        # VALIDATION (Overrides base class)
        # ========================================================================

        def _validate_options(self, options: Dict[str, str]) -> None:
            """Validate required LanceDB connection parameters."""
            if not options.get("api_key"):
                raise ValueError("Missing required parameter: api_key")
            if not options.get("project_name"):
                raise ValueError("Missing required parameter: project_name")
            if not options.get("region"):
                raise ValueError("Missing required parameter: region")

        # ========================================================================
        # BASE URL & AUTHENTICATION (Implements abstract methods)
        # ========================================================================

        def _build_base_url(self, options: Dict[str, str]) -> str:
            """
            Construct LanceDB Cloud base URL.

            Format: https://{project_name}.{region}.api.lancedb.com
            """
            project = self._sanitize_identifier(options["project_name"])
            region = self._sanitize_identifier(options["region"])
            return f"https://{project}.{region}.api.lancedb.com"

        def _setup_authentication(
            self, session: requests.Session, options: Dict[str, str]
        ) -> None:
            """
            Configure LanceDB authentication headers.

            LanceDB uses custom 'x-api-key' header.
            """
            session.headers.update({
                "x-api-key": options["api_key"],
                "Content-Type": "application/json",
                "Accept": "application/json",
                "User-Agent": "Lakeflow-LanceDB-Connector/2.0-Base"
            })

        # ========================================================================
        # LIST TABLES (Implements abstract methods)
        # ========================================================================

        def _build_list_tables_request(
            self
        ) -> tuple[str, str, Optional[dict], Optional[dict]]:
            """Build request for listing tables."""
            return ("GET", "/v1/table/", None, {"limit": 100})

        def _parse_list_tables_response(
                self, response: requests.Response) -> List[str]:
            """
            Parse table names from LanceDB response.

            Handles pagination with page_token.
            """
            all_tables = []
            data = response.json()

            # Extract tables from first response
            tables = data.get("tables", [])
            all_tables.extend(
                [t.get("name", t) if isinstance(t, dict) else t for t in tables])

            # Handle pagination
            page_token = data.get("page_token")
            while page_token:
                response = self._make_request(
                    "GET",
                    "/v1/table/",
                    params={
                        "limit": 100,
                        "page_token": page_token})
                data = response.json()
                tables = data.get("tables", [])
                all_tables.extend(
                    [t.get("name", t) if isinstance(t, dict) else t for t in tables])
                page_token = data.get("page_token")

            logger.info(
                "Found %d tables in project %s",
                len(all_tables),
                self._project_name)
            return all_tables

        # ========================================================================
        # SCHEMA (Implements abstract methods)
        # ========================================================================

        def _build_schema_request(
            self, table_name: str, table_options: Dict[str, str]
        ) -> tuple[str, str, Optional[dict], Optional[dict]]:
            """Build request for retrieving table schema."""
            endpoint = f"/v1/table/{quote(table_name)}/describe/"
            return ("POST", endpoint, {}, None)

        def _parse_schema_response(
            self, response: requests.Response, table_name: str
        ) -> StructType:
            """
            Parse LanceDB schema and convert to Spark StructType.

            LanceDB returns Arrow schema format which we convert to Spark types.
            """
            data = response.json()
            schema = data.get("schema", {})
            fields = schema.get("fields", [])

            spark_fields = []
            for field in fields:
                field_name = field.get("name", "")
                field_type = field.get("type", {})
                nullable = field.get("nullable", True)

                spark_type = self._arrow_type_to_spark_type(field_type)
                spark_fields.append(StructField(field_name, spark_type, nullable))

            logger.info(
                "Retrieved schema for table '%s' with %d fields",
                table_name,
                len(spark_fields))
            return StructType(spark_fields)

        def _arrow_type_to_spark_type(self, arrow_type):
            """
            Convert Arrow type to Spark SQL type.

            Handles primitive types, lists, and nested structures.
            """
            # Handle string representation (e.g., "fixed_size_list<float32>[768]")
            if isinstance(arrow_type, str):
                arrow_type_lower = arrow_type.lower()

                if "fixed_size_list" in arrow_type_lower or "list" in arrow_type_lower:
                    # Vector/array type - use array of strings as generic
                    # representation
                    return ArrayType(StringType(), True)
                elif "string" in arrow_type_lower or "utf8" in arrow_type_lower:
                    return StringType()
                elif "int64" in arrow_type_lower or "long" in arrow_type_lower:
                    return LongType()
                elif "int32" in arrow_type_lower or "int" in arrow_type_lower:
                    return IntegerType()
                elif "float64" in arrow_type_lower or "double" in arrow_type_lower:
                    return DoubleType()
                elif "float32" in arrow_type_lower or "float" in arrow_type_lower:
                    return FloatType()
                elif "bool" in arrow_type_lower:
                    return BooleanType()
                elif "binary" in arrow_type_lower:
                    return BinaryType()
                elif "date" in arrow_type_lower:
                    return DateType()
                elif "timestamp" in arrow_type_lower:
                    return TimestampType()
                else:
                    return StringType()  # Default fallback

            # Handle dict representation
            if isinstance(arrow_type, dict):
                type_name = arrow_type.get("type", "")

                if type_name == "fixed_size_list" or type_name == "list":
                    return ArrayType(StringType(), True)
                elif type_name == "string" or type_name == "utf8":
                    return StringType()
                elif type_name == "int64" or type_name == "long":
                    return LongType()
                elif type_name == "int32" or type_name == "int":
                    return IntegerType()
                elif type_name == "float64" or type_name == "double":
                    return DoubleType()
                elif type_name == "float32" or type_name == "float":
                    return FloatType()
                elif type_name == "bool" or type_name == "boolean":
                    return BooleanType()
                elif type_name == "binary":
                    return BinaryType()
                elif type_name == "date":
                    return DateType()
                elif type_name == "timestamp":
                    return TimestampType()
                else:
                    return StringType()

            return StringType()  # Default fallback

        # ========================================================================
        # METADATA (Implements abstract methods)
        # ========================================================================

        def _build_metadata_request(
            self, table_name: str, table_options: Dict[str, str]  # pylint: disable=unused-argument
        ) -> tuple[str, str, Optional[dict], Optional[dict]]:
            """Build request for retrieving table metadata."""
            # LanceDB uses same endpoint as schema for metadata
            endpoint = f"/v1/table/{quote(table_name)}/describe/"
            return ("POST", endpoint, {}, None)

        def _parse_metadata_response(
            self, response: requests.Response, table_name: str
        ) -> Dict[str, Any]:
            """
            Parse metadata from LanceDB response.

            Returns standard metadata format for Lakeflow.
            """
            # Verify response is valid
            _ = response.json()

            metadata = {
                "primary_keys": [],  # LanceDB doesn't expose primary keys in API
                "ingestion_type": "snapshot",  # Default for vector DBs
            }

            logger.info("Retrieved metadata for table '%s'", table_name)
            return metadata

        # ========================================================================
        # QUERY DATA (Implements abstract methods)
        # ========================================================================

        def _build_query_request(
            self,
            table_name: str,
            offset: int,  # pylint: disable=unused-argument
            batch_size: int,
            filter_expr: Optional[str],
            columns: Optional[List[str]],
            cursor_value: Optional[Any],
            **kwargs
        ) -> tuple[str, str, Optional[dict], Optional[dict]]:
            """
            Build request for querying table data.

            LanceDB requires a vector parameter for queries.
            Supports full LanceDB REST API parameters:
            https://docs.lancedb.com/api-reference/rest/table/query-a-table
            """
            endpoint = f"/v1/table/{quote(table_name)}/query/"

            # Build query payload
            query_payload = {}

            # Get LanceDB-specific options from kwargs
            query_vector = kwargs.get("query_vector")
            use_full_scan = kwargs.get("use_full_scan", True)
            vector_column = kwargs.get("vector_column")
            distance_type = kwargs.get("distance_type")
            nprobes = kwargs.get("nprobes")
            ef = kwargs.get("ef")
            refine_factor = kwargs.get("refine_factor")
            fast_search = kwargs.get("fast_search")
            bypass_vector_index = kwargs.get("bypass_vector_index")
            prefilter = kwargs.get("prefilter")
            lower_bound = kwargs.get("lower_bound")
            upper_bound = kwargs.get("upper_bound")
            with_row_id = kwargs.get("with_row_id")
            result_offset = kwargs.get("result_offset")
            version = kwargs.get("version")

            # Handle vector requirement
            if query_vector:
                query_payload["vector"] = {"single_vector": query_vector}
            elif use_full_scan:
                # Auto-detect vector dimension and create dummy vector
                vector_dim = self._get_vector_dimension(table_name)
                if vector_dim:
                    query_payload["vector"] = {"single_vector": [0.0] * vector_dim}
                else:
                    query_payload["vector"] = {"single_vector": [0.0]}  # Fallback

            # LanceDB uses 'k' for k-nearest neighbors
            query_payload["k"] = batch_size

            # Add optional vector search parameters
            if vector_column:
                query_payload["vector_column"] = vector_column
            if distance_type:
                query_payload["distance_type"] = distance_type
            if nprobes is not None:
                query_payload["nprobes"] = nprobes
            if ef is not None:
                query_payload["ef"] = ef
            if refine_factor is not None:
                query_payload["refine_factor"] = refine_factor
            if fast_search is not None:
                query_payload["fast_search"] = fast_search
            if bypass_vector_index is not None:
                query_payload["bypass_vector_index"] = bypass_vector_index

            # Add filter if provided
            if filter_expr:
                query_payload["filter"] = filter_expr

            # Add cursor filter for incremental reads
            if cursor_value and kwargs.get("cursor_field"):
                cursor_filter = f"{kwargs['cursor_field']} > '{cursor_value}'"
                if "filter" in query_payload:
                    query_payload["filter"] += f" AND {cursor_filter}"
                else:
                    query_payload["filter"] = cursor_filter

            # Add filtering parameters
            if prefilter is not None:
                query_payload["prefilter"] = prefilter
            if lower_bound is not None:
                query_payload["lower_bound"] = lower_bound
            if upper_bound is not None:
                query_payload["upper_bound"] = upper_bound

            # Add result parameters
            if with_row_id is not None:
                query_payload["with_row_id"] = with_row_id
            if result_offset is not None:
                query_payload["offset"] = result_offset
            if version is not None:
                query_payload["version"] = version

            # âœ… Add column projection for performance
            # https://docs.lancedb.com/api-reference/rest/table/query-a-table
            # columns object: {column_names: [...]} or {column_aliases: {...}}
            if columns:
                query_payload["columns"] = {"column_names": columns}
                logger.debug(
                    "Requesting specific columns from LanceDB: %s",
                    columns)

            return ("POST", endpoint, query_payload, None)

        def _parse_query_response(self, response: requests.Response) -> List[dict]:
            """
            Parse records from LanceDB query response.

            LanceDB returns data in Apache Arrow IPC format.
            """
            # Check response format
            content_type = response.headers.get("Content-Type", "")

            if "arrow" in content_type.lower() or response.content.startswith(b"ARROW"):
                # Parse Apache Arrow format
                try:
                    import pyarrow as pa

                    # Try multiple parsing methods
                    try:
                        reader = pa.ipc.open_stream(io.BytesIO(response.content))
                        table = reader.read_all()
                    except Exception:  # pylint: disable=broad-except
                        try:
                            reader = pa.ipc.open_file(io.BytesIO(response.content))
                            table = reader.read_all()
                        except Exception:  # pylint: disable=broad-except
                            buf = pa.py_buffer(response.content)
                            reader = pa.ipc.open_stream(buf)
                            table = reader.read_all()

                    records = table.to_pylist()
                    return records

                except ImportError as exc:
                    raise ImportError(
                        "PyArrow is required: pip install pyarrow") from exc
                except Exception as e:
                    logger.error("Failed to parse Arrow format: %s", e)
                    raise
            else:
                # Try JSON format
                try:
                    data = response.json()
                    return data.get("data", [])
                except Exception:  # pylint: disable=broad-except
                    logger.error("Unknown response format: %s", content_type)
                    return []

        def _get_vector_dimension(self, table_name: str) -> Optional[int]:
            """
            Detect vector dimension from table schema.

            Parses Arrow schema to find fixed_size_list dimension.
            """
            try:
                endpoint = f"/v1/table/{quote(table_name)}/describe/"
                response = self._make_request("POST", endpoint)
                data = response.json()

                schema = data.get("schema", {})
                fields = schema.get("fields", [])

                for field in fields:
                    field_type = field.get("type", {})

                    if isinstance(field_type, dict):
                        type_name = field_type.get("type", "")
                        if type_name == "fixed_size_list":
                            dimension = field_type.get("length", 0)
                            if dimension > 0:
                                logger.info(
                                    "Detected vector dimension: %d", dimension)
                                return dimension

                return None
            except Exception as e:
                logger.warning("Could not detect vector dimension: %s", e)
                return None

        # ========================================================================
        # LAKEFLOW read_table (Implements using base class patterns)
        # ========================================================================

        def _get_table_options(self, table_options: Dict[str, str]):
            """
            Parse table options into LanceDBTableOptions.

            Overrides base class to use LanceDB-specific options.
            """
            return LanceDBTableOptions(**table_options)

        def read_table(self,
                       table_name: str,
                       start_offset: dict,
                       table_options: Dict[str,
                                           str]) -> tuple[Iterator[dict],
                                                          dict]:
            """
            Read data from LanceDB table.

            Uses base class iterator pattern with LanceDB-specific query logic.
            """
            table_name = self._sanitize_identifier(table_name)

            # Parse options
            options = LanceDBTableOptions(**table_options)

            # Prepare offsets
            current_offset = start_offset.get("offset", 0) if start_offset else 0
            cursor_value = start_offset.get(
                "cursor_value") if start_offset else None

            # Build filter
            filter_expr = options.filter_expression

            # Create iterator with closure variables
            max_cursor = cursor_value

            def _iterator():
                nonlocal max_cursor

                # Fetch batch
                method, endpoint, json_data, params = self._build_query_request(
                    table_name=table_name,
                    offset=current_offset,
                    batch_size=options.batch_size,
                    filter_expr=filter_expr,
                    columns=options.columns,
                    cursor_value=cursor_value,
                    cursor_field=options.cursor_field,
                    # Vector search parameters
                    query_vector=options.query_vector,
                    use_full_scan=options.use_full_scan,
                    vector_column=options.vector_column,
                    distance_type=options.distance_type,
                    # Index optimization parameters
                    nprobes=options.nprobes,
                    ef=options.ef,
                    refine_factor=options.refine_factor,
                    fast_search=options.fast_search,
                    bypass_vector_index=options.bypass_vector_index,
                    # Filtering parameters
                    prefilter=options.prefilter,
                    lower_bound=options.lower_bound,
                    upper_bound=options.upper_bound,
                    # Result parameters
                    with_row_id=options.with_row_id,
                    result_offset=options.offset,
                    version=options.version
                )

                response = self._make_request(method, endpoint, json_data, params)
                records = self._parse_query_response(response)

                # Fallback: Filter columns post-fetch if API didn't honor the request
                # This happens if LanceDB API doesn't support column projection yet
                # Performance note: This downloads ALL columns, then filters
                # in-memory
                if options.columns:
                    # Check if filtering is needed (API may have already filtered)
                    if records and not all(
                            k in options.columns for k in records[0].keys()):
                        logger.debug(
                            "Applying post-fetch column filtering (API didn't support projection)")
                        filtered_records = []
                        for record in records:
                            filtered_record = {
                                k: v for k, v in record.items() if k in options.columns}
                            filtered_records.append(filtered_record)
                        records = filtered_records
                    else:
                        logger.debug(
                            "API returned only requested columns (projection supported)")

                # Yield records
                for record in records:
                    # Track max cursor value
                    if options.cursor_field and options.cursor_field in record:
                        cursor_val = record[options.cursor_field]
                        if max_cursor is None or cursor_val > max_cursor:
                            max_cursor = cursor_val

                    yield record

            end_offset = {
                "offset": current_offset + options.batch_size,
                "cursor_value": max_cursor
            }

            return _iterator(), end_offset


    # For backwards compatibility, export with original name
    __all__ = ["LakeflowConnect", "LanceDBTableOptions"]


    ########################################################
    # pipeline/lakeflow_python_source.py
    ########################################################

    METADATA_TABLE = "_lakeflow_metadata"
    TABLE_NAME = "tableName"
    TABLE_NAME_LIST = "tableNameList"
    TABLE_CONFIGS = "tableConfigs"
    IS_DELETE_FLOW = "isDeleteFlow"


    class LakeflowStreamReader(SimpleDataSourceStreamReader):
        """
        Implements a data source stream reader for Lakeflow Connect.
        Currently, only the simpleStreamReader is implemented, which uses a
        more generic protocol suitable for most data sources that support
        incremental loading.
        """

        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.lakeflow_connect = lakeflow_connect
            self.schema = schema

        def initialOffset(self):
            return {}

        def read(self, start: dict) -> (Iterator[tuple], dict):
            is_delete_flow = self.options.get(IS_DELETE_FLOW) == "true"
            # Strip delete flow options before passing to connector
            table_options = {
                k: v for k, v in self.options.items() if k != IS_DELETE_FLOW
            }

            if is_delete_flow:
                records, offset = self.lakeflow_connect.read_table_deletes(
                    self.options[TABLE_NAME], start, table_options
                )
            else:
                records, offset = self.lakeflow_connect.read_table(
                    self.options[TABLE_NAME], start, table_options
                )
            rows = map(lambda x: parse_value(x, self.schema), records)
            return rows, offset

        def readBetweenOffsets(self, start: dict, end: dict) -> Iterator[tuple]:
            # TODO: This does not ensure the records returned are identical across repeated calls.
            # For append-only tables, the data source must guarantee that reading from the same
            # start offset will always yield the same set of records.
            # For tables ingested as incremental CDC, it is only necessary that no new changes
            # are missed in the returned records.
            return self.read(start)[0]


    class LakeflowBatchReader(DataSourceReader):
        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.schema = schema
            self.lakeflow_connect = lakeflow_connect
            self.table_name = options[TABLE_NAME]

        def read(self, partition):
            all_records = []
            if self.table_name == METADATA_TABLE:
                all_records = self._read_table_metadata()
            else:
                all_records, _ = self.lakeflow_connect.read_table(
                    self.table_name, None, self.options
                )

            rows = map(lambda x: parse_value(x, self.schema), all_records)
            return iter(rows)

        def _read_table_metadata(self):
            table_name_list = self.options.get(TABLE_NAME_LIST, "")
            table_names = [o.strip() for o in table_name_list.split(",") if o.strip()]
            all_records = []
            table_configs = json.loads(self.options.get(TABLE_CONFIGS, "{}"))
            for table in table_names:
                metadata = self.lakeflow_connect.read_table_metadata(
                    table, table_configs.get(table, {})
                )
                all_records.append({TABLE_NAME: table, **metadata})
            return all_records


    class LakeflowSource(DataSource):
        def __init__(self, options):
            self.options = options
            self.lakeflow_connect = LakeflowConnect(options)

        @classmethod
        def name(cls):
            return "lakeflow_connect"

        def schema(self):
            table = self.options[TABLE_NAME]
            if table == METADATA_TABLE:
                return StructType(
                    [
                        StructField(TABLE_NAME, StringType(), False),
                        StructField("primary_keys", ArrayType(StringType()), True),
                        StructField("cursor_field", StringType(), True),
                        StructField("ingestion_type", StringType(), True),
                    ]
                )
            else:
                # Assuming the LakeflowConnect interface uses get_table_schema, not get_table_details
                return self.lakeflow_connect.get_table_schema(table, self.options)

        def reader(self, schema: StructType):
            return LakeflowBatchReader(self.options, schema, self.lakeflow_connect)

        def simpleStreamReader(self, schema: StructType):
            return LakeflowStreamReader(self.options, schema, self.lakeflow_connect)


    spark.dataSource.register(LakeflowSource)  # pylint: disable=undefined-variable
