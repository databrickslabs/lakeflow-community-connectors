# ==============================================================================
# Merged Lakeflow Source: klaviyo
# ==============================================================================
# This file is auto-generated by tools/scripts/merge_python_source.py
# Do not edit manually. Make changes to the source files instead.
# ==============================================================================

from datetime import datetime
from decimal import Decimal
from typing import (
    Any,
    Dict,
    Iterator,
    List,
    Tuple,
)
import json
import time

from pyspark.sql import Row
from pyspark.sql.datasource import DataSource, DataSourceReader, SimpleDataSourceStreamReader
from urllib.parse import parse_qs, urlparse
from pyspark.sql.types import *
import base64
import requests


def register_lakeflow_source(spark):
    """Register the Lakeflow Python source with Spark."""

    ########################################################
    # libs/utils.py
    ########################################################

    def _parse_struct(value: Any, field_type: StructType) -> Row:
        """Parse a dictionary into a PySpark Row based on StructType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for StructType, got {type(value)}")
        # Spark Python -> Arrow conversion require missing StructType fields to be assigned None.
        if value == {}:
            raise ValueError(
                "field in StructType cannot be an empty dict. "
                "Please assign None as the default value instead."
            )
        field_dict = {}
        for field in field_type.fields:
            if field.name in value:
                field_dict[field.name] = parse_value(value.get(field.name), field.dataType)
            elif field.nullable:
                field_dict[field.name] = None
            else:
                raise ValueError(f"Field {field.name} is not nullable but not found in the input")
        return Row(**field_dict)


    def _parse_array(value: Any, field_type: ArrayType) -> list:
        """Parse a list into a PySpark array based on ArrayType schema."""
        if not isinstance(value, list):
            if field_type.containsNull:
                return [parse_value(value, field_type.elementType)]
            raise ValueError(f"Expected a list for ArrayType, got {type(value)}")
        return [parse_value(v, field_type.elementType) for v in value]


    def _parse_map(value: Any, field_type: MapType) -> dict:
        """Parse a dictionary into a PySpark map based on MapType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for MapType, got {type(value)}")
        return {
            parse_value(k, field_type.keyType): parse_value(v, field_type.valueType)
            for k, v in value.items()
        }


    def _parse_string(value: Any) -> str:
        """Convert value to string."""
        return str(value)


    def _parse_integer(value: Any) -> int:
        """Convert value to integer."""
        if isinstance(value, str) and value.strip():
            return int(float(value)) if "." in value else int(value)
        if isinstance(value, (int, float)):
            return int(value)
        raise ValueError(f"Cannot convert {value} to integer")


    def _parse_float(value: Any) -> float:
        """Convert value to float."""
        return float(value)


    def _parse_decimal(value: Any) -> Decimal:
        """Convert value to Decimal."""
        return Decimal(value) if isinstance(value, str) and value.strip() else Decimal(str(value))


    def _parse_boolean(value: Any) -> bool:
        """Convert value to boolean."""
        if isinstance(value, str):
            lowered = value.lower()
            if lowered in ("true", "t", "yes", "y", "1"):
                return True
            if lowered in ("false", "f", "no", "n", "0"):
                return False
        return bool(value)


    def _parse_date(value: Any) -> datetime.date:
        """Convert value to date."""
        if isinstance(value, str):
            for fmt in ("%Y-%m-%d", "%m/%d/%Y", "%d-%m-%Y", "%Y/%m/%d"):
                try:
                    return datetime.strptime(value, fmt).date()
                except ValueError:
                    continue
            return datetime.fromisoformat(value).date()
        if isinstance(value, datetime):
            return value.date()
        raise ValueError(f"Cannot convert {value} to date")


    def _parse_timestamp(value: Any) -> datetime:
        """Convert value to timestamp."""
        if isinstance(value, str):
            ts_value = value.replace("Z", "+00:00") if value.endswith("Z") else value
            try:
                return datetime.fromisoformat(ts_value)
            except ValueError:
                for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S"):
                    try:
                        return datetime.strptime(ts_value, fmt)
                    except ValueError:
                        continue
        elif isinstance(value, (int, float)):
            return datetime.fromtimestamp(value)
        elif isinstance(value, datetime):
            return value
        raise ValueError(f"Cannot convert {value} to timestamp")


    def _decode_string_to_bytes(value: str) -> bytes:
        """Try to decode a string as base64, then hex, then UTF-8."""
        try:
            return base64.b64decode(value)
        except Exception:
            pass
        try:
            return bytes.fromhex(value)
        except Exception:
            pass
        return value.encode("utf-8")


    def _parse_binary(value: Any) -> bytes:
        """Convert value to bytes. Tries base64, then hex, then UTF-8 for strings."""
        if isinstance(value, bytes):
            return value
        if isinstance(value, bytearray):
            return bytes(value)
        if isinstance(value, str):
            return _decode_string_to_bytes(value)
        if isinstance(value, list):
            return bytes(value)
        return str(value).encode("utf-8")


    # Mapping of primitive types to their parser functions
    _PRIMITIVE_PARSERS = {
        StringType: _parse_string,
        IntegerType: _parse_integer,
        LongType: _parse_integer,
        FloatType: _parse_float,
        DoubleType: _parse_float,
        DecimalType: _parse_decimal,
        BooleanType: _parse_boolean,
        DateType: _parse_date,
        TimestampType: _parse_timestamp,
        BinaryType: _parse_binary,
    }


    def parse_value(value: Any, field_type: DataType) -> Any:
        """
        Converts a JSON value into a PySpark-compatible data type based on the provided field type.
        """
        if value is None:
            return None

        # Handle complex types
        if isinstance(field_type, StructType):
            return _parse_struct(value, field_type)
        if isinstance(field_type, ArrayType):
            return _parse_array(value, field_type)
        if isinstance(field_type, MapType):
            return _parse_map(value, field_type)

        # Handle primitive types via type-based lookup
        try:
            field_type_class = type(field_type)
            if field_type_class in _PRIMITIVE_PARSERS:
                return _PRIMITIVE_PARSERS[field_type_class](value)

            # Check for custom UDT handling
            if hasattr(field_type, "fromJson"):
                return field_type.fromJson(value)

            raise TypeError(f"Unsupported field type: {field_type}")
        except (ValueError, TypeError) as e:
            raise ValueError(f"Error converting '{value}' ({type(value)}) to {field_type}: {str(e)}")


    ########################################################
    # sources/klaviyo/klaviyo.py
    ########################################################

    class LakeflowConnect:
        def __init__(self, options: dict) -> None:
            """
            Initialize the Klaviyo connector with API credentials.

            Args:
                options: Dictionary containing:
                    - api_key: Klaviyo private API key (pk_*)
            """
            self.api_key = options["api_key"]
            self.base_url = "https://a.klaviyo.com/api"
            self.api_version = "2024-10-15"

            # Request headers for GET requests (no Content-Type needed)
            self.headers = {
                "Authorization": f"Klaviyo-API-Key {self.api_key}",
                "revision": self.api_version,
                "Accept": "application/vnd.api+json",
            }

            # Centralized object metadata configuration
            # supports_page_size: whether endpoint supports page[size] parameter
            self._object_config = {
                "profiles": {
                    "primary_keys": ["id"],
                    "cursor_field": "updated",
                    "ingestion_type": "cdc",
                    "endpoint": "profiles",
                    "filter_field": "updated",
                    "supports_page_size": True,
                },
                "events": {
                    "primary_keys": ["id"],
                    "cursor_field": "datetime",
                    "ingestion_type": "append",
                    "endpoint": "events",
                    "filter_field": "datetime",
                    "supports_page_size": True,
                },
                "lists": {
                    "primary_keys": ["id"],
                    "cursor_field": None,
                    "ingestion_type": "snapshot",
                    "endpoint": "lists",
                    "filter_field": None,
                    "supports_page_size": False,
                },
                "campaigns": {
                    "primary_keys": ["id"],
                    "cursor_field": "updated_at",
                    "ingestion_type": "cdc",
                    "endpoint": "campaigns",
                    "filter_field": "updated_at",
                    "supports_page_size": False,
                    "required_filter": "equals(messages.channel,'email')",
                },
                "metrics": {
                    "primary_keys": ["id"],
                    "cursor_field": None,
                    "ingestion_type": "snapshot",
                    "endpoint": "metrics",
                    "filter_field": None,
                    "supports_page_size": False,
                },
                "flows": {
                    "primary_keys": ["id"],
                    "cursor_field": "updated",
                    "ingestion_type": "cdc",
                    "endpoint": "flows",
                    "filter_field": "updated",
                    "supports_page_size": True,
                },
                "segments": {
                    "primary_keys": ["id"],
                    "cursor_field": None,
                    "ingestion_type": "snapshot",
                    "endpoint": "segments",
                    "filter_field": None,
                    "supports_page_size": False,
                },
                "templates": {
                    "primary_keys": ["id"],
                    "cursor_field": "updated",
                    "ingestion_type": "cdc",
                    "endpoint": "templates",
                    "filter_field": "updated",
                    "supports_page_size": False,
                },
            }

            # Reusable nested schema for location
            self._location_schema = StructType(
                [
                    StructField("address1", StringType(), True),
                    StructField("address2", StringType(), True),
                    StructField("city", StringType(), True),
                    StructField("region", StringType(), True),
                    StructField("zip", StringType(), True),
                    StructField("country", StringType(), True),
                    StructField("latitude", DoubleType(), True),
                    StructField("longitude", DoubleType(), True),
                    StructField("timezone", StringType(), True),
                    StructField("ip", StringType(), True),
                ]
            )

            # Nested schema for email marketing subscription
            self._email_marketing_schema = StructType(
                [
                    StructField("can_receive_email_marketing", BooleanType(), True),
                    StructField("consent", StringType(), True),
                    StructField("consent_timestamp", StringType(), True),
                    StructField("last_updated", StringType(), True),
                    StructField("method", StringType(), True),
                    StructField("method_detail", StringType(), True),
                    StructField("custom_method_detail", StringType(), True),
                    StructField("double_optin", BooleanType(), True),
                    StructField("suppression", StringType(), True),
                    StructField("list_suppressions", ArrayType(StringType()), True),
                ]
            )

            # Nested schema for SMS marketing subscription
            self._sms_marketing_schema = StructType(
                [
                    StructField("can_receive_sms_marketing", BooleanType(), True),
                    StructField("consent", StringType(), True),
                    StructField("consent_timestamp", StringType(), True),
                    StructField("last_updated", StringType(), True),
                    StructField("method", StringType(), True),
                    StructField("method_detail", StringType(), True),
                ]
            )

            # Nested schema for subscriptions
            self._subscriptions_schema = StructType(
                [
                    StructField(
                        "email",
                        StructType(
                            [StructField("marketing", self._email_marketing_schema, True)]
                        ),
                        True,
                    ),
                    StructField(
                        "sms",
                        StructType(
                            [StructField("marketing", self._sms_marketing_schema, True)]
                        ),
                        True,
                    ),
                ]
            )

            # Nested schema for predictive analytics
            self._predictive_analytics_schema = StructType(
                [
                    StructField("historic_clv", DoubleType(), True),
                    StructField("predicted_clv", DoubleType(), True),
                    StructField("total_clv", DoubleType(), True),
                    StructField("historic_number_of_orders", LongType(), True),
                    StructField("predicted_number_of_orders", LongType(), True),
                    StructField("average_days_between_orders", DoubleType(), True),
                    StructField("average_order_value", DoubleType(), True),
                    StructField("churn_probability", DoubleType(), True),
                    StructField("expected_date_of_next_order", StringType(), True),
                ]
            )

            # Nested schema for integration (used in metrics)
            self._integration_schema = StructType(
                [
                    StructField("id", StringType(), True),
                    StructField("name", StringType(), True),
                    StructField("category", StringType(), True),
                ]
            )

            # Nested schema for audiences (used in campaigns)
            self._audiences_schema = StructType(
                [
                    StructField("included", ArrayType(StringType()), True),
                    StructField("excluded", ArrayType(StringType()), True),
                ]
            )

            # Nested schema for send_options (used in campaigns)
            self._send_options_schema = StructType(
                [
                    StructField("use_smart_sending", BooleanType(), True),
                    StructField("is_transactional", BooleanType(), True),
                ]
            )

            # Nested schema for tracking_options (used in campaigns)
            self._tracking_options_schema = StructType(
                [
                    StructField("is_add_utm", BooleanType(), True),
                    StructField("is_tracking_clicks", BooleanType(), True),
                    StructField("is_tracking_opens", BooleanType(), True),
                    StructField("utm_params", ArrayType(StringType()), True),
                ]
            )

            # Nested schema for send_strategy (used in campaigns)
            self._send_strategy_schema = StructType(
                [
                    StructField("method", StringType(), True),
                    StructField("options_static", StringType(), True),
                    StructField("options_throttled", StringType(), True),
                    StructField("options_sto", StringType(), True),
                ]
            )

            # Centralized schema configuration
            self._schema_config = {
                "profiles": StructType(
                    [
                        StructField("id", StringType(), False),
                        StructField("email", StringType(), True),
                        StructField("phone_number", StringType(), True),
                        StructField("external_id", StringType(), True),
                        StructField("first_name", StringType(), True),
                        StructField("last_name", StringType(), True),
                        StructField("organization", StringType(), True),
                        StructField("locale", StringType(), True),
                        StructField("title", StringType(), True),
                        StructField("image", StringType(), True),
                        StructField("created", StringType(), True),
                        StructField("updated", StringType(), True),
                        StructField("last_event_date", StringType(), True),
                        StructField("location", self._location_schema, True),
                        StructField(
                            "properties", MapType(StringType(), StringType()), True
                        ),
                        StructField("subscriptions", self._subscriptions_schema, True),
                        StructField(
                            "predictive_analytics", self._predictive_analytics_schema, True
                        ),
                    ]
                ),
                "events": StructType(
                    [
                        StructField("id", StringType(), False),
                        StructField("metric_id", StringType(), True),
                        StructField("profile_id", StringType(), True),
                        StructField("timestamp", StringType(), True),
                        StructField("datetime", StringType(), True),
                        StructField(
                            "event_properties", MapType(StringType(), StringType()), True
                        ),
                        StructField("uuid", StringType(), True),
                    ]
                ),
                "lists": StructType(
                    [
                        StructField("id", StringType(), False),
                        StructField("name", StringType(), True),
                        StructField("created", StringType(), True),
                        StructField("updated", StringType(), True),
                        StructField("opt_in_process", StringType(), True),
                    ]
                ),
                "campaigns": StructType(
                    [
                        StructField("id", StringType(), False),
                        StructField("name", StringType(), True),
                        StructField("status", StringType(), True),
                        StructField("archived", BooleanType(), True),
                        StructField("audiences", self._audiences_schema, True),
                        StructField("send_options", self._send_options_schema, True),
                        StructField("tracking_options", self._tracking_options_schema, True),
                        StructField("send_strategy", self._send_strategy_schema, True),
                        StructField("created_at", StringType(), True),
                        StructField("updated_at", StringType(), True),
                        StructField("scheduled_at", StringType(), True),
                        StructField("send_time", StringType(), True),
                    ]
                ),
                "metrics": StructType(
                    [
                        StructField("id", StringType(), False),
                        StructField("name", StringType(), True),
                        StructField("created", StringType(), True),
                        StructField("updated", StringType(), True),
                        StructField("integration", self._integration_schema, True),
                    ]
                ),
                "flows": StructType(
                    [
                        StructField("id", StringType(), False),
                        StructField("name", StringType(), True),
                        StructField("status", StringType(), True),
                        StructField("archived", BooleanType(), True),
                        StructField("created", StringType(), True),
                        StructField("updated", StringType(), True),
                        StructField("trigger_type", StringType(), True),
                    ]
                ),
                "segments": StructType(
                    [
                        StructField("id", StringType(), False),
                        StructField("name", StringType(), True),
                        StructField(
                            "definition", MapType(StringType(), StringType()), True
                        ),
                        StructField("created", StringType(), True),
                        StructField("updated", StringType(), True),
                        StructField("is_active", BooleanType(), True),
                        StructField("is_processing", BooleanType(), True),
                        StructField("is_starred", BooleanType(), True),
                    ]
                ),
                "templates": StructType(
                    [
                        StructField("id", StringType(), False),
                        StructField("name", StringType(), True),
                        StructField("editor_type", StringType(), True),
                        StructField("html", StringType(), True),
                        StructField("text", StringType(), True),
                        StructField("created", StringType(), True),
                        StructField("updated", StringType(), True),
                    ]
                ),
            }

        def list_tables(self) -> list[str]:
            """
            List available Klaviyo tables/objects.

            Returns:
                List of supported table names
            """
            return [
                "profiles",
                "events",
                "lists",
                "campaigns",
                "metrics",
                "flows",
                "segments",
                "templates",
            ]

        def get_table_schema(
            self, table_name: str, table_options: Dict[str, str]
        ) -> StructType:
            """
            Get the Spark schema for a Klaviyo table.

            Args:
                table_name: Name of the table
                table_options: Additional options (not used for Klaviyo)

            Returns:
                StructType representing the table schema
            """
            if table_name not in self._schema_config:
                raise ValueError(
                    f"Unsupported table: {table_name}. Supported tables are: {self.list_tables()}"
                )
            return self._schema_config[table_name]

        def read_table_metadata(
            self, table_name: str, table_options: Dict[str, str]
        ) -> dict:
            """
            Get metadata for a Klaviyo table.

            Args:
                table_name: Name of the table
                table_options: Additional options (not used for Klaviyo)

            Returns:
                Dictionary with primary_keys, cursor_field, and ingestion_type
            """
            if table_name not in self._object_config:
                raise ValueError(
                    f"Unsupported table: {table_name}. Supported tables are: {self.list_tables()}"
                )
            config = self._object_config[table_name]
            return {
                "primary_keys": config["primary_keys"],
                "cursor_field": config["cursor_field"],
                "ingestion_type": config["ingestion_type"],
            }

        def read_table(
            self, table_name: str, start_offset: dict, table_options: Dict[str, str]
        ) -> Tuple[Iterator[Dict], Dict]:
            """
            Read data from a Klaviyo table.

            Args:
                table_name: Name of the table to read
                start_offset: Dictionary containing cursor information for incremental reads
                    - For incremental: {<cursor_field>: <timestamp>}
                    - For full refresh: None or {}
                table_options: Additional options (not used for Klaviyo)

            Returns:
                Tuple of (records iterator, new_offset)
            """
            if table_name not in self._object_config:
                raise ValueError(
                    f"Unsupported table: {table_name}. Supported tables are: {self.list_tables()}"
                )

            config = self._object_config[table_name]
            cursor_field = config["cursor_field"]

            # Determine if this is an incremental read
            is_incremental = (
                start_offset is not None
                and cursor_field is not None
                and start_offset.get(cursor_field) is not None
            )

            if is_incremental:
                return self._read_data_incremental(table_name, start_offset)
            else:
                return self._read_data_full(table_name)

        def _read_data_full(self, table_name: str) -> Tuple[List[Dict], Dict]:
            """
            Read all data from a Klaviyo table (full refresh).

            Args:
                table_name: Name of the table

            Returns:
                Tuple of (all_records, offset)
            """
            config = self._object_config[table_name]
            endpoint = config["endpoint"]
            cursor_field = config["cursor_field"]

            all_records = []
            next_cursor = None
            latest_cursor_value = None

            while True:
                # Build request URL
                url = f"{self.base_url}/{endpoint}/"

                # Build params - only include page[size] if endpoint supports it
                params = {}
                if config.get("supports_page_size", False):
                    params["page[size]"] = 20

                # Add required filter if endpoint needs it (e.g., campaigns requires channel filter)
                if config.get("required_filter"):
                    params["filter"] = config["required_filter"]

                # Add sort for consistent ordering if cursor field exists
                if cursor_field:
                    params["sort"] = cursor_field

                if next_cursor:
                    params["page[cursor]"] = next_cursor

                # Make API request
                response = requests.get(url, headers=self.headers, params=params if params else None)

                if response.status_code == 429:
                    # Rate limited - wait and retry
                    retry_after = int(response.headers.get("Retry-After", 60))
                    time.sleep(retry_after)
                    continue

                if response.status_code != 200:
                    raise Exception(
                        f"Klaviyo API error for {table_name}: {response.status_code} {response.text}"
                    )

                data = response.json()
                records_data = data.get("data", [])

                if not records_data:
                    break

                # Process records - extract attributes and add id
                for record in records_data:
                    processed = self._process_record(record)
                    all_records.append(processed)

                    # Track the latest cursor value for checkpointing
                    if cursor_field and processed.get(cursor_field):
                        if (
                            latest_cursor_value is None
                            or processed[cursor_field] > latest_cursor_value
                        ):
                            latest_cursor_value = processed[cursor_field]

                # Check for next page
                links = data.get("links", {})
                next_url = links.get("next")

                if not next_url:
                    break

                # Extract cursor from next URL
                next_cursor = self._extract_cursor_from_url(next_url)

                # Rate limiting - be nice to the API
                time.sleep(0.15)

            # Return records and offset for next incremental sync
            offset = {}
            if cursor_field and latest_cursor_value:
                offset[cursor_field] = latest_cursor_value

            return all_records, offset

        def _read_data_incremental(
            self, table_name: str, start_offset: dict
        ) -> Tuple[List[Dict], Dict]:
            """
            Read incremental data from a Klaviyo table using cursor.

            Args:
                table_name: Name of the table
                start_offset: Dictionary with cursor field value

            Returns:
                Tuple of (new_records, new_offset)
            """
            config = self._object_config[table_name]
            endpoint = config["endpoint"]
            cursor_field = config["cursor_field"]
            filter_field = config["filter_field"]

            # Get the starting point from offset
            cursor_start = start_offset.get(cursor_field)

            all_records = []
            next_cursor = None
            latest_cursor_value = cursor_start

            while True:
                # Build request URL
                url = f"{self.base_url}/{endpoint}/"

                # Build params - only include page[size] if endpoint supports it
                params = {}
                if config.get("supports_page_size", False):
                    params["page[size]"] = 20

                # Build filter - combine required filter with incremental filter
                filters = []
                if config.get("required_filter"):
                    filters.append(config["required_filter"])
                if filter_field and cursor_start:
                    filters.append(f"greater-than({filter_field},{cursor_start})")
                if filters:
                    # Combine filters with AND
                    if len(filters) == 1:
                        params["filter"] = filters[0]
                    else:
                        params["filter"] = f"and({','.join(filters)})"

                # Add sort for consistent ordering
                if cursor_field:
                    params["sort"] = cursor_field

                if next_cursor:
                    params["page[cursor]"] = next_cursor

                # Make API request
                response = requests.get(url, headers=self.headers, params=params if params else None)

                if response.status_code == 429:
                    # Rate limited - wait and retry
                    retry_after = int(response.headers.get("Retry-After", 60))
                    time.sleep(retry_after)
                    continue

                if response.status_code != 200:
                    raise Exception(
                        f"Klaviyo API error for {table_name}: {response.status_code} {response.text}"
                    )

                data = response.json()
                records_data = data.get("data", [])

                if not records_data:
                    break

                # Process records
                for record in records_data:
                    processed = self._process_record(record)
                    all_records.append(processed)

                    # Track the latest cursor value
                    if cursor_field and processed.get(cursor_field):
                        if processed[cursor_field] > latest_cursor_value:
                            latest_cursor_value = processed[cursor_field]

                # Check for next page
                links = data.get("links", {})
                next_url = links.get("next")

                if not next_url:
                    break

                # Extract cursor from next URL
                next_cursor = self._extract_cursor_from_url(next_url)

                # Rate limiting
                time.sleep(0.15)

            # Return new offset for next sync
            offset = {cursor_field: latest_cursor_value}
            return all_records, offset

        def _process_record(self, record: dict) -> dict:
            """
            Process a JSON:API record by extracting attributes and adding id.

            Args:
                record: Raw JSON:API record with type, id, attributes

            Returns:
                Flattened record with id at top level
            """
            result = {"id": record.get("id")}

            # Extract attributes
            attributes = record.get("attributes", {})
            result.update(attributes)

            # Handle relationships if present (extract IDs)
            relationships = record.get("relationships", {})
            for rel_name, rel_data in relationships.items():
                rel_data_inner = rel_data.get("data")
                if rel_data_inner:
                    if isinstance(rel_data_inner, dict):
                        result[f"{rel_name}_id"] = rel_data_inner.get("id")
                    elif isinstance(rel_data_inner, list):
                        result[f"{rel_name}_ids"] = [
                            item.get("id") for item in rel_data_inner
                        ]

            return result

        def _extract_cursor_from_url(self, url: str) -> str:
            """
            Extract the page cursor from a pagination URL.

            Args:
                url: Full URL with page[cursor] parameter

            Returns:
                Cursor value
            """
            parsed = urlparse(url)
            query_params = parse_qs(parsed.query)
            cursor = query_params.get("page[cursor]", [None])[0]
            return cursor

        def test_connection(self) -> dict:
            """
            Test the connection to Klaviyo API.

            Returns:
                Dictionary with status and message
            """
            try:
                url = f"{self.base_url}/profiles/?page[size]=1"
                response = requests.get(url, headers=self.headers)

                if response.status_code == 200:
                    return {"status": "success", "message": "Connection successful"}
                else:
                    return {
                        "status": "error",
                        "message": f"API error: {response.status_code} {response.text}",
                    }
            except Exception as e:
                return {"status": "error", "message": f"Connection failed: {str(e)}"}


    ########################################################
    # pipeline/lakeflow_python_source.py
    ########################################################

    METADATA_TABLE = "_lakeflow_metadata"
    TABLE_NAME = "tableName"
    TABLE_NAME_LIST = "tableNameList"
    TABLE_CONFIGS = "tableConfigs"
    IS_DELETE_FLOW = "isDeleteFlow"


    class LakeflowStreamReader(SimpleDataSourceStreamReader):
        """
        Implements a data source stream reader for Lakeflow Connect.
        Currently, only the simpleStreamReader is implemented, which uses a
        more generic protocol suitable for most data sources that support
        incremental loading.
        """

        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.lakeflow_connect = lakeflow_connect
            self.schema = schema

        def initialOffset(self):
            return {}

        def read(self, start: dict) -> (Iterator[tuple], dict):
            is_delete_flow = self.options.get(IS_DELETE_FLOW) == "true"
            # Strip delete flow options before passing to connector
            table_options = {
                k: v for k, v in self.options.items() if k != IS_DELETE_FLOW
            }

            if is_delete_flow:
                records, offset = self.lakeflow_connect.read_table_deletes(
                    self.options[TABLE_NAME], start, table_options
                )
            else:
                records, offset = self.lakeflow_connect.read_table(
                    self.options[TABLE_NAME], start, table_options
                )
            rows = map(lambda x: parse_value(x, self.schema), records)
            return rows, offset

        def readBetweenOffsets(self, start: dict, end: dict) -> Iterator[tuple]:
            # TODO: This does not ensure the records returned are identical across repeated calls.
            # For append-only tables, the data source must guarantee that reading from the same
            # start offset will always yield the same set of records.
            # For tables ingested as incremental CDC, it is only necessary that no new changes
            # are missed in the returned records.
            return self.read(start)[0]


    class LakeflowBatchReader(DataSourceReader):
        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.schema = schema
            self.lakeflow_connect = lakeflow_connect
            self.table_name = options[TABLE_NAME]

        def read(self, partition):
            all_records = []
            if self.table_name == METADATA_TABLE:
                all_records = self._read_table_metadata()
            else:
                all_records, _ = self.lakeflow_connect.read_table(
                    self.table_name, None, self.options
                )

            rows = map(lambda x: parse_value(x, self.schema), all_records)
            return iter(rows)

        def _read_table_metadata(self):
            table_name_list = self.options.get(TABLE_NAME_LIST, "")
            table_names = [o.strip() for o in table_name_list.split(",") if o.strip()]
            all_records = []
            table_configs = json.loads(self.options.get(TABLE_CONFIGS, "{}"))
            for table in table_names:
                metadata = self.lakeflow_connect.read_table_metadata(
                    table, table_configs.get(table, {})
                )
                all_records.append({TABLE_NAME: table, **metadata})
            return all_records


    class LakeflowSource(DataSource):
        def __init__(self, options):
            self.options = options
            self.lakeflow_connect = LakeflowConnect(options)

        @classmethod
        def name(cls):
            return "lakeflow_connect"

        def schema(self):
            table = self.options[TABLE_NAME]
            if table == METADATA_TABLE:
                return StructType(
                    [
                        StructField(TABLE_NAME, StringType(), False),
                        StructField("primary_keys", ArrayType(StringType()), True),
                        StructField("cursor_field", StringType(), True),
                        StructField("ingestion_type", StringType(), True),
                    ]
                )
            else:
                # Assuming the LakeflowConnect interface uses get_table_schema, not get_table_details
                return self.lakeflow_connect.get_table_schema(table, self.options)

        def reader(self, schema: StructType):
            return LakeflowBatchReader(self.options, schema, self.lakeflow_connect)

        def simpleStreamReader(self, schema: StructType):
            return LakeflowStreamReader(self.options, schema, self.lakeflow_connect)


    spark.dataSource.register(LakeflowSource)  # pylint: disable=undefined-variable
