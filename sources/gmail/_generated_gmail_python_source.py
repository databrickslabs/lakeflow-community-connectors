# ==============================================================================
# Merged Lakeflow Source: gmail
# ==============================================================================
# This file is auto-generated by tools/scripts/merge_python_source.py
# Do not edit manually. Make changes to the source files instead.
# ==============================================================================

from datetime import datetime
from decimal import Decimal
from typing import (
    Any,
    Dict,
    Generator,
    Iterator,
    List,
)
import json
import time

from concurrent.futures import ThreadPoolExecutor, as_completed
from pyspark.sql import Row
from pyspark.sql.datasource import DataSource, DataSourceReader, SimpleDataSourceStreamReader
from pyspark.sql.types import *
import base64
import requests


def register_lakeflow_source(spark):
    """Register the Lakeflow Python source with Spark."""

    ########################################################
    # libs/utils.py
    ########################################################

    def _parse_struct(value: Any, field_type: StructType) -> Row:
        """Parse a dictionary into a PySpark Row based on StructType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for StructType, got {type(value)}")
        # Spark Python -> Arrow conversion require missing StructType fields to be assigned None.
        if value == {}:
            raise ValueError(
                "field in StructType cannot be an empty dict. "
                "Please assign None as the default value instead."
            )
        field_dict = {}
        for field in field_type.fields:
            if field.name in value:
                field_dict[field.name] = parse_value(value.get(field.name), field.dataType)
            elif field.nullable:
                field_dict[field.name] = None
            else:
                raise ValueError(f"Field {field.name} is not nullable but not found in the input")
        return Row(**field_dict)


    def _parse_array(value: Any, field_type: ArrayType) -> list:
        """Parse a list into a PySpark array based on ArrayType schema."""
        if not isinstance(value, list):
            if field_type.containsNull:
                return [parse_value(value, field_type.elementType)]
            raise ValueError(f"Expected a list for ArrayType, got {type(value)}")
        return [parse_value(v, field_type.elementType) for v in value]


    def _parse_map(value: Any, field_type: MapType) -> dict:
        """Parse a dictionary into a PySpark map based on MapType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for MapType, got {type(value)}")
        return {
            parse_value(k, field_type.keyType): parse_value(v, field_type.valueType)
            for k, v in value.items()
        }


    def _parse_string(value: Any) -> str:
        """Convert value to string."""
        return str(value)


    def _parse_integer(value: Any) -> int:
        """Convert value to integer."""
        if isinstance(value, str) and value.strip():
            return int(float(value)) if "." in value else int(value)
        if isinstance(value, (int, float)):
            return int(value)
        raise ValueError(f"Cannot convert {value} to integer")


    def _parse_float(value: Any) -> float:
        """Convert value to float."""
        return float(value)


    def _parse_decimal(value: Any) -> Decimal:
        """Convert value to Decimal."""
        return Decimal(value) if isinstance(value, str) and value.strip() else Decimal(str(value))


    def _parse_boolean(value: Any) -> bool:
        """Convert value to boolean."""
        if isinstance(value, str):
            lowered = value.lower()
            if lowered in ("true", "t", "yes", "y", "1"):
                return True
            if lowered in ("false", "f", "no", "n", "0"):
                return False
        return bool(value)


    def _parse_date(value: Any) -> datetime.date:
        """Convert value to date."""
        if isinstance(value, str):
            for fmt in ("%Y-%m-%d", "%m/%d/%Y", "%d-%m-%Y", "%Y/%m/%d"):
                try:
                    return datetime.strptime(value, fmt).date()
                except ValueError:
                    continue
            return datetime.fromisoformat(value).date()
        if isinstance(value, datetime):
            return value.date()
        raise ValueError(f"Cannot convert {value} to date")


    def _parse_timestamp(value: Any) -> datetime:
        """Convert value to timestamp."""
        if isinstance(value, str):
            ts_value = value.replace("Z", "+00:00") if value.endswith("Z") else value
            try:
                return datetime.fromisoformat(ts_value)
            except ValueError:
                for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S"):
                    try:
                        return datetime.strptime(ts_value, fmt)
                    except ValueError:
                        continue
        elif isinstance(value, (int, float)):
            return datetime.fromtimestamp(value)
        elif isinstance(value, datetime):
            return value
        raise ValueError(f"Cannot convert {value} to timestamp")


    def _decode_string_to_bytes(value: str) -> bytes:
        """Try to decode a string as base64, then hex, then UTF-8."""
        try:
            return base64.b64decode(value)
        except Exception:
            pass
        try:
            return bytes.fromhex(value)
        except Exception:
            pass
        return value.encode("utf-8")


    def _parse_binary(value: Any) -> bytes:
        """Convert value to bytes. Tries base64, then hex, then UTF-8 for strings."""
        if isinstance(value, bytes):
            return value
        if isinstance(value, bytearray):
            return bytes(value)
        if isinstance(value, str):
            return _decode_string_to_bytes(value)
        if isinstance(value, list):
            return bytes(value)
        return str(value).encode("utf-8")


    # Mapping of primitive types to their parser functions
    _PRIMITIVE_PARSERS = {
        StringType: _parse_string,
        IntegerType: _parse_integer,
        LongType: _parse_integer,
        FloatType: _parse_float,
        DoubleType: _parse_float,
        DecimalType: _parse_decimal,
        BooleanType: _parse_boolean,
        DateType: _parse_date,
        TimestampType: _parse_timestamp,
        BinaryType: _parse_binary,
    }


    def parse_value(value: Any, field_type: DataType) -> Any:
        """
        Converts a JSON value into a PySpark-compatible data type based on the provided field type.
        """
        if value is None:
            return None

        # Handle complex types
        if isinstance(field_type, StructType):
            return _parse_struct(value, field_type)
        if isinstance(field_type, ArrayType):
            return _parse_array(value, field_type)
        if isinstance(field_type, MapType):
            return _parse_map(value, field_type)

        # Handle primitive types via type-based lookup
        try:
            field_type_class = type(field_type)
            if field_type_class in _PRIMITIVE_PARSERS:
                return _PRIMITIVE_PARSERS[field_type_class](value)

            # Check for custom UDT handling
            if hasattr(field_type, "fromJson"):
                return field_type.fromJson(value)

            raise TypeError(f"Unsupported field type: {field_type}")
        except (ValueError, TypeError) as e:
            raise ValueError(f"Error converting '{value}' ({type(value)}) to {field_type}: {str(e)}")


    ########################################################
    # sources/gmail/gmail.py
    ########################################################

    class LakeflowConnect:
        """Gmail connector implementing the LakeflowConnect interface with 100% API coverage."""

        # Supported tables - Full Gmail API coverage
        SUPPORTED_TABLES = [
            "messages",
            "threads",
            "labels",
            "drafts",
            "profile",
            "settings",
            "filters",
            "forwarding_addresses",
            "send_as",
            "delegates",
        ]

        # Batch API settings
        BATCH_SIZE = 50  # Gmail batch API supports up to 100, using 50 for safety
        MAX_WORKERS = 5  # Concurrent workers for parallel fetching

        def __init__(self, options: dict[str, str]) -> None:
            """
            Initialize the Gmail connector with OAuth 2.0 credentials.

            Expected options:
                - client_id: OAuth 2.0 client ID from Google Cloud Console
                - client_secret: OAuth 2.0 client secret
                - refresh_token: Long-lived refresh token obtained via OAuth flow
                - user_id (optional): User email or 'me' (default: 'me')
            """
            self.client_id = options.get("client_id")
            self.client_secret = options.get("client_secret")
            self.refresh_token = options.get("refresh_token")
            self.user_id = options.get("user_id", "me")

            if not self.client_id:
                raise ValueError("Gmail connector requires 'client_id' in options")
            if not self.client_secret:
                raise ValueError("Gmail connector requires 'client_secret' in options")
            if not self.refresh_token:
                raise ValueError("Gmail connector requires 'refresh_token' in options")

            self.base_url = "https://gmail.googleapis.com/gmail/v1"
            self.batch_url = "https://gmail.googleapis.com/batch/gmail/v1"
            self._access_token = None
            self._token_expires_at = 0

            # Session for connection reuse
            self._session = requests.Session()

        def _get_access_token(self) -> str:
            """Exchange refresh token for access token with caching."""
            # Return cached token if still valid (with 60s buffer)
            if self._access_token and time.time() < self._token_expires_at - 60:
                return self._access_token

            response = requests.post(
                "https://oauth2.googleapis.com/token",
                data={
                    "client_id": self.client_id,
                    "client_secret": self.client_secret,
                    "refresh_token": self.refresh_token,
                    "grant_type": "refresh_token",
                },
            )
            response.raise_for_status()
            data = response.json()

            self._access_token = data["access_token"]
            self._token_expires_at = time.time() + data.get("expires_in", 3600)

            return self._access_token

        def _get_headers(self) -> Dict[str, str]:
            """Get headers with valid access token."""
            return {
                "Authorization": f"Bearer {self._get_access_token()}",
                "Accept": "application/json",
            }

        def _make_request(
            self, method: str, endpoint: str, params: Dict = None, retry_count: int = 3
        ) -> Dict:
            """Make API request with retry and rate limit handling."""
            url = f"{self.base_url}{endpoint}"

            for attempt in range(retry_count):
                response = self._session.request(
                    method, url, headers=self._get_headers(), params=params
                )

                if response.status_code == 200:
                    return response.json()
                elif response.status_code == 429:
                    # Rate limited - exponential backoff
                    wait_time = (2**attempt) + 1
                    time.sleep(wait_time)
                    continue
                elif response.status_code == 404:
                    # History ID expired or resource not found
                    return None
                else:
                    response.raise_for_status()

            raise Exception(f"Failed after {retry_count} retries")

        def _make_batch_request(
            self, endpoints: List[str], params_list: List[Dict] = None
        ) -> List[Dict]:
            """
            Make batch API request for efficient bulk data retrieval.

            Gmail batch API allows up to 100 requests in a single HTTP call,
            reducing network overhead significantly.
            """
            if not endpoints:
                return []

            if params_list is None:
                params_list = [{}] * len(endpoints)

            # Build multipart batch request body
            boundary = "batch_gmail_connector"
            body_parts = []

            for i, (endpoint, params) in enumerate(zip(endpoints, params_list)):
                url = f"{self.base_url}{endpoint}"
                if params:
                    query_string = "&".join(f"{k}={v}" for k, v in params.items())
                    url = f"{url}?{query_string}"

                part = f"--{boundary}\r\n"
                part += "Content-Type: application/http\r\n"
                part += f"Content-ID: <item{i}>\r\n\r\n"
                part += f"GET {url}\r\n"
                body_parts.append(part)

            body = "\r\n".join(body_parts) + f"\r\n--{boundary}--"

            headers = self._get_headers()
            headers["Content-Type"] = f"multipart/mixed; boundary={boundary}"

            response = self._session.post(self.batch_url, headers=headers, data=body)

            if response.status_code != 200:
                # Fall back to sequential requests on batch failure
                return self._fetch_sequential(endpoints, params_list)

            return self._parse_batch_response(response.text, boundary)

        def _parse_batch_response(self, response_text: str, boundary: str) -> List[Dict]:
            """Parse multipart batch response."""
            results = []
            parts = response_text.split(f"--{boundary}")

            for part in parts:
                if "Content-Type: application/json" in part or '{"' in part:
                    # Extract JSON from the response part
                    try:
                        json_start = part.find("{")
                        json_end = part.rfind("}") + 1
                        if json_start >= 0 and json_end > json_start:
                            json_str = part[json_start:json_end]
                            results.append(json.loads(json_str))
                    except (json.JSONDecodeError, ValueError):
                        continue

            return results

        def _fetch_sequential(
            self, endpoints: List[str], params_list: List[Dict]
        ) -> List[Dict]:
            """Fallback sequential fetch when batch fails."""
            results = []
            for endpoint, params in zip(endpoints, params_list):
                result = self._make_request("GET", endpoint, params)
                if result:
                    results.append(result)
            return results

        def _fetch_details_parallel(
            self, ids: List[str], fetch_func, table_options: Dict[str, str]
        ) -> Generator[Dict, None, None]:
            """
            Fetch details in parallel using thread pool.
            Yields results as they complete for true streaming.
            """
            with ThreadPoolExecutor(max_workers=self.MAX_WORKERS) as executor:
                futures = {
                    executor.submit(fetch_func, id_, table_options): id_ for id_ in ids
                }
                for future in as_completed(futures):
                    try:
                        result = future.result()
                        if result:
                            yield result
                    except Exception:
                        # Skip failed fetches, continue with others
                        continue

        def list_tables(self) -> List[str]:
            """Return the list of available Gmail tables."""
            return self.SUPPORTED_TABLES.copy()

        def _validate_table(self, table_name: str) -> None:
            """Validate that the table is supported."""
            if table_name not in self.SUPPORTED_TABLES:
                raise ValueError(
                    f"Unsupported table: '{table_name}'. "
                    f"Supported tables are: {self.SUPPORTED_TABLES}"
                )

        def get_table_schema(
            self, table_name: str, table_options: Dict[str, str]
        ) -> StructType:
            """
            Fetch the schema of a table.

            Args:
                table_name: The name of the table
                table_options: Additional options (not used for Gmail)

            Returns:
                StructType representing the schema
            """
            self._validate_table(table_name)

            schema_map = {
                "messages": self._get_messages_schema,
                "threads": self._get_threads_schema,
                "labels": self._get_labels_schema,
                "drafts": self._get_drafts_schema,
                "profile": self._get_profile_schema,
                "settings": self._get_settings_schema,
                "filters": self._get_filters_schema,
                "forwarding_addresses": self._get_forwarding_addresses_schema,
                "send_as": self._get_send_as_schema,
                "delegates": self._get_delegates_schema,
            }
            return schema_map[table_name]()

        def _get_header_struct(self) -> StructType:
            """Return the header struct used across schemas."""
            return StructType(
                [
                    StructField("name", StringType(), True),
                    StructField("value", StringType(), True),
                ]
            )

        def _get_body_struct(self) -> StructType:
            """Return the body struct used across schemas."""
            return StructType(
                [
                    StructField("attachmentId", StringType(), True),
                    StructField("size", LongType(), True),
                    StructField("data", StringType(), True),
                ]
            )

        def _get_messages_schema(self) -> StructType:
            """Return schema for messages table."""
            header_struct = self._get_header_struct()
            body_struct = self._get_body_struct()

            # MessagePart struct for payload.parts (recursive structure simplified)
            part_struct = StructType(
                [
                    StructField("partId", StringType(), True),
                    StructField("mimeType", StringType(), True),
                    StructField("filename", StringType(), True),
                    StructField("headers", ArrayType(header_struct), True),
                    StructField("body", body_struct, True),
                ]
            )

            # Payload struct
            payload_struct = StructType(
                [
                    StructField("partId", StringType(), True),
                    StructField("mimeType", StringType(), True),
                    StructField("filename", StringType(), True),
                    StructField("headers", ArrayType(header_struct), True),
                    StructField("body", body_struct, True),
                    StructField("parts", ArrayType(part_struct), True),
                ]
            )

            return StructType(
                [
                    StructField("id", StringType(), False),
                    StructField("threadId", StringType(), True),
                    StructField("labelIds", ArrayType(StringType()), True),
                    StructField("snippet", StringType(), True),
                    StructField("historyId", StringType(), True),
                    StructField("internalDate", StringType(), True),
                    StructField("sizeEstimate", LongType(), True),
                    StructField("payload", payload_struct, True),
                ]
            )

        def _get_threads_schema(self) -> StructType:
            """Return schema for threads table."""
            # Simplified message struct for threads (without full payload)
            message_in_thread_struct = StructType(
                [
                    StructField("id", StringType(), True),
                    StructField("threadId", StringType(), True),
                    StructField("labelIds", ArrayType(StringType()), True),
                    StructField("snippet", StringType(), True),
                    StructField("historyId", StringType(), True),
                    StructField("internalDate", StringType(), True),
                ]
            )

            return StructType(
                [
                    StructField("id", StringType(), False),
                    StructField("snippet", StringType(), True),
                    StructField("historyId", StringType(), True),
                    StructField("messages", ArrayType(message_in_thread_struct), True),
                ]
            )

        def _get_labels_schema(self) -> StructType:
            """Return schema for labels table."""
            color_struct = StructType(
                [
                    StructField("textColor", StringType(), True),
                    StructField("backgroundColor", StringType(), True),
                ]
            )

            return StructType(
                [
                    StructField("id", StringType(), False),
                    StructField("name", StringType(), True),
                    StructField("messageListVisibility", StringType(), True),
                    StructField("labelListVisibility", StringType(), True),
                    StructField("type", StringType(), True),
                    StructField("messagesTotal", LongType(), True),
                    StructField("messagesUnread", LongType(), True),
                    StructField("threadsTotal", LongType(), True),
                    StructField("threadsUnread", LongType(), True),
                    StructField("color", color_struct, True),
                ]
            )

        def _get_drafts_schema(self) -> StructType:
            """Return schema for drafts table."""
            header_struct = self._get_header_struct()
            body_struct = self._get_body_struct()

            part_struct = StructType(
                [
                    StructField("partId", StringType(), True),
                    StructField("mimeType", StringType(), True),
                    StructField("filename", StringType(), True),
                    StructField("headers", ArrayType(header_struct), True),
                    StructField("body", body_struct, True),
                ]
            )

            payload_struct = StructType(
                [
                    StructField("partId", StringType(), True),
                    StructField("mimeType", StringType(), True),
                    StructField("filename", StringType(), True),
                    StructField("headers", ArrayType(header_struct), True),
                    StructField("body", body_struct, True),
                    StructField("parts", ArrayType(part_struct), True),
                ]
            )

            message_struct = StructType(
                [
                    StructField("id", StringType(), True),
                    StructField("threadId", StringType(), True),
                    StructField("labelIds", ArrayType(StringType()), True),
                    StructField("snippet", StringType(), True),
                    StructField("historyId", StringType(), True),
                    StructField("internalDate", StringType(), True),
                    StructField("sizeEstimate", LongType(), True),
                    StructField("payload", payload_struct, True),
                ]
            )

            return StructType(
                [
                    StructField("id", StringType(), False),
                    StructField("message", message_struct, True),
                ]
            )

        def _get_profile_schema(self) -> StructType:
            """Return schema for profile table."""
            return StructType(
                [
                    StructField("emailAddress", StringType(), False),
                    StructField("messagesTotal", LongType(), True),
                    StructField("threadsTotal", LongType(), True),
                    StructField("historyId", StringType(), True),
                ]
            )

        def _get_settings_schema(self) -> StructType:
            """Return schema for settings table (combined IMAP, POP, vacation, language)."""
            # Auto-forwarding settings
            auto_forwarding_struct = StructType(
                [
                    StructField("enabled", BooleanType(), True),
                    StructField("emailAddress", StringType(), True),
                    StructField("disposition", StringType(), True),
                ]
            )

            # IMAP settings
            imap_struct = StructType(
                [
                    StructField("enabled", BooleanType(), True),
                    StructField("autoExpunge", BooleanType(), True),
                    StructField("expungeBehavior", StringType(), True),
                    StructField("maxFolderSize", LongType(), True),
                ]
            )

            # POP settings
            pop_struct = StructType(
                [
                    StructField("accessWindow", StringType(), True),
                    StructField("disposition", StringType(), True),
                ]
            )

            # Language settings
            language_struct = StructType(
                [
                    StructField("displayLanguage", StringType(), True),
                ]
            )

            # Vacation settings
            vacation_struct = StructType(
                [
                    StructField("enableAutoReply", BooleanType(), True),
                    StructField("responseSubject", StringType(), True),
                    StructField("responseBodyPlainText", StringType(), True),
                    StructField("responseBodyHtml", StringType(), True),
                    StructField("restrictToContacts", BooleanType(), True),
                    StructField("restrictToDomain", BooleanType(), True),
                    StructField("startTime", StringType(), True),
                    StructField("endTime", StringType(), True),
                ]
            )

            return StructType(
                [
                    StructField("emailAddress", StringType(), False),
                    StructField("autoForwarding", auto_forwarding_struct, True),
                    StructField("imap", imap_struct, True),
                    StructField("pop", pop_struct, True),
                    StructField("language", language_struct, True),
                    StructField("vacation", vacation_struct, True),
                ]
            )

        def _get_filters_schema(self) -> StructType:
            """Return schema for filters table."""
            # Filter criteria
            criteria_struct = StructType(
                [
                    StructField("from", StringType(), True),
                    StructField("to", StringType(), True),
                    StructField("subject", StringType(), True),
                    StructField("query", StringType(), True),
                    StructField("negatedQuery", StringType(), True),
                    StructField("hasAttachment", BooleanType(), True),
                    StructField("excludeChats", BooleanType(), True),
                    StructField("size", LongType(), True),
                    StructField("sizeComparison", StringType(), True),
                ]
            )

            # Filter action
            action_struct = StructType(
                [
                    StructField("addLabelIds", ArrayType(StringType()), True),
                    StructField("removeLabelIds", ArrayType(StringType()), True),
                    StructField("forward", StringType(), True),
                ]
            )

            return StructType(
                [
                    StructField("id", StringType(), False),
                    StructField("criteria", criteria_struct, True),
                    StructField("action", action_struct, True),
                ]
            )

        def _get_forwarding_addresses_schema(self) -> StructType:
            """Return schema for forwarding_addresses table."""
            return StructType(
                [
                    StructField("forwardingEmail", StringType(), False),
                    StructField("verificationStatus", StringType(), True),
                ]
            )

        def _get_send_as_schema(self) -> StructType:
            """Return schema for send_as table."""
            # SMIME info struct
            smime_struct = StructType(
                [
                    StructField("id", StringType(), True),
                    StructField("issuerCn", StringType(), True),
                    StructField("isDefault", BooleanType(), True),
                    StructField("expiration", StringType(), True),
                ]
            )

            return StructType(
                [
                    StructField("sendAsEmail", StringType(), False),
                    StructField("displayName", StringType(), True),
                    StructField("replyToAddress", StringType(), True),
                    StructField("signature", StringType(), True),
                    StructField("isPrimary", BooleanType(), True),
                    StructField("isDefault", BooleanType(), True),
                    StructField("treatAsAlias", BooleanType(), True),
                    StructField("verificationStatus", StringType(), True),
                    StructField("smtpMsa", StructType([
                        StructField("host", StringType(), True),
                        StructField("port", LongType(), True),
                        StructField("username", StringType(), True),
                        StructField("securityMode", StringType(), True),
                    ]), True),
                ]
            )

        def _get_delegates_schema(self) -> StructType:
            """Return schema for delegates table."""
            return StructType(
                [
                    StructField("delegateEmail", StringType(), False),
                    StructField("verificationStatus", StringType(), True),
                ]
            )

        def read_table_metadata(
            self, table_name: str, table_options: Dict[str, str]
        ) -> Dict:
            """
            Fetch the metadata of a table.

            Args:
                table_name: The name of the table
                table_options: Additional options

            Returns:
                Dictionary with primary_keys, cursor_field, and ingestion_type
            """
            self._validate_table(table_name)

            metadata_map = {
                "messages": {
                    "primary_keys": ["id"],
                    "cursor_field": "historyId",
                    "ingestion_type": "cdc",
                },
                "threads": {
                    "primary_keys": ["id"],
                    "cursor_field": "historyId",
                    "ingestion_type": "cdc",
                },
                "labels": {
                    "primary_keys": ["id"],
                    "ingestion_type": "snapshot",
                },
                "drafts": {
                    "primary_keys": ["id"],
                    "ingestion_type": "snapshot",
                },
                "profile": {
                    "primary_keys": ["emailAddress"],
                    "ingestion_type": "snapshot",
                },
                "settings": {
                    "primary_keys": ["emailAddress"],
                    "ingestion_type": "snapshot",
                },
                "filters": {
                    "primary_keys": ["id"],
                    "ingestion_type": "snapshot",
                },
                "forwarding_addresses": {
                    "primary_keys": ["forwardingEmail"],
                    "ingestion_type": "snapshot",
                },
                "send_as": {
                    "primary_keys": ["sendAsEmail"],
                    "ingestion_type": "snapshot",
                },
                "delegates": {
                    "primary_keys": ["delegateEmail"],
                    "ingestion_type": "snapshot",
                },
            }
            return metadata_map[table_name]

        def read_table(
            self, table_name: str, start_offset: dict, table_options: Dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read records from a table with streaming iteration.

            For messages and threads with a start_offset containing historyId,
            uses the History API for incremental reads. Otherwise, performs
            a full list operation.

            Args:
                table_name: The name of the table to read
                start_offset: Offset containing historyId for incremental reads
                table_options: Additional options like 'q' (query), 'labelIds', 'maxResults'

            Returns:
                Tuple of (records iterator, next offset)
            """
            self._validate_table(table_name)

            read_map = {
                "messages": self._read_messages,
                "threads": self._read_threads,
                "labels": self._read_labels,
                "drafts": self._read_drafts,
                "profile": self._read_profile,
                "settings": self._read_settings,
                "filters": self._read_filters,
                "forwarding_addresses": self._read_forwarding_addresses,
                "send_as": self._read_send_as,
                "delegates": self._read_delegates,
            }
            return read_map[table_name](start_offset, table_options)

        def _read_messages(
            self, start_offset: dict, table_options: Dict[str, str]
        ) -> (Iterator[dict], dict):
            """Read messages using streaming list + batch get pattern."""
            max_results = int(table_options.get("maxResults", "100"))
            query = table_options.get("q")
            label_ids = table_options.get("labelIds")
            include_spam_trash = (
                table_options.get("includeSpamTrash", "false").lower() == "true"
            )

            start_history_id = start_offset.get("historyId") if start_offset else None

            if start_history_id:
                return self._read_messages_incremental(start_history_id, table_options)
            else:
                return self._read_messages_streaming(
                    max_results, query, label_ids, include_spam_trash, table_options
                )

        def _read_messages_streaming(
            self,
            max_results: int,
            query: str,
            label_ids: str,
            include_spam_trash: bool,
            table_options: Dict[str, str],
        ) -> (Iterator[dict], dict):
            """Stream messages with batch fetching for efficiency."""
            params = {"maxResults": min(max_results, 500)}
            if query:
                params["q"] = query
            if label_ids:
                params["labelIds"] = label_ids
            if include_spam_trash:
                params["includeSpamTrash"] = "true"

            state = {"latest_history_id": None}

            def message_generator() -> Generator[Dict, None, None]:
                page_token = None

                while True:
                    if page_token:
                        params["pageToken"] = page_token

                    response = self._make_request(
                        "GET", f"/users/{self.user_id}/messages", params=params
                    )

                    if not response or "messages" not in response:
                        break

                    message_ids = [m["id"] for m in response.get("messages", [])]
                    format_type = table_options.get("format", "full")

                    for i in range(0, len(message_ids), self.BATCH_SIZE):
                        batch_ids = message_ids[i : i + self.BATCH_SIZE]
                        endpoints = [
                            f"/users/{self.user_id}/messages/{mid}" for mid in batch_ids
                        ]
                        params_list = [{"format": format_type}] * len(batch_ids)

                        batch_results = self._make_batch_request(endpoints, params_list)

                        for msg_detail in batch_results:
                            if msg_detail:
                                if msg_detail.get("historyId"):
                                    if (
                                        not state["latest_history_id"]
                                        or msg_detail["historyId"]
                                        > state["latest_history_id"]
                                    ):
                                        state["latest_history_id"] = msg_detail["historyId"]
                                yield msg_detail

                    page_token = response.get("nextPageToken")
                    if not page_token:
                        break

            generator = message_generator()
            records = list(generator)
            next_offset = (
                {"historyId": state["latest_history_id"]}
                if state["latest_history_id"]
                else {}
            )
            return iter(records), next_offset

        def _read_messages_incremental(
            self, start_history_id: str, table_options: Dict[str, str]
        ) -> (Iterator[dict], dict):
            """Read messages incrementally using History API with batch fetching."""
            params = {
                "startHistoryId": start_history_id,
                "maxResults": 500,
                "historyTypes": "messageAdded",
            }

            page_token = None
            latest_history_id = start_history_id
            all_message_ids = set()

            while True:
                if page_token:
                    params["pageToken"] = page_token

                response = self._make_request(
                    "GET", f"/users/{self.user_id}/history", params=params
                )

                if response is None:
                    return self._read_messages_streaming(
                        100, None, None, False, table_options
                    )

                if response.get("historyId"):
                    latest_history_id = response["historyId"]

                for history_record in response.get("history", []):
                    for added in history_record.get("messagesAdded", []):
                        msg_id = added.get("message", {}).get("id")
                        if msg_id:
                            all_message_ids.add(msg_id)

                page_token = response.get("nextPageToken")
                if not page_token:
                    break

            all_messages = []
            message_ids = list(all_message_ids)
            format_type = table_options.get("format", "full")

            for i in range(0, len(message_ids), self.BATCH_SIZE):
                batch_ids = message_ids[i : i + self.BATCH_SIZE]
                endpoints = [f"/users/{self.user_id}/messages/{mid}" for mid in batch_ids]
                params_list = [{"format": format_type}] * len(batch_ids)

                batch_results = self._make_batch_request(endpoints, params_list)
                all_messages.extend([r for r in batch_results if r])

            next_offset = {"historyId": latest_history_id}
            return iter(all_messages), next_offset

        def _read_threads(
            self, start_offset: dict, table_options: Dict[str, str]
        ) -> (Iterator[dict], dict):
            """Read threads using streaming list + batch get pattern."""
            max_results = int(table_options.get("maxResults", "100"))
            query = table_options.get("q")
            label_ids = table_options.get("labelIds")
            include_spam_trash = (
                table_options.get("includeSpamTrash", "false").lower() == "true"
            )

            start_history_id = start_offset.get("historyId") if start_offset else None

            if start_history_id:
                return self._read_threads_incremental(start_history_id, table_options)
            else:
                return self._read_threads_streaming(
                    max_results, query, label_ids, include_spam_trash, table_options
                )

        def _read_threads_streaming(
            self,
            max_results: int,
            query: str,
            label_ids: str,
            include_spam_trash: bool,
            table_options: Dict[str, str],
        ) -> (Iterator[dict], dict):
            """Stream threads with batch fetching."""
            params = {"maxResults": min(max_results, 500)}
            if query:
                params["q"] = query
            if label_ids:
                params["labelIds"] = label_ids
            if include_spam_trash:
                params["includeSpamTrash"] = "true"

            state = {"latest_history_id": None}
            all_threads = []
            page_token = None

            while True:
                if page_token:
                    params["pageToken"] = page_token

                response = self._make_request(
                    "GET", f"/users/{self.user_id}/threads", params=params
                )

                if not response or "threads" not in response:
                    break

                thread_ids = [t["id"] for t in response.get("threads", [])]
                format_type = table_options.get("format", "full")

                for i in range(0, len(thread_ids), self.BATCH_SIZE):
                    batch_ids = thread_ids[i : i + self.BATCH_SIZE]
                    endpoints = [
                        f"/users/{self.user_id}/threads/{tid}" for tid in batch_ids
                    ]
                    params_list = [{"format": format_type}] * len(batch_ids)

                    batch_results = self._make_batch_request(endpoints, params_list)

                    for thread_detail in batch_results:
                        if thread_detail:
                            if thread_detail.get("historyId"):
                                if (
                                    not state["latest_history_id"]
                                    or thread_detail["historyId"]
                                    > state["latest_history_id"]
                                ):
                                    state["latest_history_id"] = thread_detail["historyId"]
                            all_threads.append(thread_detail)

                page_token = response.get("nextPageToken")
                if not page_token:
                    break

            next_offset = (
                {"historyId": state["latest_history_id"]}
                if state["latest_history_id"]
                else {}
            )
            return iter(all_threads), next_offset

        def _read_threads_incremental(
            self, start_history_id: str, table_options: Dict[str, str]
        ) -> (Iterator[dict], dict):
            """Read threads incrementally using History API."""
            params = {
                "startHistoryId": start_history_id,
                "maxResults": 500,
                "historyTypes": "messageAdded",
            }

            page_token = None
            latest_history_id = start_history_id
            all_thread_ids = set()

            while True:
                if page_token:
                    params["pageToken"] = page_token

                response = self._make_request(
                    "GET", f"/users/{self.user_id}/history", params=params
                )

                if response is None:
                    return self._read_threads_streaming(100, None, None, False, table_options)

                if response.get("historyId"):
                    latest_history_id = response["historyId"]

                for history_record in response.get("history", []):
                    for added in history_record.get("messagesAdded", []):
                        thread_id = added.get("message", {}).get("threadId")
                        if thread_id:
                            all_thread_ids.add(thread_id)

                page_token = response.get("nextPageToken")
                if not page_token:
                    break

            all_threads = []
            thread_ids = list(all_thread_ids)
            format_type = table_options.get("format", "full")

            for i in range(0, len(thread_ids), self.BATCH_SIZE):
                batch_ids = thread_ids[i : i + self.BATCH_SIZE]
                endpoints = [f"/users/{self.user_id}/threads/{tid}" for tid in batch_ids]
                params_list = [{"format": format_type}] * len(batch_ids)

                batch_results = self._make_batch_request(endpoints, params_list)
                all_threads.extend([r for r in batch_results if r])

            next_offset = {"historyId": latest_history_id}
            return iter(all_threads), next_offset

        def _read_labels(
            self, start_offset: dict, table_options: Dict[str, str]
        ) -> (Iterator[dict], dict):
            """Read all labels (snapshot mode) with batch fetching."""
            response = self._make_request("GET", f"/users/{self.user_id}/labels")

            if not response or "labels" not in response:
                return iter([]), {}

            label_ids = [label["id"] for label in response.get("labels", [])]

            all_labels = []
            for i in range(0, len(label_ids), self.BATCH_SIZE):
                batch_ids = label_ids[i : i + self.BATCH_SIZE]
                endpoints = [f"/users/{self.user_id}/labels/{lid}" for lid in batch_ids]

                batch_results = self._make_batch_request(endpoints)
                all_labels.extend([r for r in batch_results if r])

            return iter(all_labels), {}

        def _read_drafts(
            self, start_offset: dict, table_options: Dict[str, str]
        ) -> (Iterator[dict], dict):
            """Read all drafts (snapshot mode) with batch fetching."""
            params = {"maxResults": int(table_options.get("maxResults", "100"))}

            all_drafts = []
            page_token = None

            while True:
                if page_token:
                    params["pageToken"] = page_token

                response = self._make_request(
                    "GET", f"/users/{self.user_id}/drafts", params=params
                )

                if not response or "drafts" not in response:
                    break

                draft_ids = [d["id"] for d in response.get("drafts", [])]
                format_type = table_options.get("format", "full")

                for i in range(0, len(draft_ids), self.BATCH_SIZE):
                    batch_ids = draft_ids[i : i + self.BATCH_SIZE]
                    endpoints = [
                        f"/users/{self.user_id}/drafts/{did}" for did in batch_ids
                    ]
                    params_list = [{"format": format_type}] * len(batch_ids)

                    batch_results = self._make_batch_request(endpoints, params_list)
                    all_drafts.extend([r for r in batch_results if r])

                page_token = response.get("nextPageToken")
                if not page_token:
                    break

            return iter(all_drafts), {}

        def _read_profile(
            self, start_offset: dict, table_options: Dict[str, str]
        ) -> (Iterator[dict], dict):
            """Read user profile (single record)."""
            response = self._make_request("GET", f"/users/{self.user_id}/profile")

            if not response:
                return iter([]), {}

            return iter([response]), {}

        def _read_settings(
            self, start_offset: dict, table_options: Dict[str, str]
        ) -> (Iterator[dict], dict):
            """Read all user settings combined into a single record."""
            # Fetch all settings in parallel
            endpoints = [
                f"/users/{self.user_id}/settings/autoForwarding",
                f"/users/{self.user_id}/settings/imap",
                f"/users/{self.user_id}/settings/pop",
                f"/users/{self.user_id}/settings/language",
                f"/users/{self.user_id}/settings/vacation",
            ]

            results = self._make_batch_request(endpoints)

            # Get email address from profile
            profile = self._make_request("GET", f"/users/{self.user_id}/profile")
            email_address = profile.get("emailAddress", self.user_id) if profile else self.user_id

            # Combine all settings into one record
            settings = {
                "emailAddress": email_address,
                "autoForwarding": results[0] if len(results) > 0 else None,
                "imap": results[1] if len(results) > 1 else None,
                "pop": results[2] if len(results) > 2 else None,
                "language": results[3] if len(results) > 3 else None,
                "vacation": results[4] if len(results) > 4 else None,
            }

            return iter([settings]), {}

        def _read_filters(
            self, start_offset: dict, table_options: Dict[str, str]
        ) -> (Iterator[dict], dict):
            """Read all email filters."""
            response = self._make_request("GET", f"/users/{self.user_id}/settings/filters")

            if not response or "filter" not in response:
                return iter([]), {}

            return iter(response.get("filter", [])), {}

        def _read_forwarding_addresses(
            self, start_offset: dict, table_options: Dict[str, str]
        ) -> (Iterator[dict], dict):
            """Read all forwarding addresses."""
            response = self._make_request(
                "GET", f"/users/{self.user_id}/settings/forwardingAddresses"
            )

            if not response or "forwardingAddresses" not in response:
                return iter([]), {}

            return iter(response.get("forwardingAddresses", [])), {}

        def _read_send_as(
            self, start_offset: dict, table_options: Dict[str, str]
        ) -> (Iterator[dict], dict):
            """Read all send-as aliases."""
            response = self._make_request("GET", f"/users/{self.user_id}/settings/sendAs")

            if not response or "sendAs" not in response:
                return iter([]), {}

            return iter(response.get("sendAs", [])), {}

        def _read_delegates(
            self, start_offset: dict, table_options: Dict[str, str]
        ) -> (Iterator[dict], dict):
            """Read all delegates."""
            response = self._make_request("GET", f"/users/{self.user_id}/settings/delegates")

            if not response or "delegates" not in response:
                return iter([]), {}

            return iter(response.get("delegates", [])), {}

        def read_table_deletes(
            self, table_name: str, start_offset: dict, table_options: Dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read deleted records from Gmail using History API.

            Gmail History API provides messagesDeleted events for tracking deletions.
            This is only applicable for messages and threads (CDC tables).

            Args:
                table_name: The table to read deletes from
                start_offset: Offset containing historyId
                table_options: Additional options

            Returns:
                Tuple of (deleted records iterator, next offset)
            """
            self._validate_table(table_name)

            # Only messages and threads support delete tracking
            if table_name not in ("messages", "threads"):
                return iter([]), {}

            start_history_id = start_offset.get("historyId") if start_offset else None
            if not start_history_id:
                return iter([]), {}

            params = {
                "startHistoryId": start_history_id,
                "maxResults": 500,
                "historyTypes": "messageDeleted",
            }

            page_token = None
            latest_history_id = start_history_id
            deleted_records = []
            seen_ids = set()

            while True:
                if page_token:
                    params["pageToken"] = page_token

                response = self._make_request(
                    "GET", f"/users/{self.user_id}/history", params=params
                )

                if response is None:
                    break

                if response.get("historyId"):
                    latest_history_id = response["historyId"]

                for history_record in response.get("history", []):
                    for deleted in history_record.get("messagesDeleted", []):
                        msg = deleted.get("message", {})
                        if msg.get("id"):
                            if table_name == "messages":
                                if msg["id"] not in seen_ids:
                                    seen_ids.add(msg["id"])
                                    deleted_records.append(
                                        {
                                            "id": msg["id"],
                                            "threadId": msg.get("threadId"),
                                            "historyId": history_record.get(
                                                "id", latest_history_id
                                            ),
                                        }
                                    )
                            elif table_name == "threads":
                                thread_id = msg.get("threadId")
                                if thread_id and thread_id not in seen_ids:
                                    seen_ids.add(thread_id)
                                    deleted_records.append(
                                        {
                                            "id": thread_id,
                                            "historyId": history_record.get(
                                                "id", latest_history_id
                                            ),
                                        }
                                    )

                page_token = response.get("nextPageToken")
                if not page_token:
                    break

            next_offset = {"historyId": latest_history_id}
            return iter(deleted_records), next_offset


    ########################################################
    # pipeline/lakeflow_python_source.py
    ########################################################

    METADATA_TABLE = "_lakeflow_metadata"
    TABLE_NAME = "tableName"
    TABLE_NAME_LIST = "tableNameList"
    TABLE_CONFIGS = "tableConfigs"
    IS_DELETE_FLOW = "isDeleteFlow"


    class LakeflowStreamReader(SimpleDataSourceStreamReader):
        """
        Implements a data source stream reader for Lakeflow Connect.
        Currently, only the simpleStreamReader is implemented, which uses a
        more generic protocol suitable for most data sources that support
        incremental loading.
        """

        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.lakeflow_connect = lakeflow_connect
            self.schema = schema

        def initialOffset(self):
            return {}

        def read(self, start: dict) -> (Iterator[tuple], dict):
            is_delete_flow = self.options.get(IS_DELETE_FLOW) == "true"
            # Strip delete flow options before passing to connector
            table_options = {
                k: v for k, v in self.options.items() if k != IS_DELETE_FLOW
            }

            if is_delete_flow:
                records, offset = self.lakeflow_connect.read_table_deletes(
                    self.options[TABLE_NAME], start, table_options
                )
            else:
                records, offset = self.lakeflow_connect.read_table(
                    self.options[TABLE_NAME], start, table_options
                )
            rows = map(lambda x: parse_value(x, self.schema), records)
            return rows, offset

        def readBetweenOffsets(self, start: dict, end: dict) -> Iterator[tuple]:
            # TODO: This does not ensure the records returned are identical across repeated calls.
            # For append-only tables, the data source must guarantee that reading from the same
            # start offset will always yield the same set of records.
            # For tables ingested as incremental CDC, it is only necessary that no new changes
            # are missed in the returned records.
            return self.read(start)[0]


    class LakeflowBatchReader(DataSourceReader):
        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.schema = schema
            self.lakeflow_connect = lakeflow_connect
            self.table_name = options[TABLE_NAME]

        def read(self, partition):
            all_records = []
            if self.table_name == METADATA_TABLE:
                all_records = self._read_table_metadata()
            else:
                all_records, _ = self.lakeflow_connect.read_table(
                    self.table_name, None, self.options
                )

            rows = map(lambda x: parse_value(x, self.schema), all_records)
            return iter(rows)

        def _read_table_metadata(self):
            table_name_list = self.options.get(TABLE_NAME_LIST, "")
            table_names = [o.strip() for o in table_name_list.split(",") if o.strip()]
            all_records = []
            table_configs = json.loads(self.options.get(TABLE_CONFIGS, "{}"))
            for table in table_names:
                metadata = self.lakeflow_connect.read_table_metadata(
                    table, table_configs.get(table, {})
                )
                all_records.append({TABLE_NAME: table, **metadata})
            return all_records


    class LakeflowSource(DataSource):
        def __init__(self, options):
            self.options = options
            self.lakeflow_connect = LakeflowConnect(options)

        @classmethod
        def name(cls):
            return "lakeflow_connect"

        def schema(self):
            table = self.options[TABLE_NAME]
            if table == METADATA_TABLE:
                return StructType(
                    [
                        StructField(TABLE_NAME, StringType(), False),
                        StructField("primary_keys", ArrayType(StringType()), True),
                        StructField("cursor_field", StringType(), True),
                        StructField("ingestion_type", StringType(), True),
                    ]
                )
            else:
                # Assuming the LakeflowConnect interface uses get_table_schema, not get_table_details
                return self.lakeflow_connect.get_table_schema(table, self.options)

        def reader(self, schema: StructType):
            return LakeflowBatchReader(self.options, schema, self.lakeflow_connect)

        def simpleStreamReader(self, schema: StructType):
            return LakeflowStreamReader(self.options, schema, self.lakeflow_connect)


    spark.dataSource.register(LakeflowSource)  # pylint: disable=undefined-variable
