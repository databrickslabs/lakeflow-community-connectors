# ==============================================================================
# Merged Lakeflow Source: employmenthero
# ==============================================================================
# This file is auto-generated by tools/scripts/merge_python_source.py
# Do not edit manually. Make changes to the source files instead.
# ==============================================================================

from datetime import datetime, timedelta
from decimal import Decimal
from typing import (
    Any,
    Dict,
    Iterator,
    List,
    Optional,
    Tuple,
)
import json
import re
import time

from pydantic import BaseModel, ConfigDict, PositiveInt
from pyspark.sql import Row
from pyspark.sql.datasource import DataSource, DataSourceReader, SimpleDataSourceStreamReader
from pyspark.sql.types import *
import base64
import random
import requests


def register_lakeflow_source(spark):
    """Register the Lakeflow Python source with Spark."""

    ########################################################
    # src/databricks/labs/community_connector/libs/utils.py
    ########################################################

    def _parse_struct(value: Any, field_type: StructType) -> Row:
        """Parse a dictionary into a PySpark Row based on StructType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for StructType, got {type(value)}")
        # Spark Python -> Arrow conversion require missing StructType fields to be assigned None.
        if value == {}:
            raise ValueError(
                "field in StructType cannot be an empty dict. "
                "Please assign None as the default value instead."
            )
        field_dict = {}
        for field in field_type.fields:
            if field.name in value:
                field_dict[field.name] = parse_value(value.get(field.name), field.dataType)
            elif field.nullable:
                field_dict[field.name] = None
            else:
                raise ValueError(f"Field {field.name} is not nullable but not found in the input")
        return Row(**field_dict)


    def _parse_array(value: Any, field_type: ArrayType) -> list:
        """Parse a list into a PySpark array based on ArrayType schema."""
        if not isinstance(value, list):
            if field_type.containsNull:
                return [parse_value(value, field_type.elementType)]
            raise ValueError(f"Expected a list for ArrayType, got {type(value)}")
        return [parse_value(v, field_type.elementType) for v in value]


    def _parse_map(value: Any, field_type: MapType) -> dict:
        """Parse a dictionary into a PySpark map based on MapType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for MapType, got {type(value)}")
        return {
            parse_value(k, field_type.keyType): parse_value(v, field_type.valueType)
            for k, v in value.items()
        }


    def _parse_string(value: Any) -> str:
        """Convert value to string."""
        return str(value)


    def _parse_integer(value: Any) -> int:
        """Convert value to integer."""
        if isinstance(value, str) and value.strip():
            return int(float(value)) if "." in value else int(value)
        if isinstance(value, (int, float)):
            return int(value)
        raise ValueError(f"Cannot convert {value} to integer")


    def _parse_float(value: Any) -> float:
        """Convert value to float."""
        return float(value)


    def _parse_decimal(value: Any) -> Decimal:
        """Convert value to Decimal."""
        return Decimal(value) if isinstance(value, str) and value.strip() else Decimal(str(value))


    def _parse_boolean(value: Any) -> bool:
        """Convert value to boolean."""
        if isinstance(value, str):
            lowered = value.lower()
            if lowered in ("true", "t", "yes", "y", "1"):
                return True
            if lowered in ("false", "f", "no", "n", "0"):
                return False
        return bool(value)


    def _parse_date(value: Any) -> datetime.date:
        """Convert value to date."""
        if isinstance(value, str):
            for fmt in ("%Y-%m-%d", "%m/%d/%Y", "%d-%m-%Y", "%Y/%m/%d"):
                try:
                    return datetime.strptime(value, fmt).date()
                except ValueError:
                    continue
            return datetime.fromisoformat(value).date()
        if isinstance(value, datetime):
            return value.date()
        raise ValueError(f"Cannot convert {value} to date")


    def _parse_timestamp(value: Any) -> datetime:
        """Convert value to timestamp."""
        if isinstance(value, str):
            ts_value = value.replace("Z", "+00:00") if value.endswith("Z") else value
            try:
                return datetime.fromisoformat(ts_value)
            except ValueError:
                for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S"):
                    try:
                        return datetime.strptime(ts_value, fmt)
                    except ValueError:
                        continue
        elif isinstance(value, (int, float)):
            return datetime.fromtimestamp(value)
        elif isinstance(value, datetime):
            return value
        raise ValueError(f"Cannot convert {value} to timestamp")


    def _decode_string_to_bytes(value: str) -> bytes:
        """Try to decode a string as base64, then hex, then UTF-8."""
        try:
            return base64.b64decode(value)
        except Exception:
            pass
        try:
            return bytes.fromhex(value)
        except Exception:
            pass
        return value.encode("utf-8")


    def _parse_binary(value: Any) -> bytes:
        """Convert value to bytes. Tries base64, then hex, then UTF-8 for strings."""
        if isinstance(value, bytes):
            return value
        if isinstance(value, bytearray):
            return bytes(value)
        if isinstance(value, str):
            return _decode_string_to_bytes(value)
        if isinstance(value, list):
            return bytes(value)
        return str(value).encode("utf-8")


    # Mapping of primitive types to their parser functions
    _PRIMITIVE_PARSERS = {
        StringType: _parse_string,
        IntegerType: _parse_integer,
        LongType: _parse_integer,
        FloatType: _parse_float,
        DoubleType: _parse_float,
        DecimalType: _parse_decimal,
        BooleanType: _parse_boolean,
        DateType: _parse_date,
        TimestampType: _parse_timestamp,
        BinaryType: _parse_binary,
    }


    def parse_value(value: Any, field_type: DataType) -> Any:
        """
        Converts a JSON value into a PySpark-compatible data type based on the provided field type.
        """
        if value is None:
            return None

        # Handle complex types
        if isinstance(field_type, StructType):
            return _parse_struct(value, field_type)
        if isinstance(field_type, ArrayType):
            return _parse_array(value, field_type)
        if isinstance(field_type, MapType):
            return _parse_map(value, field_type)

        # Handle primitive types via type-based lookup
        try:
            field_type_class = type(field_type)
            if field_type_class in _PRIMITIVE_PARSERS:
                return _PRIMITIVE_PARSERS[field_type_class](value)

            # Check for custom UDT handling
            if hasattr(field_type, "fromJson"):
                return field_type.fromJson(value)

            raise TypeError(f"Unsupported field type: {field_type}")
        except (ValueError, TypeError) as e:
            raise ValueError(f"Error converting '{value}' ({type(value)}) to {field_type}: {str(e)}")


    ########################################################
    # src/databricks/labs/community_connector/interface/lakeflow_connect.py
    ########################################################

    class LakeflowConnect:
        def __init__(self, options: dict[str, str]) -> None:
            """
            Initialize the source connector with parameters needed to connect to the source.
            Args:
                options: A dictionary of parameters like authentication tokens, table names,
                    and other configurations.
            """

        def list_tables(self) -> list[str]:
            """
            List names of all the tables supported by the source connector.
            The list could either be a static list or retrieved from the source via API.
            Returns:
                A list of table names.
            """

        def get_table_schema(
            self, table_name: str, table_options: dict[str, str]
        ) -> StructType:
            """
            Fetch the schema of a table.
            Args:
                table_name: The name of the table to fetch the schema for.
                table_options: A dictionary of options for accessing the table. For example,
                    the source API may require extra parameters needed to fetch the schema.
                    If there are no additional options required, you can ignore this
                    parameter, and no options will be provided during execution.
                    Only add parameters to table_options if they are essential for accessing
                    or retrieving the data (such as specifying table namespaces).
            Returns:
                A StructType object representing the schema of the table.
            """

        def read_table_metadata(
            self, table_name: str, table_options: dict[str, str]
        ) -> dict:
            """
            Fetch the metadata of a table.
            Args:
                table_name: The name of the table to fetch the metadata for.
                table_options: A dictionary of options for accessing the table. For example,
                    the source API may require extra parameters needed to fetch the metadata.
                    If there are no additional options required, you can ignore this
                    parameter, and no options will be provided during execution.
                    Only add parameters to table_options if they are essential for accessing
                    or retrieving the data (such as specifying table namespaces).
            Returns:
                A dictionary containing the metadata of the table. It should include the
                following keys:
                    - primary_keys: List of string names of the primary key columns of
                        the table.
                    - cursor_field: The name of the field to use as a cursor for
                        incremental loading.
                    - ingestion_type: The type of ingestion to use for the table. It
                        should be one of the following values:
                        - "snapshot": For snapshot loading.
                        - "cdc": Capture incremental changes (no delete support).
                        - "cdc_with_deletes": Capture incremental changes with delete
                            support. Requires implementing read_table_deletes().
                        - "append": Incremental append.
            """

        def read_table(
            self, table_name: str, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the records of a table and return an iterator of records and an offset.
            The read starts from the provided start_offset.
            Records returned in the iterator will be one batch of records marked by the
            offset as its end_offset.
            The read_table function could be called multiple times to read the entire table
            in multiple batches and it stops when the same offset is returned again.
            If the table cannot be incrementally read, the offset can be None if we want to
            read the entire table in one batch.
            We could still return some fake offsets (cannot checkpointing) to split the
            table into multiple batches.
            Args:
                table_name: The name of the table to read.
                start_offset: The offset to start reading from.
                table_options: A dictionary of options for accessing the table. For example,
                    the source API may require extra parameters needed to read the table.
                    If there are no additional options required, you can ignore this
                    parameter, and no options will be provided during execution.
                    Only add parameters to table_options if they are essential for accessing
                    or retrieving the data (such as specifying table namespaces).
            Returns:
                An iterator of records in JSON format and an offset.
                DO NOT convert the JSON based on the schema in `get_table_schema` in
                `read_table`.
                records: An iterator of records in JSON format.
                offset: An offset in dict.
            """

        def read_table_deletes(
            self, table_name: str, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read deleted records from a table for CDC delete synchronization.
            This method is called when ingestion_type is "cdc_with_deletes" to fetch
            records that have been deleted from the source system.

            The returned records should have at minimum the primary key fields and
            cursor field populated. Other fields can be null.

            Args:
                table_name: The name of the table to read deleted records from.
                start_offset: The offset to start reading from (same format as read_table).
                table_options: A dictionary of options for accessing the table.
            Returns:
                An iterator of deleted records in JSON format and an offset.
                records: An iterator of deleted records (must include primary keys and cursor).
                offset: An offset in dict (same format as read_table).
            """


    ########################################################
    # src/databricks/labs/community_connector/sources/employmenthero/employmenthero_client.py
    ########################################################

    class EmploymentHeroAPIClient:  # pylint: disable=too-many-instance-attributes
        """
        HTTP client for Employment Hero API with OAuth2 authentication.

        Handles:
        - OAuth2 token refresh
        - Rate limiting with exponential backoff
        - Paginated API responses
        """

        def __init__(
            self,
            client_id: str,
            client_secret: str,
            redirect_uri: str,
            authorization_code: str,
            refresh_token: Optional[str] = None,
            base_url: str = "https://api.employmenthero.com",
            oauth_url: str = "https://oauth.employmenthero.com",
            session: Optional[requests.Session] = None,
        ) -> None:
            """
            Initialize the API client.

            Args:
                client_id: OAuth Client ID from Employment Hero Developer Portal.
                client_secret: OAuth Client Secret from Employment Hero Developer Portal.
                redirect_uri: Redirect URI registered with the OAuth application. Required for authorization code exchange.
                authorization_code: Authorization code from the OAuth callback, used for the initial token exchange.
                refresh_token: Optional OAuth refresh token. If not provided, the client will use the authorization code grant; otherwise, it will use the refresh token grant.
                base_url: Base URL for the Employment Hero API. Defaults to "https://api.employmenthero.com".
                oauth_url: Base URL for the Employment Hero OAuth2 endpoints. Defaults to "https://oauth.employmenthero.com".
                session: Optional requests.Session or compatible HTTP session for making requests. Used for connection pooling and easier testing.
            """
            self.client_id = client_id
            self.client_secret = client_secret
            self.redirect_uri = redirect_uri
            self.authorization_code = authorization_code
            self.refresh_token = refresh_token
            self.base_url = base_url
            self.oauth_url = oauth_url

            # Token management
            self._access_token: Optional[str] = None
            self._token_expires_at: Optional[datetime] = None

            # HTTP session for connection pooling
            self._session = session or requests.Session()

        def _get_access_token(self) -> str:
            """
            Return a valid access token, obtaining or refreshing via the OAuth token endpoint.
            Access tokens expire after 15 minutes (900 seconds). Uses authorization_code
            when no refresh_token is available, otherwise refresh_token grant.
            """
            if self._access_token and self._token_expires_at:
                if datetime.now() < self._token_expires_at - timedelta(minutes=5):
                    return self._access_token

            data = {
                "client_id": self.client_id,
                "client_secret": self.client_secret,
            }
            if self.refresh_token:
                data["grant_type"] = "refresh_token"
                data["refresh_token"] = self.refresh_token
            else:
                data["grant_type"] = "authorization_code"
                data["code"] = self.authorization_code
                data["redirect_uri"] = self.redirect_uri

            response = self._session.post(f"{self.oauth_url.rstrip('/')}/oauth2/token", data=data, timeout=30)
            response.raise_for_status()
            token_data = response.json()
            if "access_token" not in token_data:
                raise ValueError("Token response missing access_token")

            self._access_token = token_data["access_token"]
            expires_in = token_data.get("expires_in", 900)
            self._token_expires_at = datetime.now() + timedelta(seconds=expires_in)
            if "refresh_token" in token_data:
                self.refresh_token = token_data["refresh_token"]
            return self._access_token

        def _make_http_request(  # pylint: disable=too-many-arguments,too-many-positional-arguments
            self,
            method: str,
            url: str,
            headers: dict,
            params: Optional[dict],
            data: Optional[dict],
        ) -> requests.Response:
            """
            Execute the HTTP request using the session.

            Args:
                method: HTTP method (GET, POST, PUT, DELETE)
                url: Full URL to request
                headers: Request headers including Authorization
                params: Query parameters
                data: Request body for POST/PUT

            Returns:
                requests.Response object
            """
            method = method.upper()
            if method == "GET":
                return self._session.get(url, headers=headers, params=params, timeout=30)
            if method == "POST":
                return self._session.post(url, headers=headers, json=data, params=params, timeout=30)
            if method == "PUT":
                return self._session.put(url, headers=headers, json=data, params=params, timeout=30)
            if method == "DELETE":
                return self._session.delete(url, headers=headers, params=params, timeout=30)
            raise ValueError(f"Unsupported HTTP method: {method}")

        def request(  # pylint: disable=too-many-arguments,too-many-positional-arguments
            self,
            method: str,
            endpoint: str,
            params: Optional[dict] = None,
            data: Optional[dict] = None,
            max_retries: int = 3,
        ) -> dict:
            """
            Make an authenticated API request to Employment Hero.

            Args:
                method: HTTP method (GET, POST, PUT, DELETE)
                endpoint: API endpoint path (e.g., "/api/v1/organisations/my-org-id/employees")
                params: Query parameters
                data: Request body for POST/PUT
                max_retries: Maximum retry attempts for rate limiting

            Returns:
                Parsed JSON response as dictionary
            """
            access_token = self._get_access_token()
            url = f"{self.base_url.rstrip('/')}{endpoint}"
            headers = {"Authorization": f"Bearer {access_token}"}

            for attempt in range(max_retries + 1):
                response = self._make_http_request(method, url, headers, params, data)
                if response.status_code >= 429 and attempt < max_retries:
                    time.sleep(2**attempt)
                    continue
                response.raise_for_status()
                if not response.text or not response.text.strip():
                    return {}
                return response.json()

        def paginate(
            self,
            endpoint: str,
            params: Optional[dict] = None,
            data_key: str = "data",
            per_page: int = 200,
        ) -> Iterator[dict]:
            """
            Iterate through paginated API responses.

            Uses Employment Hero pagination: page_index (1-based) and item_per_page.
            Calls request() for each page and yields individual records from response[data_key]["items"].

            Args:
                endpoint: API endpoint path
                params: Base query parameters (page_index and item_per_page will be added)
                data_key: Key in response containing the page data (default "data")
                per_page: Number of records per page (sent as item_per_page; max 200 for Employment Hero)

            Yields:
                Individual records from each page
            """
            base_params = dict(params) if params else {}
            page_index = 1

            while True:
                request_params = {**base_params, "page_index": page_index, "item_per_page": per_page}
                response = self.request("GET", endpoint, params=request_params)
                page_data = response.get(data_key, {})
                items = page_data.get("items", [])
                total_pages = page_data.get("total_pages", 1)

                yield from items

                if page_index >= total_pages or not items:
                    break
                page_index += 1


    ########################################################
    # src/databricks/labs/community_connector/sources/employmenthero/employmenthero_schemas.py
    ########################################################

    REFERENCE_STRUCT = StructType(
        [
            StructField("id", StringType(), True),
            StructField("name", StringType(), True),
        ]
    )

    """Nested address struct schema used across multiple tables (e.g. residential_address, postal_address)."""
    ADDRESS_STRUCT = StructType(
        [
            StructField("address_type", StringType(), True),
            StructField("line_1", StringType(), True),
            StructField("line_2", StringType(), True),
            StructField("line_3", StringType(), True),
            StructField("block_number", StringType(), True),
            StructField("level_number", StringType(), True),
            StructField("unit_number", StringType(), True),
            StructField("city", StringType(), True),
            StructField("postcode", StringType(), True),
            StructField("state", StringType(), True),
            StructField("suburb", StringType(), True),
            StructField("country", StringType(), True),
            StructField("is_residential", BooleanType(), True),
            StructField("street_name", StringType(), True),
            StructField("is_manually_entered", BooleanType(), True),
        ]
    )

    """Nested business detail struct schema used across multiple tables."""
    BUSINESS_DETAIL_STRUCT = StructType(
        [
            StructField("country", StringType(), True),
            StructField("number", StringType(), True),
            StructField("business_type", StringType(), True),
        ]
    )

    """Nested tax and national insurance struct schema used across multiple tables."""
    TAX_AND_NATIONAL_INSURANCE_STRUCT = StructType(
        [
            StructField("id", StringType(), True),
            StructField("unique_taxpayer_reference", StringType(), True),
            StructField("national_insurance_number", StringType(), True),
        ]
    )

    """Nested struct for custom field permission (custom_field_permissions array)."""
    CUSTOM_FIELD_PERMISSION_STRUCT = StructType(
        [
            StructField("id", StringType(), True),
            StructField("permission", StringType(), True),
            StructField("role", StringType(), True),
        ]
    )

    """Nested struct for custom field option (custom_field_options array)."""
    CUSTOM_FIELD_OPTION_STRUCT = StructType(
        [
            StructField("id", StringType(), True),
            StructField("value", StringType(), True),
        ]
    )

    """Nested struct for department in work site (departments array)."""
    DEPARTMENT_STRUCT = StructType(
        [
            StructField("id", StringType(), True),
            StructField("name", StringType(), True),
            StructField("assignment_count", LongType(), True),
        ]
    )

    """Nested struct for HR position in work site (hr_positions array)."""
    HR_POSITION_STRUCT = StructType(
        [
            StructField("id", StringType(), True),
            StructField("status", StringType(), True),
            StructField("roster_positions_count", LongType(), True),
            StructField("cost_centre", REFERENCE_STRUCT, True),
            StructField("color", StringType(), True),
            StructField("team_assign_mode", StringType(), True),
        ]
    )


    # =============================================================================
    # Table Schema Definitions
    # =============================================================================

    # --- Organisation-scoped list endpoints (only require organisation_id in path) ---

    """Schema for the employees table (Get Employees / Get Employee API response)."""
    EMPLOYEES_SCHEMA = StructType(
        [
            # Identity
            StructField("id", StringType(), True),
            StructField("account_email", StringType(), True),
            StructField("email", StringType(), True),
            StructField("title", StringType(), True),
            StructField("role", StringType(), True),
            StructField("first_name", StringType(), True),
            StructField("last_name", StringType(), True),
            StructField("middle_name", StringType(), True),
            StructField("known_as", StringType(), True),
            StructField("full_name", StringType(), True),
            StructField("full_legal_name", StringType(), True),
            StructField("legal_name", StringType(), True),
            StructField("pronouns", StringType(), True),
            StructField("avatar_url", StringType(), True),
            # Contact
            StructField("address", StringType(), True),
            StructField("personal_email", StringType(), True),
            StructField("personal_mobile_number", StringType(), True),
            StructField("home_phone", StringType(), True),
            StructField("company_email", StringType(), True),
            StructField("company_mobile", StringType(), True),
            StructField("company_landline", StringType(), True),
            StructField("display_mobile_in_staff_directory", BooleanType(), True),
            # Employment
            StructField("job_title", StringType(), True),
            StructField("code", StringType(), True),
            StructField("location", StringType(), True),
            StructField("employing_entity", StringType(), True),
            StructField("start_date", StringType(), True),
            StructField("termination_date", StringType(), True),
            StructField("status", StringType(), True),
            StructField("termination_summary", StringType(), True),
            StructField("employment_type", StringType(), True),
            StructField("typical_work_day", StringType(), True),
            StructField("roster", StringType(), True),
            StructField("trial_or_probation_type", StringType(), True),
            StructField("trial_length", LongType(), True),
            StructField("probation_length", LongType(), True),
            StructField("global_teams_start_date", StringType(), True),
            StructField("global_teams_probation_end_date", StringType(), True),
            StructField("external_id", StringType(), True),
            StructField("work_country", StringType(), True),
            StructField("payroll_type", StringType(), True),
            StructField("time_zone", StringType(), True),
            StructField("nationality", StringType(), True),
            # Personal / demographic
            StructField("gender", StringType(), True),
            StructField("country", StringType(), True),
            StructField("date_of_birth", StringType(), True),
            StructField("marital_status", StringType(), True),
            StructField("aboriginal_torres_strait_islander", BooleanType(), True),
            StructField("previous_surname", StringType(), True),
            StructField("biography", StringType(), True),
            StructField("instapay_referral_opted_out", BooleanType(), True),
            # Contractor-only
            StructField("independent_contractor", BooleanType(), True),
            StructField("trading_name", StringType(), True),
            StructField("abn", StringType(), True),
            StructField("business_detail", BUSINESS_DETAIL_STRUCT, True),
            StructField("uk_tax_and_national_insurance", TAX_AND_NATIONAL_INSURANCE_STRUCT, True),
            # Nested: teams (Groups in UI)
            StructField("teams", ArrayType(REFERENCE_STRUCT), True),
            StructField("primary_cost_centre", REFERENCE_STRUCT, True),
            StructField("secondary_cost_centres", ArrayType(REFERENCE_STRUCT), True),
            StructField("primary_manager", REFERENCE_STRUCT, True),
            StructField("secondary_manager", REFERENCE_STRUCT, True),
            # Regional: Residential/postal address
            StructField("residential_address", ADDRESS_STRUCT, True),
            StructField("postal_address", ADDRESS_STRUCT, True),
        ]
    )

    """Schema for the certifications table (Get Certifications API)."""
    CERTIFICATIONS_SCHEMA = StructType(
        [
            StructField("id", StringType(), True),
            StructField("name", StringType(), True),
            StructField("type", StringType(), True),  # e.g. "check", "training"
        ]
    )

    """Schema for the cost_centres table (Get Cost Centres API)."""
    COST_CENTRES_SCHEMA = StructType(
        [
            StructField("id", StringType(), True),
            StructField("name", StringType(), True),
        ]
    )

    """Schema for the custom_fields table (Get Custom Fields API)."""
    CUSTOM_FIELDS_SCHEMA = StructType(
        [
            StructField("id", StringType(), True),
            StructField("name", StringType(), True),
            StructField("hint", StringType(), True),
            StructField("description", StringType(), True),
            StructField("type", StringType(), True),  # e.g. "free_text", "single_select", "multi_select"
            StructField("in_onboarding", BooleanType(), True),
            StructField("required", BooleanType(), True),
            StructField("custom_field_permissions", ArrayType(CUSTOM_FIELD_PERMISSION_STRUCT), True),
            StructField("custom_field_options", ArrayType(CUSTOM_FIELD_OPTION_STRUCT), True),
        ]
    )

    """Schema for the employing_entities table (Get Employing Entities API)."""
    EMPLOYING_ENTITIES_SCHEMA = StructType(
        [
            StructField("id", StringType(), True),
            StructField("name", StringType(), True),
        ]
    )

    """Schema for the leave_categories table (Get Leave Categories API)."""
    LEAVE_CATEGORIES_SCHEMA = StructType(
        [
            StructField("id", StringType(), True),
            StructField("name", StringType(), True),
            StructField("unit_type", StringType(), True),  # e.g. "days", "hours"
        ]
    )

    """Schema for the policies table (Get Policies API)."""
    POLICIES_SCHEMA = StructType(
        [
            StructField("id", StringType(), True),
            StructField("name", StringType(), True),
            StructField("induction", BooleanType(), True),
            StructField("created_at", StringType(), True),
        ]
    )

    """Schema for the roles table (Get Roles/Tags API)."""
    ROLES_SCHEMA = StructType(
        [
            StructField("id", StringType(), True),
            StructField("name", StringType(), True),
        ]
    )

    """Schema for the teams table (Get Teams API; shown as Groups in UI)."""
    TEAMS_SCHEMA = StructType(
        [
            StructField("id", StringType(), True),
            StructField("name", StringType(), True),
            StructField("status", StringType(), True),  # e.g. "active"
        ]
    )

    """Schema for the work_locations table (Get Work Locations API)."""
    WORK_LOCATIONS_SCHEMA = StructType(
        [
            StructField("id", StringType(), True),
            StructField("name", StringType(), True),
            StructField("country", StringType(), True),
        ]
    )

    """Schema for the work_sites table (Get Work Sites API)."""
    WORK_SITES_SCHEMA = StructType(
        [
            StructField("id", StringType(), True),
            StructField("name", StringType(), True),
            StructField("status", StringType(), True),  # e.g. "active"
            StructField("roster_positions_count", LongType(), True),
            StructField("hr_positions", ArrayType(HR_POSITION_STRUCT), True),
            StructField("address", ADDRESS_STRUCT, True),
            StructField("departments", ArrayType(DEPARTMENT_STRUCT), True),
        ]
    )


    # =============================================================================
    # Schema Mapping
    # =============================================================================

    """Mapping of table names to their StructType schemas."""
    TABLE_SCHEMAS: dict[str, StructType] = {
        "employees": EMPLOYEES_SCHEMA,
        "certifications": CERTIFICATIONS_SCHEMA,
        "cost_centres": COST_CENTRES_SCHEMA,
        "custom_fields": CUSTOM_FIELDS_SCHEMA,
        "employing_entities": EMPLOYING_ENTITIES_SCHEMA,
        "leave_categories": LEAVE_CATEGORIES_SCHEMA,
        "policies": POLICIES_SCHEMA,
        "roles": ROLES_SCHEMA,
        "teams": TEAMS_SCHEMA,
        "work_locations": WORK_LOCATIONS_SCHEMA,
        "work_sites": WORK_SITES_SCHEMA,
    }


    # =============================================================================
    # Table Metadata Definitions
    # =============================================================================

    """Metadata for each table including primary keys, cursor field, and ingestion type."""
    TABLE_METADATA: dict[str, dict] = {
        "employees": {
            "primary_keys": ["id"],
            "ingestion_type": "snapshot",
        },
        "certifications": {
            "primary_keys": ["id"],
            "ingestion_type": "snapshot",
        },
        "cost_centres": {
            "primary_keys": ["id"],
            "ingestion_type": "snapshot",
        },
        "custom_fields": {
            "primary_keys": ["id"],
            "ingestion_type": "snapshot",
        },
        "employing_entities": {
            "primary_keys": ["id"],
            "ingestion_type": "snapshot",
        },
        "leave_categories": {
            "primary_keys": ["id"],
            "ingestion_type": "snapshot",
        },
        "policies": {
            "primary_keys": ["id"],
            "ingestion_type": "snapshot",
        },
        "roles": {
            "primary_keys": ["id"],
            "ingestion_type": "snapshot",
        },
        "teams": {
            "primary_keys": ["id"],
            "ingestion_type": "snapshot",
        },
        "work_locations": {
            "primary_keys": ["id"],
            "ingestion_type": "snapshot",
        },
        "work_sites": {
            "primary_keys": ["id"],
            "ingestion_type": "snapshot",
        },
    }


    # =============================================================================
    # Supported Tables
    # =============================================================================

    """List of all table names supported by the Employment Hero connector."""
    SUPPORTED_TABLES: list[str] = list(TABLE_SCHEMAS.keys())


    ########################################################
    # src/databricks/labs/community_connector/sources/employmenthero/employmenthero.py
    ########################################################

    class EmploymentHeroLakeflowConnect(LakeflowConnect):
        def __init__(self, options: Dict[str, str], client: Optional[EmploymentHeroAPIClient] = None) -> None:
            """
            Initialize source parameters. Options may include authentication or other configs.
            """
            self.options = options
            self.client = client or EmploymentHeroAPIClient(
                client_id=self.options.get("client_id"),
                client_secret=self.options.get("client_secret"),
                redirect_uri=self.options.get("redirect_uri"),
                authorization_code=self.options.get("authorization_code"),
            )

        def list_tables(self) -> List[str]:
            """
            Returns a list of available tables.
            """
            return SUPPORTED_TABLES.copy()

        def get_table_schema(
            self, table_name: str, table_options: Dict[str, str]
        ) -> StructType:
            """
            Fetch the schema of a table.

            The schema is static and derived from the Employment Hero REST API documentation
            """
            if table_name not in TABLE_SCHEMAS:
                raise ValueError(f"Unsupported table: {table_name!r}")
            return TABLE_SCHEMAS[table_name]

        def read_table_metadata(
            self, table_name: str, table_options: Dict[str, str]
        ) -> Dict[str, str]:
            """
            Fetch the metadata of a table.
            """
            if table_name not in TABLE_METADATA:
                raise ValueError(f"Unsupported table: {table_name!r}")
            return TABLE_METADATA[table_name]

        def _read_snapshot_table(
            self, table_name: str, table_options: Dict[str, str]
        ) -> Tuple[Iterator[dict], dict]:
            """
            Reads a full snapshot of the specified table.
            """
            organisation_id = table_options.get("organisation_id")
            if not organisation_id:
                raise ValueError("table_options must contain 'organisation_id'")
            endpoint = f"/api/v1/organisations/{organisation_id}/{table_name}"
            records = self.client.paginate(
                endpoint=endpoint,
                params={},
                data_key="data",
                per_page=200,
            )
            return records, {}

        def read_table(
            self, table_name: str, start_offset: dict, table_options: Dict[str, str]
        ) -> Tuple[Iterator[dict], dict]:
            """
            Read data from a table and return an iterator of records along with the next offset.
            """
            if table_name not in SUPPORTED_TABLES:
                raise ValueError(f"Unsupported table: {table_name!r}")

            ingestion_type = self.read_table_metadata(table_name, table_options).get("ingestion_type")
            if ingestion_type == "snapshot":
                return self._read_snapshot_table(table_name, table_options)

            raise NotImplementedError(
                f"Ingestion type '{ingestion_type}' for table '{table_name}' is not yet implemented."
            )


    ########################################################
    # src/databricks/labs/community_connector/sparkpds/lakeflow_datasource.py
    ########################################################

    LakeflowConnectImpl = EmploymentHeroLakeflowConnect
    # Constant option or column names
    METADATA_TABLE = "_lakeflow_metadata"
    TABLE_NAME = "tableName"
    TABLE_NAME_LIST = "tableNameList"
    TABLE_CONFIGS = "tableConfigs"
    IS_DELETE_FLOW = "isDeleteFlow"


    class LakeflowStreamReader(SimpleDataSourceStreamReader):
        """
        Implements a data source stream reader for Lakeflow Connect.
        Currently, only the simpleStreamReader is implemented, which uses a
        more generic protocol suitable for most data sources that support
        incremental loading.
        """

        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.lakeflow_connect = lakeflow_connect
            self.schema = schema

        def initialOffset(self):
            return {}

        def read(self, start: dict) -> (Iterator[tuple], dict):
            is_delete_flow = self.options.get(IS_DELETE_FLOW) == "true"
            # Strip delete flow options before passing to connector
            table_options = {
                k: v for k, v in self.options.items() if k != IS_DELETE_FLOW
            }

            if is_delete_flow:
                records, offset = self.lakeflow_connect.read_table_deletes(
                    self.options[TABLE_NAME], start, table_options
                )
            else:
                records, offset = self.lakeflow_connect.read_table(
                    self.options[TABLE_NAME], start, table_options
                )
            rows = map(lambda x: parse_value(x, self.schema), records)
            return rows, offset

        def readBetweenOffsets(self, start: dict, end: dict) -> Iterator[tuple]:
            # TODO: This does not ensure the records returned are identical across repeated calls.
            # For append-only tables, the data source must guarantee that reading from the same
            # start offset will always yield the same set of records.
            # For tables ingested as incremental CDC, it is only necessary that no new changes
            # are missed in the returned records.
            return self.read(start)[0]


    class LakeflowBatchReader(DataSourceReader):
        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.schema = schema
            self.lakeflow_connect = lakeflow_connect
            self.table_name = options[TABLE_NAME]

        def read(self, partition):
            all_records = []
            if self.table_name == METADATA_TABLE:
                all_records = self._read_table_metadata()
            else:
                all_records, _ = self.lakeflow_connect.read_table(
                    self.table_name, None, self.options
                )

            rows = map(lambda x: parse_value(x, self.schema), all_records)
            return iter(rows)

        def _read_table_metadata(self):
            table_name_list = self.options.get(TABLE_NAME_LIST, "")
            table_names = [o.strip() for o in table_name_list.split(",") if o.strip()]
            all_records = []
            table_configs = json.loads(self.options.get(TABLE_CONFIGS, "{}"))
            for table in table_names:
                metadata = self.lakeflow_connect.read_table_metadata(
                    table, table_configs.get(table, {})
                )
                all_records.append({TABLE_NAME: table, **metadata})
            return all_records


    class LakeflowSource(DataSource):
        """
        PySpark DataSource implementation for Lakeflow Connect.
        """

        def __init__(self, options):
            self.options = options
            # TEMPORARY: LakeflowConnectImpl is replaced with the actual implementation
            # class during merge. See the placeholder comment at the top of this file.
            self.lakeflow_connect = LakeflowConnectImpl(options)

        @classmethod
        def name(cls):
            return "lakeflow_connect"

        def schema(self):
            table = self.options[TABLE_NAME]
            if table == METADATA_TABLE:
                return StructType(
                    [
                        StructField(TABLE_NAME, StringType(), False),
                        StructField("primary_keys", ArrayType(StringType()), True),
                        StructField("cursor_field", StringType(), True),
                        StructField("ingestion_type", StringType(), True),
                    ]
                )
            else:
                return self.lakeflow_connect.get_table_schema(table, self.options)

        def reader(self, schema: StructType):
            return LakeflowBatchReader(self.options, schema, self.lakeflow_connect)

        def simpleStreamReader(self, schema: StructType):
            return LakeflowStreamReader(self.options, schema, self.lakeflow_connect)


    spark.dataSource.register(LakeflowSource)
